{
  "title": "[DUBUCE] 3. laboratorijska vjeÅ¾ba - 2019/2020",
  "creator": "Kristijan",
  "slug": "dubuce-3-laboratorijska-vjezba-20192020",
  "tags": [
    "FER",
    "Duboko uÄenje",
    "Laboratorijske vjeÅ¾be"
  ],
  "posts": {
    "26673": {
      "poster": "Kristijan",
      "content": "Jeli moÅ¾da netko ima takoÄ‘er problema sa 2. i 3. zadatkom? (vezano uz toÄnost samoga modela). Konkretno, meni onaj _baseline_ model ima maksimalnu toÄnost do 68%, a implementacija rnn-a uz koriÅ¡tenje _\"vanilla RNN\"_ Äelije ima joÅ¡ loÅ¡iji rezultat , tipa 60-ak %. Prvi zadatak pretpostavljam da mi je toÄan sudeÄ‡i po onim kontrolnim ispisima.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "26678": {
      "poster": "NekocBraca",
      "content": "@Kristijan#26673 Ja ostvarujem jos losije rezultate, jedva iznad 50%. Isto su mi oni ispisi u prvom tocni.\n\nMeni nije najjasniji ovaj dio s avg_pool i eliminiranjem vremenske dimenzije. Nisam uspio skuziti kako bi tu iskoristio `torch.nn.AvgPool2d` s obzirom da je ulaz varijabilne dimenzije. Ja sam onda rucno implementirao sloj koji samo racuna `torch.mean` po `dim=1`. Jel to mozda krivo?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "26700": {
      "poster": "Kristijan",
      "content": "@NekocBraca#26678 Ja sam isto iÅ¡ao ruÄno implementirat (pretpostavljam da je to i cilj, a i ne znam kak bi se to napravilo pomoÄ‡u AvgPool ). Mislim da bi tvoje rjeÅ¡enje trebalo bit toÄno. Kolko sam ja skuÅ¾io da se svaka instanca, tj. svaka ona lista indeksa, prvo pretvori u vektorske reprezentacije (koje su dimenzionalnosti 300). Dobije se lista dim= (duljina_instance x 300). Onda se te reprezentacije zbroje i uzme se sredina kako bi se svaka instanca predstavila samo jednim vektorom dim=(1 x 300). I to se radi za svaku instancu batch-a, i na kraju se dobije batch dim=(10 x 300)  koji ide u mreÅ¾u.  _Jedina promjena_  koju sam ja napravio je da nisam uzeo _mean_ nego sam dijelio sa _izvornom duljinom_ instance (one se dobiju kod _DataLoader-a_ zbog one pad_collate_fn).  Ali koliko vidim, nije baÅ¡ previÅ¡e pomoglo.",
      "votes": {
        "upvoters": [
          "NekocBraca"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "26752": {
      "poster": "antunmod",
      "content": "@Kristijan#26700 \n\nMoÅ¾eÅ¡ koristiti AdaptiveAvgPool2d((1, 300)).\n\nI meni su rezultati Valid accuracy jedva iznad 50%..",
      "votes": {
        "upvoters": [
          "Kristijan",
          "NekocBraca"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "27245": {
      "poster": "Jimothy",
      "content": "![](assets/2020-05-19/00018.png)\n\nS obzirom da je za numerikalizaciju texta i labela potrebno znati vokabulare, treba li onda nakon stvaranja dataseta i vokabulara predati  tom datasetu vokabulare za text i label? Ili je bolje to rijeÅ¡iti na neki drugi naÄin?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "27257": {
      "poster": "Kristijan",
      "content": "@Jimothy#27245  Ja sam tako rijeÅ¡io. Prvo sam napravio vokabular iz train datoteke. Onda sam napravio _Dataset_ objekt i predo mu vokabular pa sam u _getitem()_ koristio onaj \"_.encode()_\" od vokabulara.",
      "votes": {
        "upvoters": [
          "Jimothy",
          "Satanael"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "27376": {
      "poster": "micho (MÌµÍ‘Í€ÍÌ©Ì§iÌ¶Ì‚Ì‰ÍÄ‡Ì´Ì¾ÌÌ€ÌoÌ¶Í‚Ì½ÌºÌŸÌ£)",
      "content": "@Jimothy#27245 Iz prakse - tako se to radi. Uz model ti uvijek ide i datoteka vokabulara, jer bez toga ne moÅ¾eÅ¡ mreÅ¾i predati ispravne podatke. Ovakve stvari **spremaÅ¡ uz model**, tj., to su ti u neku ruku parametri modela. NajlakÅ¡e je napraviti neki json, s obzirom da su te stvari najobiÄniji rjeÄnici.\n\nOno Å¡to se u praksi jako Äesto radi je da se kvantizacija tokena radi po nekoj shemi. Npr. popularan je poredak rijeÄi po frekvenciji (ÄeÅ¡Ä‡e rijeÄi, manji ID). Onda Ä‡e ti svaki dokument uvijek dati isti rjeÄnik. U tom sluÄaju ne trebaÅ¡ snimiti taj rjeÄnik dok god imaÅ¡ originalan tekst po kojem si izvlaÄio rijeÄi, jer do njega doÄ‘eÅ¡ opet na jednoznaÄan naÄin.",
      "votes": {
        "upvoters": [
          "Jimothy",
          "NekocBraca"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "28079": {
      "poster": "antunmod",
      "content": "U 3. zadatku podatke imamo u formatu *batch_size x batch_text_max_length x embedded_dim*, npr. 10x30x300. KaÅ¾e da bi podatke trebalo transponirati u \"time-first\" format, pretpostavljam 30x10x300.\n\nZa mreÅ¾u sam stavio {RNN(300, 150, 2), RNN(150, 150, 2), Linear(150, 150), Linear(150,1)}. Na izlazu mreÅ¾e dobijam podatak 30x10x1 i ne vidim kako bih sad tu trebao izraÄunati gubitak za koji imam samo 10 vrijednosti, po jednu po primjeru. MoÅ¾e li me netko uputiti gdje grijeÅ¡im?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "28090": {
      "poster": "micho (MÌµÍ‘Í€ÍÌ©Ì§iÌ¶Ì‚Ì‰ÍÄ‡Ì´Ì¾ÌÌ€ÌoÌ¶Í‚Ì½ÌºÌŸÌ£)",
      "content": "@antunmod#28079 To ti ovisi o tipu mreÅ¾e. Ako oÄekujeÅ¡ izlaz 10 x 1, onda ili trebaÅ¡ layer koji Ä‡e kao ulaz imati sve time stepove i pretvarati to u konaÄan rezultat (time su ti RNN-ovi i linearni layeri ekstraktori znaÄajki), ili uzimaÅ¡ samo zadnji time step (`[-1, :, :]`). PreporuÄam ovaj drugi pristup.",
      "votes": {
        "upvoters": [
          "antunmod"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "28163": {
      "poster": "talos",
      "content": "Ja sam za baseline napravio s nn.AdaptiveAvgPool1d(1) i torch.mean() i dobivam iste rezultate. \n\nJedva dobivam iznad 50% na validation i test acc. Jel netko moÅ¾e potvrdit da je dobio njihove rezultate?",
      "votes": {
        "upvoters": [
          "Jimothy",
          "brzisha"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "28344": {
      "poster": "InCogNiTo124",
      "content": "@talos#28163 evo meni isprve ovako radi\n\n![](assets/2020-05-21/00030.png)\n\ni dost je consistent izmedu runnova Äak\n\npazi da ako koristis `BCEWithLogitsLoss` da ne stavljas sigmoidu na kraj, to je mene muÄilo prije",
      "votes": {
        "upvoters": [
          "Jimothy",
          "talos"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "28357": {
      "poster": "Jimothy",
      "content": "@InCogNiTo124#28344 \n\nI ja dobivam otprilike takve rezultate, s tim da za neke vrijednosti seeda se accuracy kreÄ‡e oko 70%, a za neke oko 77%.\n\nTako da mislim da acc dosta ovisi o seedu pa probajte viÅ¡e seedova ako vam se Äini da vam je acc nizak.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "28358": {
      "poster": "talos",
      "content": "@InCogNiTo124#28344 \n\nTnx, sigmoida je na mjestu.\n\nNaÅ¡ao sam problem.\n\nPretvarao sam dimenzije unosa(s transpose) prije torch.mean i pogrijeÅ¡io tamo. Treba jednostavno torch.mean(tensor, dim=DIMENZIJA_VREMENA).",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "28361": {
      "poster": "InCogNiTo124",
      "content": "@Jimothy#28357 ja ni ne koristim fiksni seed, svaki tun mi je drukciji, al su vrijednosti uvijek tu negdje (i generalno se penju)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "28515": {
      "poster": "Jimothy",
      "content": "Recimo da postoji ova linija output, hn = model.rnn(input).\n\nJe li joÅ¡ netko primjetio da mu model koji koristi Vanila RNN radi dobro (test_acc >= 0.75) ako u prvi fully connected layer poÅ¡alje hn[0], a model s GRU i LSTM radi dobro ako se poÅ¡alje hn[-1] (Å¡to i je oÄekivano ponaÅ¡anje)?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "28531": {
      "poster": "InCogNiTo124",
      "content": "@Jimothy#28515 ja sam samo lstm koristio, ali ne treba li slat h[0]?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "28538": {
      "poster": "micho (MÌµÍ‘Í€ÍÌ©Ì§iÌ¶Ì‚Ì‰ÍÄ‡Ì´Ì¾ÌÌ€ÌoÌ¶Í‚Ì½ÌºÌŸÌ£)",
      "content": "@InCogNiTo124#28531 Nema previÅ¡e smisla izvoditi ikakvu vrstu RNN-a ako Ä‡eÅ¡ uzimati rezultat nakon prvog vremenskog koraka\n\n@Jimothy \n\nA razlog zaÅ¡to RNN loÅ¡ije radi za `hn[-1]` je cijeli razlog zaÅ¡to su nam potrebni LSTM i GRU - klasiÄan RNN drastiÄno gubi na performansama Å¡to je dublji i Å¡to je sekvenca dulja. Prvi vremenski korak ti ima najkvalitetnije podatke, ali nema osjeÄ‡aj za \"vrijeme\", tj. to ti je praktiÄki samo potpuno povezani sloj. Mislim da Ä‡e se najbolje performanse dobiti ako se kod RNN-a predaju prvih par slojeva, ajmo reÄ‡i prva 3. U svakom sluÄaju bi kod RNN-a bilo smisleno uzeti bar `hn[1]` jer u suprotnom imaÅ¡ viÅ¡e manje samo potpuno povezani sloj. MoÅ¾da probam ako Ä‡u imati vremena ovo s uzimanjem prvih `n` koraka kao ulaz.",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "28539": {
      "poster": "InCogNiTo124",
      "content": "> @micho#28538 Nema previÅ¡e smisla izvoditi ikakvu vrstu RNN-a ako Ä‡eÅ¡ uzimati rezultat nakon prvog vremenskog koraka\n\nJa sam dakle potpuno misinterpretiro dimenzije izlaza `nn.LSTM`",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [
          "micho (MÌµÍ‘Í€ÍÌ©Ì§iÌ¶Ì‚Ì‰ÍÄ‡Ì´Ì¾ÌÌ€ÌoÌ¶Í‚Ì½ÌºÌŸÌ£)"
        ],
        "tuga": []
      }
    },
    "28544": {
      "poster": "micho (MÌµÍ‘Í€ÍÌ©Ì§iÌ¶Ì‚Ì‰ÍÄ‡Ì´Ì¾ÌÌ€ÌoÌ¶Í‚Ì½ÌºÌŸÌ£)",
      "content": "@InCogNiTo124#28539 Nisam baÅ¡ siguran na Å¡to je kolega mislio, ali vidim da on hovori o `h_n`, tj. skrivenim stanjima. Ak se ne varam sve RNN varijante u pytorch vraÄ‡aju par `(output, h_n)`, pa bi onda output `nn.lstm` na 0. indeksu bio output, tj. to bi bilo pravilno. Ali ako priÄamo o izlazima skrivenih slojeva, onda je logiÄno uzimati zadnje.\n\nTak da ovisi o Äemu se priÄa, ja uzimam `nn.lstm(x)[0]`, al to nisu skrivena stanja `h_n`, to je cijeli izlaz po vremenu xD\n\nOpet, ak si ti mislio na `h_n[0]`, onda to Å¡tima samo ako imaÅ¡ jednoslojne RNN-ove. U suprotnom, kaj, morao bi Å egviÄ‡u objaÅ¡njavati da je to tvoja luda distilacijska ideja gdje model uÄiÅ¡ na n slojeva a koristiÅ¡ skrivena stanja samo prvog ğŸ˜‚",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "28662": {
      "poster": "InCogNiTo124",
      "content": "@micho#28544 oke primjenio sam ali jos uvijek mi rezultati sa lstm nisu znacajno bolji, mozda 2-3%, ne znam jel  tak i ostalima\n\nedit okej nvm ako se procita zadatak do kraja, prva recenica u 4. podzadatka to potvrduje",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "28680": {
      "poster": "antunmod",
      "content": "ZaÅ¡to su rijeÄi u csv datotekama podijeljene? Npr. zaÅ¡to je *doesn't* zapisano kao *does n't*? Je li to zbog toga da se u vokabular zapisuju korijeni rijeÄi? I kako nam onda ovaj ostatak *n't* koristi kasnije?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "28707": {
      "poster": "antunmod",
      "content": "Jel u zadnjem zadatku zamiÅ¡ljeno da mi za svaki od 5 hiperparametara napravimo cross product sa svakim drugim (3^5 kombinacija) ili uzmemo najbolje hiperparametre iz prethodnih testiranja i onda mijenjamo samo jedan hiperparametar po testu (3*5 kombinacija). Ovo prvo mi se Äini suludo, no tako mi zvuÄi iz teksta zadatka.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "28719": {
      "poster": "InCogNiTo124",
      "content": "@antunmod#28680 doesn't nosi skroz drukciju informaciju od does, zato jer ne zapravo spojeni does not. Taj n't se koristi ko not i enkodira se tako (ja mislim)\n\n@antunmod#28707 doslovno pise `NaÄin na koji Ä‡ete kombinirati te vrijednost je potpuno na vama (iscrpna reÅ¡etkasta pretraga je vremenski previÅ¡e zahtjevna)`. Meni grid search samo za lstm traje preko sat vremena",
      "votes": {
        "upvoters": [
          "antunmod"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "28742": {
      "poster": "micho (MÌµÍ‘Í€ÍÌ©Ì§iÌ¶Ì‚Ì‰ÍÄ‡Ì´Ì¾ÌÌ€ÌoÌ¶Í‚Ì½ÌºÌŸÌ£)",
      "content": "@antunmod#28707 Samo napravi random search\n\nRandom search unutar intervala se trenutno smatra najboljom pretragom, a postojeÄ‡i pretraÅ¾ivaÄ‡i hiperparametara ionako nude samo to i grid search (grid search je smeÄ‡e)",
      "votes": {
        "upvoters": [
          "antunmod"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "28908": {
      "poster": "talos",
      "content": "Å to se tiÄe izlaznih dimenzija LSTM-a i RNN-ova. Nisam skuÅ¾io koji je zadnji sloj unatraÅ¾nog puta kada je bidirekcijski a koji je zadnji sloj unaprijednog.\n\nZa LSTM:\n\n(output, (h_n, c_n)) - gdje je h_n (broj_slojeva * 2, ...)\n\nTraÅ¾io sam po internetu i naÅ¡ao samo jedan odgovor da je h_n[0] zadnji sloj unaprijedne, a h_n[-1] zadnji sloj unatraÅ¾nog. Zna li netko moÅ¾da?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "28942": {
      "poster": "talos",
      "content": "NaÅ¡ao sam odgovor na pitanje:\n\nhttps://discuss.pytorch.org/t/last-hidden-state-in-bidirectional-stacked-gru/57971/2\n\nSad sve testove ispoÄetka ğŸ™ƒ",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "29192": {
      "poster": "sth",
      "content": "@NekocBraca#26678 Jesi uspio mozda nekako popraviti accuracy?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "29273": {
      "poster": "micho (MÌµÍ‘Í€ÍÌ©Ì§iÌ¶Ì‚Ì‰ÍÄ‡Ì´Ì¾ÌÌ€ÌoÌ¶Í‚Ì½ÌºÌŸÌ£)",
      "content": "@sth \n\n@NekocBraca \n\nProvjerite da ispravno stvarate embedding matrice. Mislim da u uputama labosa piÅ¡e zavaravajuÄ‡e da bi se prilikom stvaranje embedding matrice trebale predavati liste rijeÄi: ja sam jedva naÅ¡ao taj bug i po meni je bolji pristup sljedeÄ‡i:\n\n- stvorite vokabular\n- vokabular mora mapirati redom sve rijeÄi u cijele brojeve (ne smijete imati Å¡upljina)\n- predajte funkciji vokabular:\n\n```python\ndef get_embedding_matrix(vocabulary,\n                         file_path: str = None,\n                         vector_length: int = None,\n                         separate_unk: bool = True) -> np.ndarray:\n    ...\n    words = [x[0] for x in sorted(vocabulary.stoi.items(), key=lambda x: x[1])]\n    embedding_matrix = np.random.normal(loc=0,\n                                        scale=1,\n                                        size=(len(words), vector_length))\n    ...\n```\n\novim Ä‡ete garantirati da imate ispravnu embedding matricu. Bez ovog nisam mogao proÄ‡i 67% toÄnosti. Sad bez problema imam 73-80% na validacijskom setu, ovisno o seedu. BTW, meni je ovaj `vocabulary.stoi` property koji vraÄ‡a objekt tipa `Dict[str, int]`, tj. mapira rijeÄi u indekse.\n\nLjudi koji ne mogu proÄ‡i 50% imaju negdje problem s uÄenjem (tj. aÅ¾uriranjem parametara). Pobrinite se da ADAMu predajete parametre za treniranje:\n\n```python\nclass Baseline(torch.nn.Module):\n    def __init__(units=(300, 150, 150, 1)):\n        ...\n        self.fc = list()\n        \n        for i, unit in enumerate(units[:-1]):\n            self.fc.append(torch.nn.Linear(unit, units[i + 1]))\n            \n        ...\n\n    def get_trainable_parameters(self):\n        parameters = list()\n\n        for fc in self.fc:\n            parameters.extend(fc.parameters())\n\n        return parameters\n\n    def fit(learning_rate, ...):\n        ...\n        optimizer = torch.optim.Adam(self.get_trainable_parameters(),\n                                     lr=learning_rate,\n                                     weight_decay=1e-5)\n        ...\n```",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "NekocBraca",
          "sth"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "29281": {
      "poster": "NekocBraca",
      "content": "@sth#29192 jesam malo. Jedna greska mi je bila da sam stvarao posebni vokabular za svami set podataka (ovo jasno pise u uputi da ne treba raditi, al mi se potkralo). Druga da nisam ukljucio parametre embedding matrice u parametre za treniranje.\n\n@micho#29273 Kod ovih `get_trainable_parameters`, ti ne ukljucujes parametre embedding matrice ili njih zasebno ucis?\n\nI nije mi bas jasno zasto moras imati ti metodu, ja sam shvatio (ali jako moguce da sam krivo shvatio) da kad nasljedis nn.module i u konstruktoru postavljas slojeve s nn.{Linear|Rnn|whatever} da ce pytoch sam povuci te sve parametre kad pozoves `Baseline.parameters()`",
      "votes": {
        "upvoters": [
          "sth"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "29347": {
      "poster": "InCogNiTo124",
      "content": "> @NekocBraca#29281 I nije mi bas jasno zasto moras imati ti metodu, ja sam shvatio (ali jako moguce da sam krivo shvatio) da kad nasljedis nn.module i u konstruktoru postavljas slojeve s nn.{Linear|Rnn|whatever} da ce pytoch sam povuci te sve parametre kad pozoves Baseline.parameters()\n\nDobro si shvatio, samo pobacas atribute u initu i dalje ih torch sam zna pokupit sve. Mozda kolega zeli vecu fleksibilnost haha",
      "votes": {
        "upvoters": [
          "NekocBraca"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "29373": {
      "poster": "micho (MÌµÍ‘Í€ÍÌ©Ì§iÌ¶Ì‚Ì‰ÍÄ‡Ì´Ì¾ÌÌ€ÌoÌ¶Í‚Ì½ÌºÌŸÌ£)",
      "content": "@NekocBraca#29281 Koliko sam ja skuÅ¾io parametri embedding matrice se ne bi trebali uÄiti: kad ih uÄitavaÅ¡ freeze se stavlja na True, i oni su veÄ‡ nauÄeni (ovi iz glovea). Druga stvar, uÄenje embeddinga na tako jednostavnom problemu nema baÅ¡ smisla.\n\nPyTorch ne detektira automatski parametre za liste modula, koliko mi je poznato. + kao Å¡to je kolega rekao, ovako na relativno jednostavan naÄin mogu implementirati zamrzavanje slojeva.\n\nPrije je bilo `self.parameters()` a ja uÄitavam potpuno povezane slojeve u listu, pogodi kako sam saznao da mi optimizator ne dobiva parametre modela :)",
      "votes": {
        "upvoters": [
          "NekocBraca"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "InCogNiTo124"
        ]
      }
    },
    "29387": {
      "poster": "NekocBraca",
      "content": "@micho#29373 Ja sam bio kod Tuteka na konzultacijama i on mi je rekao da se embeddinzi trebaju uciti. Da su oni jedni od kljucnih parametara modela i da se zato moraju uciti, a mi radimo inicijalizaciju samo zato da ne bi trebali uciti od nule. Ti predtrenirani embedinzi su nauceni na nekom vecem skupu podataka pa ih onda mi fine tuneamo na nas skup. Ali s obzirom na rezultate koje si napisao da ostvarujes, ocito u ovom konkretnom slucaju (ne) treniranje njih ne radi veliku razliku.\n\nKuzim, ovo za parametre mi sada ima smisla.",
      "votes": {
        "upvoters": [
          "micho (MÌµÍ‘Í€ÍÌ©Ì§iÌ¶Ì‚Ì‰ÍÄ‡Ì´Ì¾ÌÌ€ÌoÌ¶Í‚Ì½ÌºÌŸÌ£)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "29404": {
      "poster": "micho (MÌµÍ‘Í€ÍÌ©Ì§iÌ¶Ì‚Ì‰ÍÄ‡Ì´Ì¾ÌÌ€ÌoÌ¶Í‚Ì½ÌºÌŸÌ£)",
      "content": "@NekocBraca#29387 E al u ovom ti je stvar, tj. to ja nekako zakljuÄujem kao netko tko radi NLP: Ti imaÅ¡ klasifikacijski problem na jako shitnom zadatku i jako shitnim modelima. Shitni zadatak Ä‡e ti ubiti generalizaciju vektorskih reprezentacija (jer glove je treniran na teÅ¾em zadatku), shitni model Ä‡e ih prenauÄiti.\n\nMoÅ¾da je za 3. i 4. zadatak taj pristup okej, ali svejedno ak ne piÅ¡e neÄ‡u to raditi, jer znam iz iskustva da te stvari ubijaju generalizaciju. Te glove reprezentacije su ionako jako dobre same po sebi, uz attention se vjerojatno mogu dobiti dosta dobri rezultati.\n\nAl isto kao Å¡to ne piÅ¡e da se trebaju uÄiti reprezentacije, ne piÅ¡e ni da se ne smiju pa ko voli nek izvoli.\n\nEDIT: Probao sam taj 2. s treniranjem embeddinga, puno lakÅ¡e se doÄ‘e do njihovih rezultata (konzistentnije). Jedino Å¡to se dogodi da trening dosta uspori. Evo Å¡to mi se izgenerira za validacijski skup:\n\n```json\n{\n  \"loss\": [\n    0.6725784797939477,\n    0.5880095641374719,\n    0.5589046775803731,\n    0.5660436120189067,\n    0.575928290472261\n  ],\n  \"acc\": [\n    0.7512355848434926,\n    0.7957166392092258,\n    0.7907742998352554,\n    0.8017572762218561,\n    0.8088962108731467\n  ],\n  \"pr\": [\n    0.7597597092279733,\n    0.7971042864173348,\n    0.80222869529874,\n    0.8081634941721897,\n    0.8114399820932852\n  ],\n  \"re\": [\n    0.7513841844710787,\n    0.7956599936309421,\n    0.7906136008337676,\n    0.8016382230328296,\n    0.808821507150715\n  ],\n  \"f1\": [\n    0.7555487361495901,\n    0.796381485193515,\n    0.796378799109374,\n    0.8048876336310777,\n    0.8101286287872813\n  ]\n}\n```\n\nRezultati treninga su:\n\n```json\n{\n  \"loss\": [\n    0.5571036715469608\n  ],\n  \"acc\": [\n    0.9033236994219653\n  ],\n  \"pr\": [\n    0.9030456655838686\n  ],\n  \"re\": [\n    0.9033738942681876\n  ],\n  \"f1\": [\n    0.9032097501062476\n  ]\n}\n```\n\nModel nije overfittao pa su onda valjda htjeli da treniramo i embeddinge kao Å¡to je kolega @NekocBraca sugerirao. Pretpostavljam da poboljÅ¡anja idu iz toga Å¡to se glove rjeÄnik i ovaj koji mi imamo ne poklapaju u potpunosti, pa onda bez treniranja ovisimo o tome da se rijeÄi koje fale u gloveu dobro inicijaliziraju normalnom razdiobom. Moja iskustva su sa embeddinzima koji su istrenirani za specifiÄan zadatak, pa onda nemamo takav sluÄaj, zato samo i testirao ovo.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "NekocBraca"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "29598": {
      "poster": "InCogNiTo124",
      "content": "@micho#29373 https://pytorch.org/docs/master/generated/torch.nn.ParameterList.html",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "29666": {
      "poster": "micho (MÌµÍ‘Í€ÍÌ©Ì§iÌ¶Ì‚Ì‰ÍÄ‡Ì´Ì¾ÌÌ€ÌoÌ¶Í‚Ì½ÌºÌŸÌ£)",
      "content": "@InCogNiTo124#29598 Zaboravio sam na ovo jer sam se Å¾urio, nisam uspio ni cijeli 4. predati xD",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "34823": {
      "poster": "InCogNiTo124",
      "content": "Za buduce generacije, sto su vas pitali prosli tjedan ako se itko sjeca?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "34939": {
      "poster": "NekocBraca",
      "content": "@InCogNiTo124#34823 odnos broja parametara izmedju lstm i drugih mreza, sto znace parametri dropout, bidirectional, malo pokazati po kodu i komentirati ona \"istrazivanja\"",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    }
  }
}