{
  "title": "[STRUCE1] Gradivo",
  "creator": "Stark",
  "slug": "struce1-gradivo",
  "tags": [
    "FER",
    "Strojno učenje 1"
  ],
  "posts": {
    "59040": {
      "poster": "Stark",
      "content": "U nadi da nisam jedini koji ima ovo za položiti na jesenskim rokovima, i da ima ljudi koji imaju volje sudjelovati, otvaram ovu temu. Mislim da je zgodnije da imamo jednu temu za oba jesenska roka jer su međusobno blizu, pa da se pitanja ne ponavljaju. \n\nMože pomoć s b) zadatkom?\n\n![](assets/2020-08-09/00001.jpeg)\n\nKoliko shvaćam, prostor inačica su sve hipoteze koje ispravno rade klasifikaciju. To su sve one za koje vrijedi da je x>=w? I kako odrediti VS?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "59054": {
      "poster": "InCogNiTo124",
      "content": "> @Stark#59040 To su sve one za koje vrijedi da je x>=w?\n\nDa\n\n> @Stark#59040 kako odrediti VS?\n\nPomocu [ovoga](https://en.m.wikipedia.org/wiki/Version_space_learning) to je broj svih `w` koji dobro odjeljuju dataset, a to je interval <4, 6]",
      "votes": {
        "upvoters": [
          "ALovelace",
          "Stark"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "59278": {
      "poster": "Stark",
      "content": "Vektor optimalnih težina je okomit na ovaj crtež i prolazi ovom središnjom crvenom točkom?![](assets/2020-08-10/00012.jpeg)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "59394": {
      "poster": "InCogNiTo124",
      "content": "@Stark#59278 ne, vektor optimalnih tezina je vektor koji spaja ishodiste i upravo tu crvenu tocku te lezi u ravnini crteza",
      "votes": {
        "upvoters": [
          "Stark"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "59462": {
      "poster": "Stark",
      "content": "Kako se izračuna ovo pod c)? Nisam nigdje našao a nemam ideju kako.\n\n ![](assets/2020-08-11/00024.jpeg)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "59503": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@Stark#59462 \n\n[math]\n\\begin{aligned}\n&\nsoftmax \\left(\n\\begin{bmatrix}\n1 & 1 & 1 & 1\n\\end{bmatrix}\n\n\\times\n\n\\begin{bmatrix}\n1 & 4 & -2 \\\\\n2 & 4 & -3 \\\\\n2 & 1 & 4 \\\\\n3 & 0 & 5\n\\end{bmatrix}\n\\right)\n\n= \\\\\\\\\n\n&\nsoftmax \\left(\n\\begin{bmatrix}\n8 & 9 & 4\n\\end{bmatrix}\n\\right)\n\n= \\\\\\\\\n\n&\n\\begin{bmatrix}\n0.268 & 0.727 & 0.005\n\\end{bmatrix}\n\n\n\\end{aligned}\n[/math]\n\nOvo se nalazi više manje u 7. natuknicama s predavanja",
      "votes": {
        "upvoters": [
          "Red_Baron",
          "Stark"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "60952": {
      "poster": "Stark",
      "content": "Općenito me dosta bune ovi zadaci kad treba nacrtati pogrešku učenja i ispitnu pogrešku. Znam kako izgleda nazovimo to defaultni graf s prvog predavanja ali kad ulaze ovi hiperparametri nije mi jasno. Vidim da ima dosta takvih zadataka pa ako bi netko mogao objasniti bio bih zahvalan.\n\n![](assets/2020-08-17/00018.jpeg)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "61826": {
      "poster": "Nabas",
      "content": "Kako se rješavaju Bayesove mreže? Ima negdje postupak, primjer? U google dokumentu od domaće zadaće i ishoda učenja nema.\n\nTreći zadatak ZI 19/20, sličan je i u ZIR 2020\n\n![](assets/2020-08-20/00009.png)",
      "votes": {
        "upvoters": [
          "ALovelace"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "62397": {
      "poster": "InCogNiTo124",
      "content": "# TUTORIAL\n## Učenje Bayesove mreže (PGM) iz dataseta\n\nZa potrebe ovog tutoriala izmislio sam svoj neki model i u numpyju generirao dataset. To radim da a) imate ovaj gore zadatak za vjezbu (jer je zadatke **izgleda nemoguće za naci na internetu**) i b) ovaj moj je općenitiji. Takoder, molio bih da sve sto sam napisao provjerite i da se moguce greske prijave ili meni ili administratoru ili pritiskom na objavu->prijavi pa da se popravi.\n\n### Problem:\nZajednicka distribucija raspisana je na sljedeci nacin:\n\n[imath]P(A,B,C,D) = P(A)P(B | A)P(C | A,B)P(D|C)[/imath] \n\nNauciti parametre pomocu sljedeceg dataseta:\n\n[math]\n\\begin{array}{ |c|c|c|c| }\nA & B & C & D \\\\\n\\hline\n0 & 3 & 1 & 1 \\\\\n1 & 3 & 1 & 0 \\\\\n1 & 1 & 0 & 1 \\\\\n1 & 2 & 1 & 0 \\\\\n1 & 1 & 1 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 3 & 1 & 0 \\\\\n0 & 2 & 0 & 0 \\\\\n1 & 2 & 1 & 1 \\\\\n1 & 3 & 0 & 0 \\\\\n0 & 2 & 1 & 1 \\\\\n0 & 2 & 0 & 0\n\\end {array}[/math]\n\n#### Zadatak 1\n[u]Nacrtati model[/u]\n>! ![](assets/2020-08-21/00032.svg)\n\n#### Zadatak 2\n[u] Nauciti distribucije varijabli pomocu dataseta. [/u]\n\nPrvo ponavljanje ViSa, sto je (diskretna) distribucija? To je u biti tablica s jednim retkom, gdje stupci odgovaraju vrijednostima, a u tablici se nalaze odgovarajuce vjerojatnosti stupaca. Suma svih vrijednosti u jednom retku mora biti, dakako, 1.\n\nZadatak se rjesava redom, od prvog cvora ka ostalima (haha streberi bi rekli topoloski sortiranim grafom). Tako se u ovom slucaju prvo odreduje distribucija od A. A moze poprimiti samo 2 vrijednosti, 0 ili 1, od toga 8 jedinica a 4 nule. Therefore\n\n[math]\nA \\sim\n\\begin{array}{ c|c }\n0 & 1 \\\\\n\\hline\n4 & 8\n\\end{array}\n[/math]\n\nPazljivi citatelj ce primjetiti \"cekaj malo pa suma u retku nije 1\" i to je istina. Razlog za to je osobni: ja uvijek prvo popisem brojnost svih vrijednosti pa onda na kraju kad sve provjerim, samo dodam `/12` ili koji vec broj. Drugi razlog je Laplaceovo zagladivanje - ali to necemo sad spominjat sve do samog kraja.\n\nOkej sad, kad imamo [imath]P(A)[/imath], iduca je po redu [imath]P(B)[/imath]. No, ovaj puta ona ne dolazi sama - ona je uvjetovana varijablom A. Kako se to odražava na njenu tablicu distribucije? Pa jako puno u biti - vise nema samo jedan redak, vec je jedan redak _po mogucoj realizaciji uvjeta_. Da ponovim: tablica uvjetne distribucije neke varijable ima onoliko redova koliko njen uvjet ima razlicitih vrijednosti. To znaci da ce nam tablica [imath]P(B|A)[/imath] imati dva retka, a stupaca koliko ona ima razlicitih vrijednosti (tocno 3).\n\nPrvo uzmemo, recimo, kad je uvjet A=0 ispunjen (konkretno 1, 8, 11, 12 redak (ako retke indeksiramo od 1 \\:) )). Vidimo da se B realizirala jednom kao 3, triput kao 2 i nijednom kao 1. Pravimo tablicu:\n\n[math]\nB|A \\sim\n\\begin{array}{ c||c|c|c }\nA & 1 & 2 & 3 \\\\\n\\hline\n0 & 0 & 3 & 1\n\\end{array}\n[/math]\n\nA sad gledamo sve one retke di je uvjet A=1 ispunjen. Ovdje se pak B realizirala triput kao 1, dvaput kao 2 i triput kao 3. Dakle nadopunjujemo nasu tablicu:\n\n[math]\nB|A \\sim\n\\begin{array}{ c||c|c|c }\nA & 1 & 2 & 3 \\\\\n\\hline\n0 & 0 & 3 & 1\\\\\n1 & 3 & 2 & 3\n\\end{array}\n[/math]\n\nSanity check - suma svih vrijednosti je 12, sto je jednako svim broju primjera. All gucci. Prije iduce varijable, kratka neobavezna digresija:\n>! Ovo nekog mozda podsjeti na zajednicku distribuciju dvije varijable [imath]P(A, B)[/imath] i to i nije toliko daleko od istine jer su obje tablice, u biti, funkcija od dvije varijable (i zbog tog dvije dimenzije ([imath]P(A)[/imath] je imala samo jednu dimenziju \\:) ) Razlika je u tome sto u tablici zajednicke distribucije suma bas svih elemenata mora bit 1, dok je u uvjetnoj suma jednog retka 1. No, zasad ionako samo pisemo brojnosti\n\nMoving on. E sad problem moze biti sto ovaj put varijabla ima dva roditelja, stoga ponavljam recenicu:\n> @InCogNiTo124#62397  tablica uvjetne distribucije neke varijable ima onoliko redova koliko njen uvjet ima razlicitih vrijednosti\n\nKoliko razlicitih vrijednosti mogu imati A i B? Odgovor je velicina skupa koji je kartezijev produkt skupova domene slucajnih varijabli A i B. Fensi rječnik za reći [imath]2 \\cdot 3 = 6[/imath]. Dakle, plan rada je ic po svim kombinacijama A i B i pisat kakve se sve vrijednosti od C pojavljuju. Sad necu vise ici korak po korak, nego cu u spojler stavit rjesenje. Napominjem kako je ful bitno pokusati samostalno rijesiti.\n>! SAJK! fkt je bitno da solo probas rijesit. ako ti je samo do tocnog rjesnja, osudujem. Ako pak imas rjesenje, nadam se da sam stavio smjesak na tvoje lice i izvinjavam se zbog neugodnosti\n\n\n>! [math]\nC|A,B \\sim\n\\begin{array}{ c|c||c|c }\nA & B & 0 & 1  \\\\\n\\hline\n0 & 1 & 0 & 0\\\\\n0 & 2 & 2 & 1\\\\\n0 & 3 & 0 & 1\\\\\n1 & 1 & 2 & 1\\\\\n1 & 2 & 0 & 2\\\\\n1 & 3 & 1 & 2\\\\\n\\end{array}\n[/math]\n\n> Yet another sanity check, suma vrijednosti u desnom dijelu tablice je 12 kao sto bi i trebao biti\n\n> Also fun fact: s obzirom da imamo cak 2 uvjeta, ovo bi u teoriji trebao biti tenzor treceg reda (kao matrica samo kockica) no ovo je kakti ispeglani prikaz tog istog tenzora. All is still gucci.\n\nI sad zadnja tablica, D uvjetovan po C, ako ste prezivili proslu tablicu ova je komadić torte. Ukratko, opet ista stvar, idemo po uvjetu, gledamo njegove vrijednosti i gledamo koliko se dogodilo kakvih realizacija nase ciljne varijable.\n\n[math]\nD|C \\sim\n\\begin{array}{ c||c|c }\nC & 0 & 1  \\\\\n\\hline\n0 & 4 & 1\\\\\n1 & 4 & 3\n\\end{array}\n[/math]\n\nI za kraj su nam ostale dvije stvari. Prva je to \"zloglasno\" Laplaceovo zagladivanje koje je u biti najlaksa stvar na svijetu - samo idete po svim brojevima u svim tablicama i dodate +1. Zasto se to radi? Najbolje je objasnit na primjeru uvjetne distribucije [imath]P(B|A)[/imath]. Naime u nasem datasetu se nikad nije pojavilo dogadaj [imath]A=0 \\land B=1[/imath]. Znaci li to da se to nikad _nikad **nikad**_ nece desit? pa ne bas - vise je vjerojatno da imamo sh\\*tan sampling. Zbog tog pretpostavimo da su se svi dogadaji bar jednom desili. To je nase vjerovanje - nasa pristranost, takoder pod nazivom prior probability. To je ono sto vjerujemo da je istina _bez da ista znamo o stvanom svijetu_. Onda taj nas prior napadnemo s nasim podacima, i dobimo nesto izmedu, tzv. posterior. Novi fun fact:\n>! Kakve veze Laplace ima s time? pa on je u biti takoreci izmislio cijelo podrucje vjerojatnosti i dokazao brdo teorema i svasta. Jedan dan su ga iz fore pitali \"ej cika pjer, a kolka je vjerojatnost da ce sutra svanut sunce?\" a on se tog ozbiljno uhvatio. Na kraju je rekao da problem nije moguce rjesiti bez nekih dodanih pretpostavki te je kao odgovor predlozio [imath]1 - \\frac1{n+1}[/imath] gdje je [imath]n[/imath] broj dana koji su prosli od pocetka svemira. Tako da, dosta su velike sanse da ce se ovaj ispit ipak pisat, ali opet nisu bas 100% 😛\n\n\n\nI zadnja stvar koja nam je ostala je sad sve te tablice (s dodanim jedinicama!) pretvorit u [u]actual[/u] distribucije na nacin da dijelimo svaki redak sa sumom u tom retku. Rezultati se nalaze u spoileru:\n>! [math]\nA \\sim\n\\begin{array}{ c|c }\n0 & 1 \\\\\n\\hline\n\\frac{5}{14} & \\frac{9}{14}\n\\end{array}\n[/math]\n\n[math]\nB|A \\sim\n\\begin{array}{ c||c|c|c }\nA & 1 & 2 & 3 \\\\\n\\hline\n0 & \\frac17 & \\frac47 & \\frac27\\\\\n1 & \\frac4{11} & \\frac3{11} & \\frac4{11}\n\\end{array}\n[/math]\n\n[math]\nC|A,B \\sim\n\\begin{array}{ c|c||c|c }\nA & B & 0 & 1  \\\\\n\\hline\n0 & 1 & \\frac12 & \\frac12 \\\\\n0 & 2 & \\frac35 & \\frac25\\\\\n0 & 3 & \\frac13 & \\frac23\\\\\n1 & 1 & \\frac35 & \\frac25\\\\\n1 & 2 & \\frac14 & \\frac34\\\\\n1 & 3 & \\frac25 & \\frac35\\\\\n\\end{array}\n[/math]\n\n[math]\nD|C \\sim\n\\begin{array}{ c||c|c }\nC & 0 & 1  \\\\\n\\hline\n0 & \\frac57 & \\frac27\\\\\n1 & \\frac59 & \\frac49\n\\end{array}\n[/math]\n\nUmro sam dok sam sve ovo pretipko u [imath]\\LaTeX[/imath] 🙃",
      "votes": {
        "upvoters": [
          "Bananaking",
          "Emma63194",
          "Franksta",
          "Nabas",
          "Red_Baron",
          "Stark",
          "basic919 (byk)",
          "data",
          "kix7 (Fish99)",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "62569": {
      "poster": "Nabas",
      "content": "@InCogNiTo124#62397 Hvala puno na tutorialu. Kad je pitanje procjenite MAP procjeniteljem npr. ovdje P(C|A,B) odgovor bi bio C=1 za A=1 i B=2 jer je vjerojatnost 0.75?",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "62591": {
      "poster": "InCogNiTo124",
      "content": "@Nabas#62569 da, tako je",
      "votes": {
        "upvoters": [
          "Nabas"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "67073": {
      "poster": "Stark",
      "content": "Za ovaj zadatak\n\n![](assets/2020-09-05/00011.jpeg)\n\nKako bi išao crtež? Nekako ovako? Svi su linearni osim logističke?\n\n![](assets/2020-09-05/00012.jpeg)\n\nI onda kad se doda primjer (8,1) koji je duboko u području crvenih, što se dogodi? \n\nEvo ako treba prazna slika za crtanje:\n\n![](assets/2020-09-05/00013.jpeg)",
      "votes": {
        "upvoters": [
          "andiamo"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "67230": {
      "poster": "[deleted]",
      "content": "@Stark#67073 Budući da prilikom korištenja linearne regresije kao klasifikatora primjeri koji su ispravno klasificirani isto tako budu kažnjeni i to što su \"ispravnije\" klasificirani, to će biti više kažnjeni, funkcija linearne regresije će se pomaknuti prema tom primjeru kao bi se smanjilo kvadratno odstupanje. Svi ostali će ostati (više-manje) isti.",
      "votes": {
        "upvoters": [
          "Stark"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "67613": {
      "poster": "Red_Baron",
      "content": "@Stark#67073 Mislim da linearna regresija nije dobra na slici. Puno bi bolja bila funkcija y=2; ona dakle prolazi kroz skroz lijevu točku, a kod ostalih minimizira udaljenost; time je ukupna pogreška 2 (ako me matematika ne vara), a po ovom tvojem je 4 (opet, ako me mentalna matematika ne vara).",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "80351": {
      "poster": "a_ko_si_ti",
      "content": "Di se nalazi kviz o kojem je profesor pricao?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "89869": {
      "poster": "[deleted]",
      "content": "![](assets/2020-11-08/00009.png)\n\nČini mi se da bi se ovisno o odabiru hipoteza na ovo pitanje moglo odgovoriti i sa c i sa d. Recimo ako prva hipoteza odvaja 3 pozitivna i 2 negativna + 1 pozitivan uljez, a druga margina odvaja jedan negativan i 4 pozitivna + 1 negativan uljez, tada bi odgovor bio pod c. Ako bi prva hipoteza odvajala isto kao druga, ali s drugim negativnim primjerom, tada bi točan odgovor bio pod d. Je li to zaista tako?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "89912": {
      "poster": "Svarog (Veles)",
      "content": "@gentleman#89869 Neka me netko ispravi ako sam napravio negdje lapsus, ali evo mog obrazloženja zašto je d.\n\nPrvo, primijetimo da je skup primjera linearno neodvojiv, stoga prvi model nikada neće moći točno klasificirati sve primjere. S druge strane, ako bi mogli koristiti dvije hipoteze iz prvog modela, tj. ravnine, onda bi u potpunosti točno klasificirali sve primjere koristeći presjek potprostora koje te dvije ravnine omeđuju. Presjek ta dva potprostora možemo, koristeći indikatorske funkcije, predstaviti s množenjem te dolazimo do ovog drugog modela koji je upravo to. Zbog toga što će postupak optimizacije egzaktno pronaći optimalnu hipotezu, zaključujemo da drugi model može u potpunosti točno klasificirati ovaj skup primjera i slijedno tomu klasificira strogo točnije u odnosu na prvi model.",
      "votes": {
        "upvoters": [
          "Bananaking",
          "Cvija",
          "Emma63194",
          "Red_Baron",
          "[deleted]",
          "in1",
          "member",
          "mikimoj"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "89920": {
      "poster": "[deleted]",
      "content": "@Svarog#89912 \n\n> @Svarog#89912 Zbog toga što će postupak optimizacije egzaktno pronaći optimalnu hipotezu\n\nSmetnuo sam s uma da bi optimizacijski postupak pronašao upravo taj slučaj koji odgovara odgovoru d, hvala!",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "89922": {
      "poster": "Cvija",
      "content": "Evo moja pitanja i odgovori s prvog moodle kviza. Imamo li svi ista pitanja ili su se nekome pojavila i druga?\n\n![](assets/2020-11-08/00012.png)\n\n![](assets/2020-11-08/00013.png)\n\n![](assets/2020-11-08/00014.png)\n\n![](assets/2020-11-08/00015.png)\n\n![](assets/2020-11-08/00016.png)\n\n![](assets/2020-11-08/00017.png)\n\n![](assets/2020-11-08/00018.png)\n\n![](assets/2020-11-08/00019.png)\n\n![](assets/2020-11-08/00020.png)\n\n![](assets/2020-11-08/00021.png)\n\n![](assets/2020-11-08/00022.png)\n\n![](assets/2020-11-08/00023.png)\n\n![](assets/2020-11-08/00024.png)\n\n![](assets/2020-11-08/00025.png)\n\n![](assets/2020-11-08/00026.png)",
      "votes": {
        "upvoters": [
          "Cubii",
          "Emma63194",
          "Noggenfogger (dammitimmad)",
          "Red_Baron",
          "Simpy",
          "neja_negoti",
          "setuid0"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "90169": {
      "poster": "Emma63194",
      "content": "@Cvija#89922 Svi imamo ista pitanja.",
      "votes": {
        "upvoters": [
          "Cvija"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "90273": {
      "poster": "Yeltneb",
      "content": "Zašto ovdje d) nije isto točno? \n\n![](assets/2020-11-09/00013.png)",
      "votes": {
        "upvoters": [
          "Cubii",
          "rockymus (Daniel Plainview)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "90310": {
      "poster": "InCogNiTo124",
      "content": "@Yeltneb#90273 ja mislim da ne mozes znat hoce li generalizrati bolje jer je sum u podacima velik, a ovo je vrlo jaka regularizacija",
      "votes": {
        "upvoters": [
          "Yeltneb"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "90519": {
      "poster": "tito",
      "content": "kako se najbolje pripremiti za ispit, imajući na umu da je na zaokruživanje",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "90540": {
      "poster": "Emma63194",
      "content": "Zna možda netko objasniti ovak zadatak?\n\nNisam sigurna kako su došli do tih težina, niti kako opće pristupiti zadatku. \n\n![](assets/2020-11-10/00011.png)",
      "votes": {
        "upvoters": [
          "Stark"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "90565": {
      "poster": "InCogNiTo124",
      "content": "@\"Emma63194\"#p90540 ma taj je lagani kjut zadatak haha\n\nPrvo krenimo od dataseta koji je generiran onom Gaussovom razdiobom, onaj izraz ti je pravac kojeg bi kao trebala dobit da ti je loss funkcija dobra, to je stvarni pravac\n\nNo, nemas dobru loss funkciju.\n\nDoduse, fear not, jer ako se samo malo algebarski poigras, (y + h(x))^2 = (y - (-h(x))^2, odnosno, efektivno ucis nad podacima koji su flipani oko x osi.\n\n(Alternativno, loss ti moze biti i (h(x) - (-y))^2 gdje se bolje vidi da y mjenja predznak. Razmisli zasto)\n\nE sad ostavljam tebi za vjezbu da nades jednazbu pravca koji nastane flipanjem ovog zadanog u Gaussovoj oko x osi\n\nEdit: nisam objasnio odakle tezine, pliz napravi korespondenciju izmedu tog kak ti izgleda hipoteza i kak ti izgleda pravac, i trebala bi moci matchat iz tog sta ti je w0 a sta w1",
      "votes": {
        "upvoters": [
          "Emma63194",
          "Fran_- (random_trooper)",
          "Noggenfogger (dammitimmad)",
          "Stark",
          "[deleted]",
          "in1"
        ],
        "downvoters": [
          "Juren"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "90810": {
      "poster": "rockymus (Daniel Plainview)",
      "content": "@InCogNiTo124#90310 pa baš zato što je šum velik bi d) trebao biti točan",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "90818": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@rockymus#90810 Ne, jer šum ti onemogućava generalizaciju. Zapamti, ti na šumu učiš. Ako je šum velik, uopće ne moraš točno učiti. A s obzirom na to da imaš istu dimenzionalnost, isto se prenauče, ovaj reguliziraniji se manje prenauči (iako s tim faktorom se vjv ni ništa ne nauči), ali za generalizaciju nemaš pojma.",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "90843": {
      "poster": "InCogNiTo124",
      "content": "@rockymus#90810 da, samo zato sto je regularizacija velika ne znaci da model magicno dobro generalizira, veliki sum ti ubije sve",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "91150": {
      "poster": "FICHEKK",
      "content": "Koja je zadnja lekcija koja ulazi u ispit?",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "[deleted]"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "91431": {
      "poster": "Cvija",
      "content": "https://www.mentimeter.com/s/df70e13555085506b871dc515e4f44cb/ce9e776fe49b\n\nhttps://www.mentimeter.com/s/38c974c6640c7520ad54fc83dd1f468b/e4860d794295\n\nhttps://www.mentimeter.com/s/4b3a322d9a5f54d7fd253a02518ac220/e77cdb7938c2\n\nhttps://www.mentimeter.com/s/f902613407ba3a6ab9778bde7b7d50e4/8262859a3f35\n\nhttps://www.mentimeter.com/s/781be9192b1e81635b6f73c40854512e/df1436b7dddf\n\nhttps://www.mentimeter.com/s/63d94575856c4b46e47284cd7f9eb3e3/9e974b9d4432\n\nhttps://www.mentimeter.com/s/4e525c01d764d0b3eada097d627d86c9/1e95be59ccaa\n\nhttps://www.mentimeter.com/s/c49df11f9b48e73acc50d814ef92148a/297af9dfbcf7\n\nhttps://www.mentimeter.com/s/a2ed2615daff6262eb076b0e6004dd38/d886773799f1\n\nhttps://www.mentimeter.com/s/5de36dbb4fb97f6dca1328d8b661d055/bacaa22dd274\n\nEvo tu su svi kvizovi",
      "votes": {
        "upvoters": [
          "Amali (Amajli)",
          "Amon",
          "Cubii",
          "Daorson",
          "Emma63194",
          "FICHEKK",
          "Fran_- (random_trooper)",
          "InCogNiTo124",
          "Joji",
          "Juren",
          "Longclaw",
          "Louverture (Žuti Kišobran)",
          "Njet",
          "Noggenfogger (dammitimmad)",
          "PrisonMike (Števo)",
          "PudingIzMenze",
          "Stark",
          "Svarog (Veles)",
          "Vrba",
          "[deleted]",
          "chuuya (temari)",
          "in1",
          "johndoe12 (enaiks)",
          "koBASA (hackerman)",
          "member",
          "setuid0",
          "tito",
          "toni98",
          "tre_besty (luk)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "91447": {
      "poster": "Bananaking",
      "content": "![](assets/2020-11-12/00004.png)\n\nMože mi netko pojasniti ovaj zadatak? Zašto alfa2 nije overfitan?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "91461": {
      "poster": "[deleted]",
      "content": "@Bananaking#91447 ![](assets/2020-11-12/00008.png)\n\nMoguće je da se u danoj situaciji nalazimo sa lijeve strane od optimalnog modela, oba su podnaučena. Jedino što znamo iz ovoga je da je razlika između greške u učenju i ispitivanju veća kod hiperparametra [imath]\\alpha_2[/imath] nego kod [imath]\\alpha_1[/imath]",
      "votes": {
        "upvoters": [
          "Bananaking"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "91577": {
      "poster": "Hus",
      "content": "![](assets/2020-11-12/00020.png)\n\n![](assets/2020-11-12/00021.png)\n\n![](assets/2020-11-12/00022.png)\n\n![](assets/2020-11-12/00023.png)\n\nAko netko ima vremena objasniti ova 4 zadatka. Meni uopce nije jasno zasšto su ovo točni odgovori.",
      "votes": {
        "upvoters": [
          "Cubii",
          "hi_doggy"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "hi_doggy"
        ]
      }
    },
    "91627": {
      "poster": "InCogNiTo124",
      "content": "@Hus#91577\n\nPrvi zadatak:\n\nAko si skiciras problem, vidit ces da je najbolji pravac koji dijeli ove dvije tocke zapravo pravac `y=x` odnosno u implicitnom obliku `-ax+ay=0` gdje je a neka konstanta, jer je implicitni oblik pravca invarijantnan na mnozenje skalarom (ekvivalent toga su homogene koordinate iz IRG-a ako se sjecas; ako ne jebiga).\n\nDruga informacija koju bi trebao znati je naci udaljenost od pravca do tocke: jedna lijepa slatka formula `d=h(x) / |w|`, gdje je `h(x)` izlaz hipoteze, a `|w|` norma vektora tezina. Udaljenost tocke od pravca znamo (ja znam da je `sqrt(2) / 2` a od tebe zahtjevam da ako ne znas napamet da izvedes iz geometrije) i lijepo u jednadzbu gore uvrstis neku tocku, recimo (0, 1): `sqrt(2) / 2 = (-a*0+a*1) / (sqrt((-a)**2 + (a**2))` i malo se poigras algebarski da dobis a. To ce ti bit jednako `w2`.\n\nNaravno ista stvar bi trebala doc kao rezultat i ako stavis drugu tocku, to je zato jer (a) matematika tako funkcionira (b) maksimizacija udaljenosti hiperravnine eksplicitno enkodira u nas problem da ce granica prepolavljat udaljenost te dve tocke, odnosno s lijeve i desne strane pravca ce bit ista udaljenost.\n>! ![](https://i.kym-cdn.com/photos/images/original/001/561/446/27d.jpg)\n\nDrugi zadatak:\n\nNadam se da si rjesavao drugi labos koji se ticao logisticke regresije, gdje si uzeo `wTx` i omotao to oko sigmoide. E sad, sigmoida je poprilicno nelinearna, ali granica koju si (trebao) dobiti je linearna. Zasto je tome tako? Zato jer su ti podaci unutar sigmoide linearni. Dakle, ako ti je funkcija PHI linearna, dobit ces linearnu granicu. Ako je nelinerna, i granica ce bit (perhaps surprisingly) nelinearna. Also, ako ti je PHI nelinearna, a aktivacija f linearna, ponovo granica nije linearna. Podaci su kljuc!\n\nTreci:\n\nVrlo mehanicki i brainless zadatak. Pomnozis W sa x i dobijes Wx ciji izlaz nazivamo \"logiti\" (nebitno). Od tih logita racuna se softmax tako da izracunas `exp()` za svaki element vektora, i onda skupa podijelis sa sumom. Softmax bi ti trebo ispast, ako nisam nesto sjebo, `(0.999, 1.026e-10, 1.67e-5)`. I sad treba samo uzet nas softmax vektor i vektor koji trebamo dobit i s njim u multiclass logistic loss ciju formulu imas u skriptama, labosima ili internetu. Jedino je eto problem sto ja dobijem broj 23 kao rjesenje koji nije ponuden tako da ne znam kj se desilo no postupak je okej haha\n\nCetvrti zadatak:\n\nadaptivne bazne funkcije == neuronska mreza s jednim skrivenim slojem. ulazni parametri imaju 10 varijabli, svaka adaptivna bazna funkcij ima dakle 10 tezina + 1 bias. Takvih adaptvnih baznih funkcija ima 4, dakle sve skupa 4(10+1) = 44. Zavrsili smo prvi sloj, ai time smo samo iz podataka izvadili znacajke, sto znaci da tek sad idemo u softmax. Znaci sad se pravimo da su ovih 4 \"pravi podaci\", dakle trebat ce nam jos 4 tezine i 1 bias. Sve skupa 44+5=49. Puno je lakse ako gledas slikicu i pratis sta se dogada nego ovak napamet rjesavat",
      "votes": {
        "upvoters": [
          "-Ivan- (Ivančica)",
          "Amon",
          "Anonimity (BoJack)",
          "Broono (Burućuh)",
          "Daorson",
          "Emma63194",
          "FICHEKK",
          "Fran_- (random_trooper)",
          "Hus",
          "Juren",
          "Stark",
          "WhiteMamba",
          "[deleted]",
          "in1"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "91649": {
      "poster": "Amali (Amajli)",
      "content": "@Cvija#91431 https://docs.google.com/document/d/1pCK4LPwiY9AZON6lZWK7vFXNB6UogtXnDSOYEmg1w0U/edit?usp=sharing prepisani tocni odgovori, bar koji su oznaceni tocno tam il se sjecam da je bio issue s pitanjem, al ne sjecam se bas svih koji su bili sporni",
      "votes": {
        "upvoters": [
          "Cvija",
          "Daorson",
          "InCogNiTo124",
          "Stark",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "91666": {
      "poster": "sphera",
      "content": "@InCogNiTo124#91627 kako onda na slici za softmax piše da je w indeksiran s j,k šta ne bi onda trebao w biti drugačiji za različite klase, i zašto ako j ide od 0 mi svejedno na to dodamo bias, jel nije bias uključen kao w0 kad je j=0",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "91684": {
      "poster": "InCogNiTo124",
      "content": "@sphera#91666 prvo pitanje, svaki stupac u matrici W ti je jedan model za jednu klasu. Onda se oni svi skupe i pretvore softmaxom u distribuciju po klasama\n\nDrugo pitanje, to su sad vec detalji koji nisu nikad nigdje konzistentni, konkretno u ovom slucaju cini mi se da nema biasa pa da je prvi element od 0 indeksiran",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "91728": {
      "poster": "Amali (Amajli)",
      "content": "> @InCogNiTo124#91627 Vrlo mehanicki i brainless zadatak. Pomnozis W sa x i dobijes Wx ciji izlaz nazivamo “logiti” (nebitno). Od tih logita racuna se softmax tako da izracunas exp() za svaki element vektora, i onda skupa podijelis sa sumom. Softmax bi ti trebo ispast, ako nisam nesto sjebo, `(0.999, 1.026e-10, 1.67e-5)`. I sad treba samo uzet nas softmax vektor i vektor koji trebamo dobit i s njim u multiclass logistic loss ciju formulu imas u skriptama, labosima ili internetu. Jedino je eto problem sto ja dobijem broj 23 kao rjesenje koji nije ponuden tako da ne znam kj se desilo no postupak je okej haha\n\nmeni isto ovo nije jasno, i ja dobijem \\~23 generickim softmax postupkom\n\njel itko to skuzio? jel mozda, slucajno, snajder sjebo?",
      "votes": {
        "upvoters": [
          "khm19 (ajmemeni5)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "91730": {
      "poster": "InCogNiTo124",
      "content": "@Amali#91728 1.55 je najblizi 23 tak da je zato xD",
      "votes": {
        "upvoters": [
          "Amon",
          "Fran_- (random_trooper)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "91733": {
      "poster": "Hus",
      "content": "@InCogNiTo124#91627 To je to! Hvala! (meni je isto ispadao u trećem 23 pa sam mislio da sam nesto tesko profulao oko gubitka i modela multinomijalne regresije, no izgleda da nisam jedini pa barem nešto)",
      "votes": {
        "upvoters": [
          "Amali (Amajli)",
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "91736": {
      "poster": "cotfuse",
      "content": "@Hus#91577\n\n4) Ovo prakticki opisuje neuronsku mrezu sa ulaznom dimenzijom 10, skrivenim slojem dimenzije 3 i izlaznim slojem dimenzije 4. Svaki od slojeva ima svoje biase, pa racunica ispada: [imath] (10+1)*3 + (3+1)*4 = 49 [/imath]\n\nOsim toga, ovaj zadatak koji je 3. kod husa, tocan odgovor je 23 i u medjuvremenu su promijenili odgovore tako da je sada i 23 ponudjeno.",
      "votes": {
        "upvoters": [
          "Vrba",
          "in1",
          "member"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "91741": {
      "poster": "Amali (Amajli)",
      "content": "@Amali#91728 sad sam drugi put pokrenula kviz i druga su rjesenja pod tim zadatkom, netko prijavio il oni skuzili I guess",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "91742": {
      "poster": "member",
      "content": "![](assets/2020-11-12/00036.png)\n\nJa sam preko formule za gubitak mislila dobit h(x) ako uvrstim y=1/0, al naravno dobila bi dvije različite vrijednosti. A h(x) se ne mijenja pa bi onda y promijenila i dobila gubitak, al fali mi tu nešto. Ne znam kako bi to do kraja haha",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "91744": {
      "poster": "Amali (Amajli)",
      "content": "@member#91742 dobijes h(x) tak, ispast ce ti simetricni, i sam ih uvrsit u formulu koji god sa suprotnom oznakom od one za koju je dobiven (tipa za y = 1 dobijes jedan broj i taj iskoristis u loss funkciji u clanu za klasu 0. Koju god kombu uzmes dobit ces isto svakako)",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "91854": {
      "poster": "tito",
      "content": "zašto funkcija pogreške ne konvergira kod linearne regresije kada su značajke linearno zavisne?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "91857": {
      "poster": "InCogNiTo124",
      "content": "@tito#91854 u tom slucaju ti matrica dizanja nema puni rang i pseudoinverz ne postoji",
      "votes": {
        "upvoters": [
          "Fran_- (random_trooper)",
          "tito"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92146": {
      "poster": "Bananaking",
      "content": "Što je onaj VS(H,D) ?",
      "votes": {
        "upvoters": [
          "MsBrightside"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92150": {
      "poster": "InCogNiTo124",
      "content": "@Bananaking#92146 version space, prostor inacica, je skup hipoteza koje na datasetu D imaju empirijsku pogresku 0. Takvih moze biti beskonacno.\n\nPrimjer ti je [tu na slici](url), ako su ti hipoteze pravokutnici, onda ti je zelena povrsina VS na example datasetu",
      "votes": {
        "upvoters": [
          "Bananaking"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92224": {
      "poster": "Stark",
      "content": "Može objašnjenje ovog? \n\n![](assets/2020-11-13/00040.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92229": {
      "poster": "Amon",
      "content": "@Stark#92224 Gore je kolega objasnio @Svarog#89912",
      "votes": {
        "upvoters": [
          "Stark"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92263": {
      "poster": "Stark",
      "content": "@Amon#92229 Hvala, nisam uopće skužio da je već bilo pitanje 😅",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92292": {
      "poster": "YenOfVen",
      "content": "Sto zadnje ulazi u ispit?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92308": {
      "poster": "Bananaking",
      "content": "@YenOfVen#92292 11 - Neparametarske metode",
      "votes": {
        "upvoters": [
          "YenOfVen"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92433": {
      "poster": "-Ivan- (Ivančica)",
      "content": "Je li mi može netko molim vas objasniti kako se ovo rješava?\n\n14.\tKoristimo regresiju za predviđanje uspjeha studija na temelju prosjeka ocjena u četiri razreda srednje škole te uspjeha iz matematike i fizike na državnoj maturi (ukupno 6 značajki). Za preslikavanje u prostor značajki koristimo polinom drugog stupnja s interakcijskim parovima značajki (samo parovi!). Pretpostavite da nema multikolinearnosti. Koliko minimalno primjera za učenje trebamo imati, a da bi rješenje bilo stabilno i bez regularizacije?",
      "votes": {
        "upvoters": [
          "Juren",
          "Stark",
          "in1"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92434": {
      "poster": "-Ivan- (Ivančica)",
      "content": "@-Ivan-#92433 (znam da je odgovor 28, ali ne znam zašto)",
      "votes": {
        "upvoters": [
          "Juren",
          "Stark"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92453": {
      "poster": "InCogNiTo124",
      "content": "@-Ivan-#92433 mislim da je do tog da, kad uzmes n varijabli i preslikas ih polinomom drugog stupnja, dobijes n znacajki jednakih originalnim, n(n-1)/2 parova i n kvadrata, za n=6 to je 6+15+6=27 i jos bias za sve skupa 28 znacajki\n\nMinimalno toliko i primjera ti onda treba da ti matrica ima puni rang i da postoji (pseudo)inverz",
      "votes": {
        "upvoters": [
          "-Ivan- (Ivančica)",
          "doki",
          "happysun",
          "in1"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92468": {
      "poster": "-Ivan- (Ivančica)",
      "content": "@InCogNiTo124#92453 Hvala lijepa 😁",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92507": {
      "poster": "Emma63194",
      "content": "> @InCogNiTo124#91627 Drugi zadatak:\n\n> Nadam se da si rjesavao drugi labos koji se ticao logisticke regresije, gdje si uzeo wTx i omotao to oko sigmoide. E sad, sigmoida je poprilicno nelinearna, ali granica koju si (trebao) dobiti je linearna. Zasto je tome tako? Zato jer su ti podaci unutar sigmoide linearni. Dakle, ako ti je funkcija PHI linearna, dobit ces linearnu granicu. Ako je nelinerna, i granica ce bit (perhaps surprisingly) nelinearna. Also, ako ti je PHI nelinearna, a PHI linearna, ponovo granice nije linearna. Podaci su kljuc!\n\nPročitala sam ovo nekoliko puta i razmislila o tome, ali zaista mi ne sjeda. \n\nNije mi jasno, ako imamo sigmoidu ili ako je npr. f neka jako divlja nelinearna funkcija (šta ja znam, sinusi, kosinus, apsolutne vrijednosti, šta se već sve da skombinirati u jednu funkciju), kako unatoč tome funkcija uspije dati neku linearnu granicu samo zbog toga što su podaci linearni?",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92601": {
      "poster": "TentationeM",
      "content": "Što mislite odgovaraju li ovi moodle kvizovi težini pitanja na MI?",
      "votes": {
        "upvoters": [
          "Stark",
          "narval13068 (Dima)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92604": {
      "poster": "InCogNiTo124",
      "content": "@Emma63194#92507 ovo je u biti dobro pitanje, nadao sam se da cete ili uzet to as is, ili ce nekom neka matematicka intucija pomoc, al zaista nije u redu od mene da to ne objasnim.\n\nKratak odgovor: nelinearnost je u ortogonalnoj dimenziji pa se \"ne broji\". Dugi odgovor slijedi u nekoliko skrinova ispod:\n\nOvo je sigmoida od linearne kombinacije:\n>! ![](assets/2020-11-14/00017.png)\n\n ovdje su mi a i b neke \"tezine\", a f je sigmoida, nebitno. Bitno je da bi sad klasifikacijska granica bila f(x)=0.5, odnosno presjekli bi ovu plohu sa ravninom z=0.5. Tebi prepustam da si zamislis sto bi dobila: odgovor je lijepi ravni pravac. Razlog tome je sto je funkcija nelinearna kad se gleda po z dimenziji, dok je linearna kad se gleda u x-y dimenzijama.\n\n\nNovi primjer:\n>! ![](assets/2020-11-14/00018.png)\n\nOvdje ako obratis pozornost na ulaz u sigmoidu, vidit ces da sam promjenio tako da sam dodao \"interakcijsku znacajku\" - fensi rijec za nelinearnost. Radimo opet istu stvar, presjeces povrsinu sa ravninom z=0.5 i kj dobis ovaj put? ERMAGERD pa crta koju dobis vise nije linearna!! Upravo zbog podataka haha\n\nZadnji primjer:\n>! ![](assets/2020-11-14/00019.png)\n\nSad sam dodao kvadrate i vidi sta se dobi, slatki mali bulge ^^ naravno kad bi ovo cudo sad presjekla sa z=0.5 dobila bi neku kruznicu. Y iz that? opet nelinearno, samo zato jer su znacajke nelinearne.\n\n\nTLDR linearna kombinacija znacajki daje linearnu granicu (nakon sto plohu presjecemo ravninom) cak i uz nelinearnu aktivaciju. Ako je kombinacija znacajki nelinearna, granica ce bit nelinearna.",
      "votes": {
        "upvoters": [
          "Cvija",
          "Emma63194",
          "Stark",
          "WhiteMamba",
          "[deleted]",
          "in1",
          "member",
          "neja_negoti",
          "saitama"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92743": {
      "poster": "Yasuke (Bono)",
      "content": "Ako može neko pls objasnit koji je točan odgovor ovdje? \n\n![](assets/2020-11-14/00032.png)\n\nI ovaj također\n\n![](assets/2020-11-14/00033.png)",
      "votes": {
        "upvoters": [
          "Emma63194",
          "Stark"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92772": {
      "poster": "koBASA (hackerman)",
      "content": "![](assets/2020-11-14/00038.png)\n\nMožda ovaj netko?",
      "votes": {
        "upvoters": [
          "Stark",
          "in1"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92785": {
      "poster": "Yasuke (Bono)",
      "content": "@koBASA#92772 Ja sam tu tražio koji su primjeri potporni vektori tako da sam pomnožio w sa svakim primjerom iz X i onda ako kao izlaz dođe -1 ili 1 ili nešto jako blizu, ja sam za prvi i treći primjer dobio 0.99 i -0.99, onda su to potporni vektori što znači da je za ostale primjere alfa=0 i kako je ponuđeno samo jedno rješenje koje na 2. i 4. mjestu ima 0 onda je to to.",
      "votes": {
        "upvoters": [
          "Broono (Burućuh)",
          "Cvija",
          "Emma63194",
          "Fran_- (random_trooper)",
          "[deleted]",
          "koBASA (hackerman)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92802": {
      "poster": "koBASA (hackerman)",
      "content": "@Yasuke#92743 \n\nJa sam sebi nacrtao, u prvom slucaju je udaljenost od tocke 3,3 do 3,4 jednaka 1 pa je sirina margine 0.5, a u drugom slucaju vidis da je udaljenost tocke 3,4 od ovog pravca koji prolazi kroz dvije tocke jednak 3 korijena iz 2, sirina je pola toga, dakle sirina margine je 3 korijena iz 2 puta veca od ove prve, ne znam jel postoji bolji način, al eto.\n\n![](assets/2020-11-14/00043.png)",
      "votes": {
        "upvoters": [
          "Yasuke (Bono)",
          "[deleted]",
          "moji_prsti_prsti_klize_po_njoj"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92817": {
      "poster": "narval13068 (Dima)",
      "content": "@Yasuke#92743 ![](assets/2020-11-14/00044.heic)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92824": {
      "poster": "narval13068 (Dima)",
      "content": "@Yasuke#92743 ![](assets/2020-11-14/00045.png)",
      "votes": {
        "upvoters": [
          "Yasuke (Bono)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92829": {
      "poster": "narval13068 (Dima)",
      "content": "![](assets/2020-11-14/00047.png)\n\nJel zna netko zasto se w0 prije izracuna rezultata mora ovdje dobiti preko prvog zadanog primjera, a ne preko drugog (preko prvog sam dobia da je on -1283.56 pa onaj dio s alfama za novi primjer mi je -265.35 pa daje rjesenje ponudjeno, ali ako idem preko drugog dobit w0 ispadne -57.73, zasto nije isti ka preko prvog :/ )",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92857": {
      "poster": "narval13068 (Dima)",
      "content": "@narval13068#92829 Uspio sam pronaci da postoji neka nestabilnost pa da se nece moc dobiti iz svakog nego iz jednog od njih pa izgleda da treba izracunat iz oba i vidit koje je ponudjeno rjesenje u zdk jer to je ono sta se trazi",
      "votes": {
        "upvoters": [
          "Broono (Burućuh)",
          "member"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [
          "Amali (Amajli)",
          "Broono (Burućuh)",
          "InCogNiTo124",
          "neja_negoti"
        ],
        "tuga": [
          "Fran_- (random_trooper)",
          "sphera",
          "stateboli"
        ]
      }
    },
    "92861": {
      "poster": "MJ3",
      "content": "@narval13068#92829 jel možeš stavit postupak? meni ispadne jako mala vrijednost w0 pa onda ni jedan od ponuđenih odgovora",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92866": {
      "poster": "narval13068 (Dima)",
      "content": "@MJ3#92861 \n\n![](assets/2020-11-14/00052.png)\n\n![](assets/2020-11-14/00053.png)\n\nTamo desno pise jos +w0 u prvoj slici u ove dvije donje jednajdbe kad se izracunava samo je ispalo iz slike",
      "votes": {
        "upvoters": [
          "WhiteMamba"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92884": {
      "poster": "johndoe12 (enaiks)",
      "content": "iz cega ucite za ovaj ispit? samo moodle blicevi, bez starih ispita (uz skriptu i video predavanja naravno)?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92890": {
      "poster": "MJ3",
      "content": "@narval13068#92866 hvala :D",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92908": {
      "poster": "Stark",
      "content": "Da li bi netko znao objasniti zašto je u ovom zadatku d) točan?\n\n![](assets/2020-11-14/00058.png)",
      "votes": {
        "upvoters": [
          "logitech"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92917": {
      "poster": "Vrba",
      "content": "![](assets/2020-11-14/00062.png)\n\njel moze netko ovaj objasnit?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92921": {
      "poster": "am22",
      "content": "@Vrba#92917 Trebas sve modele kojima je C >= 1 (veci C daje slozeniji model) i [imath]\\gamma[/imath] <= 1 (manja preciznost isto daje slozeniji model). Takvih je 36, i onda oduzmes 1 jer C = 1 i [imath]\\gamma[/imath] = 1 su oni vec napisali, tako da je odgovor 35.",
      "votes": {
        "upvoters": [
          "Vrba"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92932": {
      "poster": "Stark",
      "content": "Ovdje mi ispada 0.70, pa očito krivo radim. Može objašnjenje?\n\n![](assets/2020-11-14/00063.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92945": {
      "poster": "RogerRoger",
      "content": "@Stark#92932 \n\nFormula za gubitak binarne logističke regresije je [imath]-y * ln(h(x)) - (1-y) * ln((1-h(x))[/imath].\n\nAko računaš da je npr. y = 1 i izjednačiš s 1.20, onda ti se drugi član poništava jer je pomnožen s (1 - y), što je 0. Uglavnom, imaš [imath]-ln(h(x)) = 1.20[/imath], tj. [imath]h(x) = exp(-1.20)[/imath] i ponoviš izračun samo uz pretpostavku da je y = 0 (jer je oznaka promijenjena).\n\nDalje samo izračunaš [imath]- ln(1 - h(x)) = -ln(1 - exp(-1.20)) = 0.36[/imath].",
      "votes": {
        "upvoters": [
          "Stark"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "92985": {
      "poster": "Noggenfogger (dammitimmad)",
      "content": "molim nekog za objasnjenje, na koji nacin razmisljati redom \n\n ![](assets/2020-11-14/00067.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93149": {
      "poster": "Ziher",
      "content": "Moze li netko objasniti ovaj? Prvo sam dobio 33.935, ovo sam pogodio\n\n![](assets/2020-11-15/00005.png)",
      "votes": {
        "upvoters": [
          "neja_negoti"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93178": {
      "poster": "Lujonlu (Gannicus)",
      "content": "@Ziher#93149 ne regulariziras w0, tako da racunas |w| bez w0",
      "votes": {
        "upvoters": [
          "Amali (Amajli)",
          "cajaznun",
          "neja_negoti"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93186": {
      "poster": "Lujonlu (Gannicus)",
      "content": "@am22#92921 veca preciznost daje slozeniji model. iz skriptice:\n\nNpr.,  ako odaberemo visoku vrijednost za γ,  dobit  ćemo složeniji model,  pa  će  trebati  pojačati  regularizaciju  odabirom  manje  vrijednosti  za C.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93190": {
      "poster": "khm19 (ajmemeni5)",
      "content": "@Vrba#92917 Veci gama daje slozeniji model jer je gama=1/2sigma^2, a sto je varijanca manja to je Gaussovo zvono uze pa bi primjeri trebali biti sto blize kako bi bili slicni, a to daje slozeniji model. C takoder s povecanjem daje vecu slozenost jer je c=1/lambda, a sto je lambda manji to je regularizacija manja pa imamo slozeniji model. U zadatku ima 6 C i 6 gama ukljucujuci i C=1 i gama=1, kombinacija je 6^2=36 i oduzmemo ponuđenu za C=1 i gama=1 i to su preostali prenaučeni modeli.",
      "votes": {
        "upvoters": [
          "Vrba"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93200": {
      "poster": "saitama",
      "content": "@cotfuse#91736  zasto je skriveni sloj dimenzije 3,a ne 4 ?",
      "votes": {
        "upvoters": [
          "ImJustAKid (lumity)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93209": {
      "poster": "korisnickoime",
      "content": "@Ziher#93149 \n\n@Lujonlu#93178 ima neko mozda postupak za ovaj?",
      "votes": {
        "upvoters": [
          "RogerRoger"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93226": {
      "poster": "johndoe12 (enaiks)",
      "content": "![](assets/2020-11-15/00010.png)\n\n![](assets/2020-11-15/00011.png)\n\n![](assets/2020-11-15/00012.png)\n\nZna li netko tocne odgovore na ova pitanja i objasnjenje?",
      "votes": {
        "upvoters": [
          "blast"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93231": {
      "poster": "RogerRoger",
      "content": "@johndoe12#93226 \n\nZa prvo pitanje, svaka značajka ti je jedan klasifikator. OVO ima K povrh 2 klasifikatora, OVR ima K, gdje je K broj klasa. To znači da u ovom primjeru OVO ima 10 značajki, a OVR 5. Međutim, linearni modeli imaju problem kad su klase neuravnotežene, tj. kad jedna klasa ima puno više primjera od druge, i često ga rješavaju tako da klasificiraju sve ispitne primjere u klasu s više primjera za učenje jer tako minimiziraju empirijsku pogrešku (na štetu manje klase). Za 5 klasa i OVR, omjer primjera za svaki klasifikator je 1:4 i rezultira pogrešnom klasifikacijom primjera \"znanosti\" (koja je ionako 5x manja od \"politike\". Zato je točan odgovor drugi ponuđeni.\n\nZa drugo pitanje pogledaj onaj graf složenosti modela naspram empirijske pogreške učenja i ispitivanja (iz skriptice Osnovni koncepti). Pogreška učenja asimptotski teži nuli, ali ispitna pogreška pati od prevelike složenosti. Regularizacija smanjuje složenost modela ovisno o magnitudi parametara w, što kao rezultat ima bolju generalizaciju, ali lošiju prilagođenost primjerima za učenje. Za regularizirani model pogreška učenja se smanjuje do određene točke, ali zatim stagnira jer joj regularizacija ne da da teži nuli. Zato je točan odgovor drugi ponuđeni.\n\nNewtonov postupak koristi Hesseovu matricu za čiji je izračun potrebna cijela matrica preslikavanja i zato je neiskoristiv za online učenje. Mora imati sve primjere za učenje odjednom i ne može primati jedan po jedan primjer i podešavati težine, što je moguće stohastičkim gradijentnim spustom. Newtonov je postupak doduše optimizacija drugog reda i uzima u obzir drugu derivaciju funkcije, tj. zakrivljenost plohe čime sprječava krivudanje i zato može brže konvergirati. Opet je točan odgovor drugi ponuđeni.",
      "votes": {
        "upvoters": [
          "Bananaking",
          "[deleted]",
          "in1",
          "johndoe12 (enaiks)",
          "member"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93244": {
      "poster": "cotfuse",
      "content": "@saitama#93200 Suma koja se direktno vidi u zadatku ide od 0 do 3, a s obzirom da imas bias w0, ostanu ti 3 weighta, sto povlaci da je sirina skrivenog sloja 3",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93247": {
      "poster": "koBASA (hackerman)",
      "content": "@korisnickoime#93209 \n\n![](assets/2020-11-15/00015.png)",
      "votes": {
        "upvoters": [
          "in1",
          "korisnickoime"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93251": {
      "poster": "MJ3",
      "content": "![](assets/2020-11-15/00018.png)\n\nkako izgleda ulaz x nakon preslikavanja polinomom 2.stupnja?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93259": {
      "poster": "[deleted]",
      "content": "@MJ3#93251 [imath]1 + x_1 + x_2 + x_1x_2 + x_1^2 + x_2^2[/imath] \n\nhttps://fer.studosi.net/d/1348-struce-pitanja-i-odgovori/73",
      "votes": {
        "upvoters": [
          "MJ3",
          "in1"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93261": {
      "poster": "johndoe12 (enaiks)",
      "content": "gdje se moze naci objasnjene o induckijskoj pristranosti, u skripti na webu ne vidim..",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93270": {
      "poster": "InCogNiTo124",
      "content": "@johndoe12#93261 [strana 10 poglavlje 2.3](https://www.fer.unizg.hr/_download/repository/StrojnoUcenje.pdf)",
      "votes": {
        "upvoters": [
          "johndoe12 (enaiks)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93287": {
      "poster": "stateboli",
      "content": "@InCogNiTo124#91627 Treci zadatak, 23 je rješenje promijenili su zadatak![](assets/2020-11-15/00026.png)",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93333": {
      "poster": "Yeltneb",
      "content": "@Stark#92908 \n\nNije nitko odgovorio pa ponavljam, jel zna netko ovo objasniti?",
      "votes": {
        "upvoters": [
          "Stark"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93337": {
      "poster": "cotfuse",
      "content": "@Stark#92908 @Yeltneb#93333 \n\nako skup podataka nije linearno odvojiv to znaci da kakav god pravac (pravac odluke modela perceptrona) provuces kroz te podatke, barem jedan primjer ce biti s krive strane pravca. Ti primjeri s krive strane pravca ce uzrokovati da model",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93340": {
      "poster": "cotfuse",
      "content": "@Yeltneb#93333 @Stark#92908 \n\nako skup podataka nije linearno odvojiv to znaci da kakav god pravac (pravac odluke modela perceptrona) provuces kroz te podatke, barem jedan primjer ce biti s krive strane pravca. Ti primjeri s krive strane pravca ce uzrokovati da je gradijent gubtka veci od 0 i zbog toga ce se pravac stalno micati kako bi pokusao uhvatiti te preostale primjere, odnosno model nece konvergirati.",
      "votes": {
        "upvoters": [
          "Red_Baron",
          "Stark",
          "Yeltneb",
          "in1"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93359": {
      "poster": "Atem",
      "content": "@cotfuse#93340 Na temelju čega si zaključio da će gradijent u svakom primjeru biti veći od nule? Zato što svaki par (x,y) ima barem jednu nulu u sebi?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93375": {
      "poster": "hi_doggy",
      "content": "![](assets/2020-11-15/00042.jpeg)\n\n![](assets/2020-11-15/00043.jpeg)\n\nMože netko objasnit? Hvala",
      "votes": {
        "upvoters": [
          "Cubii"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "Cubii"
        ]
      }
    },
    "93390": {
      "poster": "cotfuse",
      "content": "@Atem#93359 ne znam sto znaci da svaki par x,y ima barem jednu nulu u sebi\n\nMeni je to najlakse onak intuitivno poznavajuci onaj graf prostora funkcije gubtka perceptrona koji mozes naci na dnu stranice 15 skriptice o linearnim diskriminativnim modelima. Tamo mozes vidjeti da je nagib grafa konstantan na segmentima. Svaki taj segment predstavlja prostor parametara gdje model jednak broj primjera netocno klasificira. Gdje vise grijesimo, taj je segment strmiji. Ti svi segmenti gradijentnim spustom vode u jedan segment koji je ravan, gdje je gradijent 0. To je prostor gdje model perceptrona sve primjere dobro klasificira. Sad, kad znamo da prostor nije linearno odvojiv, kako nam se mijenja taj graf? Pa znamo da ne mozemo sve primjere tocno klasificirati pa ce taj graf uvijek imati neki nagib, aka gradijent ce uvijek biti veci od 0. Nas postupak optimizacije ce zapravo stalno plesati oko nekog sjecista dva segmenta.",
      "votes": {
        "upvoters": [
          "Atem",
          "Bananaking",
          "in1",
          "member"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93392": {
      "poster": "Stark",
      "content": "@koBASA#93247 Možeš objasniti kako znamo da je w2 = 0, tj druga težina, ako je prvi član u vektoru alfa 0, a ne drugi?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93405": {
      "poster": "koBASA (hackerman)",
      "content": "@Stark#93392 \n\nw2 bi ti bilo [imath]-1*0*3 + (-1)*0.01*4 + 1*0.01*4[/imath] pa je to nula, nema to veze s tim da je prvi član vektora alfa 0, to ti samo znači da ne gledaš vektor koji nije potporni jer ga 0 poništi.",
      "votes": {
        "upvoters": [
          "Stark"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93412": {
      "poster": "Ziher",
      "content": "@hi_doggy#93375 Znaci imamo 2 slucaja: alpha je 0 (promatrani vektor nije potporni) i alpha >0 (promatrani vektor je potporni), sto znaci da izraz u drugoj zagradi mora biti jednak 0 da bi jednakost vrijedila. Taj drugi izraz znaci da se promatrani vektor nalazi na margini (ksi je 0) ili unutar nje (ksi je veci od 0). S obzirom kako ksi ne moze biti < 0, potporni vektori nikad nece izaci izvan margine s prave strane granice.\n\nSto se tice drugog pitanja, zapises si tu razliku u odnosu na rijec \"straja\" i onda gledas koji su ti primjeri najblizi (konkretno, 3 clana za koji imas najmanji L) i onda ih prebrojis i napravis glasanje (2 jedinice + 1 nula =  klasa 1). Onda, samo sto trebas je uvrstiti to sto si dobio u ovu jezgru i gledas najvece brojeve koje si dobio (jer je slicnost veca) i opet dobijes 1. Znaci h1=h2=1",
      "votes": {
        "upvoters": [
          "Broono (Burućuh)",
          "Cubii",
          "Stark",
          "hi_doggy"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93436": {
      "poster": "Stark",
      "content": "@narval13068#92866 Za ovaj primjer gdje je vrijednost -1 (na prvoj slici na kraju) mi ispada w0 = -59.73. Što radim krivo? Ne bi se smjelo dogoditi da w0 ima dvije različite vrijednosti, zar ne?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93440": {
      "poster": "Bananaking",
      "content": "Ovo se čini kao relativno jednostavan zadatak ali sam loš i ne kužim baš to preslikavanje pa jel bi mogao netko šerati svoj postupak?\n\nPretpostavljam da se treba samo računati empirijska greška po formuli iz skripte kao sumu od i do N funkcije max(0, -w^T preslikano(x^i) y^i \n\n![](assets/2020-11-15/00052.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93456": {
      "poster": "Stark",
      "content": "Može objašnjenje?\n\n![](assets/2020-11-15/00053.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93459": {
      "poster": "Atem",
      "content": "![](assets/2020-11-15/00054.png)\n\n![](assets/2020-11-15/00055.png)\n\n![](assets/2020-11-15/00056.png)\n\nAko bi netko mogao objasniti ova tri ili barem reći koje je točno",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93461": {
      "poster": "cotfuse",
      "content": "@Stark#93456 taj jedan hiperparametar je [imath]\\sigma[/imath], koji je dijeljen izmedju svih kernela, 1001 parametar su parametri odabira znacajki u rijetkom jezgrenom stroju, 2829 parametara su 28*100 sto oznacava znacajke odabranih prototipa, + 28 znacajki znacajnosti tih prototipa, to je znaci 28 od ovih 1001 koje optimiziras, znaci koliko svaki od njih pridonosi konacnom rezultatu i jedan je bias",
      "votes": {
        "upvoters": [
          "Emma63194",
          "Stark",
          "chuuya (temari)",
          "in1"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93465": {
      "poster": "ImJustAKid (lumity)",
      "content": "@Bananaking#93440 ![](assets/2020-11-15/00057.jpeg)",
      "votes": {
        "upvoters": [
          "Murin",
          "in1"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93474": {
      "poster": "sphera",
      "content": "ima netko ovaj postupak\n\n![](assets/2020-11-15/00058.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93502": {
      "poster": "Amali (Amajli)",
      "content": "@Atem#93459 \n- 1. ENm\n    - E za svaku epohu. U svakoj epohi prodjes po svakom primjeru da bi azurirao tezine, dakle *N. Za svaki primjer prodjes po svakoj znacajki , dakle *m.</LI>\n- 2. fi(x) = (1, x)\n    - granica nam treba biti linearna u ulaznom prostoru. Narav funkcije f nam nije bitna za izgled granice, bitne su samo znacajke, dakle preslikavanje mora biti linearno, jer ako je preslikavanje nelinearno imas linearnu granicu u tom visem prostoru, a kad se vratis u ulazni dobijes nelinearnu granicu.</LI>\n- 3. funkciju gubitka i optimizacijski postupak\n    - kada promijenis funkciju gubitka s ovog sto je, moras i promijeniti optimizacijski postupak da pase za tu funkciju gubitka koju si odabrao, postupak najmanjih kvadrata je sam za kvadratni gubitak</LI>",
      "votes": {
        "upvoters": [
          "Atem",
          "Bananaking",
          "InCogNiTo124",
          "Noggenfogger (dammitimmad)",
          "Stark",
          "in1",
          "tito"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93515": {
      "poster": "Amali (Amajli)",
      "content": "@sphera#93474 \n\nPrvo izracunas tezine jer ti ionak trebaju za regulariziranu gresku. To mozes napravit matricno - vektor alpha, svaki alpha pomnozen oznakom y pripadnog vektora, puta matrica dizajna (jedan redak jedan primjer). To su ti tezine\n\n![](assets/2020-11-15/00059.png)\n\nOnda imas funkciju pogreske koja je suma gubitaka + regularizacijski faktor, a to je 1/2C * norma tezina na kvadrat (C je 1/lambda). Ovdje pazis da ne ukljucujes tezinu w0 u normu tezina jer se ona ne regularizira, dakle sam ono kaj daje matricno mnozenje ubacis u formulu za normu vektora i kvadriras (ili ni ne korjenujes in the first place, suma kvadrata elemenata, skalarni produkt)\n\n![](assets/2020-11-15/00060.png)\n\nZa gubitak u pogresci (ovo u sumi) ti treba h(x) za svaki vektor, i to mozes izracunat na primarni il dualni nacin sam po formulama i uvrstis u pogresku.\n\n![](assets/2020-11-15/00061.png)",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "in1",
          "sphera",
          "tito"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93533": {
      "poster": "carrieb",
      "content": "@Stark#93436 u skripti pise da bi se ovo trebalo napraviti, makar je i meni cudno da je toliko velika razlika ako uzmemo jedan ili drugi primjer... sa srednjom vrijednosti w0 ne dolazi dobro rjesenje na pitanju\n\n![](assets/2020-11-15/00063.png)",
      "votes": {
        "upvoters": [
          "Amali (Amajli)",
          "Stark",
          "member"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93538": {
      "poster": "johndoe12 (enaiks)",
      "content": "@Amali#93515 \n\ntezine su mi w1=-0.09, w2=-0.08, w3=0.05\n\nh(x1) i h(x2) su mi -1, znaci njihov loss je 0, dakle u empirijskoj pogeski imam samo x3\n\nh(x3) dobijem -0.87\n\n(1-h(x3)) = 1.87\n\ni kad sve to uvrstim u E, dobijem gresku 2.72.\n\nGdje grijesim?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93554": {
      "poster": "Amali (Amajli)",
      "content": "@johndoe12#93538 ja tezine dobijem 0.02, 0, -0.03, tak da tu nesto. Mnozenje s oznakom y (alpha bude [0, -0.01, 0.01] s tim y ukljucenim) mislim da je, iako bi w1 bio -0.06 tako, a i msm da bi w2 bio pozitivan\n\nbuduci da je prvi 0, mozes i smanjit izracun, pa imas alpha*y onda = [-0.01, 0.01], a matrica s kojom mnozis je [[-4, 4, 4],[-2, 4, 1]]",
      "votes": {
        "upvoters": [
          "johndoe12 (enaiks)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93556": {
      "poster": "Amon",
      "content": "@johndoe12#93538 \n\n@Amali#93554 \n\nKolega je gore riješio taj zadatak\n\nhttps://fer.studosi.net/d/1348-struce-pitanja-i-odgovori/95",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "johndoe12 (enaiks)",
          "koBASA (hackerman)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93560": {
      "poster": "Bananaking",
      "content": "Ovaj još nisam vidio ako se ne varam, može netko opisati rješavanje?\n\n![](assets/2020-11-15/00066.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93561": {
      "poster": "johndoe12 (enaiks)",
      "content": "@Noggenfogger#92985 Je li netko zna ovaj? Meni se cini da je a, al nisam garant..",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93562": {
      "poster": "Amali (Amajli)",
      "content": "@Bananaking#93560 ima par rasprava o ovom ako se ne varam",
      "votes": {
        "upvoters": [
          "Amon",
          "Bananaking",
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93563": {
      "poster": "Amon",
      "content": "@Bananaking#93560 Cetvrti zadatak:\n\nadaptivne bazne funkcije == neuronska mreza s jednim skrivenim slojem. ulazni parametri imaju 10 varijabli, svaka adaptivna bazna funkcij ima dakle 10 tezina + 1 bias. Takvih adaptvnih baznih funkcija ima 4, dakle sve skupa 4(10+1) = 44. Zavrsili smo prvi sloj, ai time smo samo iz podataka izvadili znacajke, sto znaci da tek sad idemo u softmax. Znaci sad se pravimo da su ovih 4 “pravi podaci”, dakle trebat ce nam jos 4 tezine i 1 bias. Sve skupa 44+5=49. Puno je lakse ako gledas slikicu i pratis sta se dogada nego ovak napamet rjesavat\n\ncopy-pasted from @InCogNiTo124",
      "votes": {
        "upvoters": [
          "Bananaking",
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93564": {
      "poster": "Amali (Amajli)",
      "content": "@Noggenfogger#92985 \n\n@johndoe12#93561 \n\nodg je c\n\nhttps://docs.google.com/document/d/1pCK4LPwiY9AZON6lZWK7vFXNB6UogtXnDSOYEmg1w0U/edit",
      "votes": {
        "upvoters": [
          "johndoe12 (enaiks)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93576": {
      "poster": "Yeltneb",
      "content": "Jel ima netko rješenja MI 2019/2020?\n\n![](assets/2020-11-15/00067.jpeg)\n\n![](assets/2020-11-15/00068.jpeg)\n\n![](assets/2020-11-15/00069.jpeg)",
      "votes": {
        "upvoters": [
          "member"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93579": {
      "poster": "Amali (Amajli)",
      "content": "> @cotfuse#91736 4) Ovo prakticki opisuje neuronsku mrezu sa ulaznom dimenzijom 10, skrivenim slojem dimenzije 3 i izlaznim slojem dimenzije 4. Svaki od slojeva ima svoje biase, pa racunica ispada: \n\n>                     [imath] (10+1)*3 + (3+1)*4 = 49 [/imath]\n\njel ovo gleda da je fi~0 dummy pa se ne broji kao dio sloja unutra, a izlaz je 4-dim tj za svaku klasu jedan?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93601": {
      "poster": "johndoe12 (enaiks)",
      "content": "![](assets/2020-11-15/00070.png)\n\nima netko postupak za ovaj",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93604": {
      "poster": "Amon",
      "content": "@johndoe12#93601 \n\nIsto je gore kolega napisao\n\n@Yasuke#92785 \n\nDajte čitajte thread ljudi, nema razloga da se neka pitanja 3 puta ponavljaju",
      "votes": {
        "upvoters": [
          "in1",
          "megi7 (someone7)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Amali (Amajli)"
        ],
        "wtf": [],
        "tuga": [
          "InCogNiTo124",
          "PiqueBlinders (zisku)"
        ]
      }
    },
    "93606": {
      "poster": "InCogNiTo124",
      "content": "@Amon#93604 nema❌ se vremena ⏱️",
      "votes": {
        "upvoters": [
          "PiqueBlinders (zisku)",
          "Zogen",
          "toni98"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Amali (Amajli)",
          "Amon"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "93634": {
      "poster": "in1",
      "content": "> @InCogNiTo124#91627 Dakle, ako ti je funkcija PHI linearna, dobit ces linearnu granicu. Ako je nelinerna, i granica ce bit (perhaps surprisingly) nelinearna. Also, ako ti je PHI nelinearna, a PHI linearna, ponovo granice nije linearna. \n\nMislim da si neku riječ falio, možeš srediti to? :)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93643": {
      "poster": "InCogNiTo124",
      "content": "@in1#93634 tehnologija foruma ne dopusta izmjenu objava nakon isteka 10 minuta od postanja",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "in1"
        ]
      }
    },
    "93657": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#93643 Ali dobrostivi diktator ovog pakla nudi mogućnost edita ako mu pošalješ link posta i tekst supstitut",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93679": {
      "poster": "PrisonMike (Števo)",
      "content": "Jel ovo točno? Ako je, kako broj parametara ovisi o druga dva odgovora?\n\n![](assets/2020-11-15/00079.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93684": {
      "poster": "sunceko",
      "content": "@johndoe12#93601 @koBASA#92772 \n\n ![](assets/2020-11-15/00080.jpeg)\n\nNemam pojma jel postupak dobar. Brojke nisu skroz točne, ali su za mene dovoljno blizu.",
      "votes": {
        "upvoters": [
          "Noggenfogger (dammitimmad)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93697": {
      "poster": "Amali (Amajli)",
      "content": "@PrisonMike#93679 mogu ti sam rec da je tocno. Msm da je broj znacajki objasnjeno u predavanju al ne sjecam se, mogu bubnut da tezine ovise o znacajkama, a iz tezina se mogu dobit alphe koje su parametri dualne pa indirektno ovise?, a broj klasa... not sure, msm da je sam reko da nismo radili viseklasni SVM al da je to tocno isto u biti\n\n@sunceko#93684 \n\n@johndoe12#93601 \n\nja bih sam tu probala dobit tezine koje su zadali iz ponudjenih alphi i podataka\n\nnpr vektor alpha se pomnozi s vektorom prvih elemenata (preskoci se dummy!) primjera i trebala bi se dobit prva (ne w~0 vec w~1!) tezina i onda je dobro, tak sam i rijesila zadatak. Nisam gledala koji bi vektori trebali biti potporni da ista eliminiram 😅",
      "votes": {
        "upvoters": [
          "PrisonMike (Števo)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93704": {
      "poster": "PrisonMike (Števo)",
      "content": "> @Amali#93697 mogu ti sam rec da je tocno. Msm da je broj znacajki objasnjeno u predavanju al ne sjecam se, mogu bubnut da tezine ovise o znacajkama, a iz tezina se mogu dobit alphe koje su parametri dualne pa indirektno ovise?, a broj klasa… not sure, msm da je sam reko da nismo radili viseklasni SVM al da je to tocno isto u biti\n\nHvala, evo išao sam pogledat snimku predavanja. Uglavnom ovo kaj kažeš, broj klasa zbog višeklasnog SVM, a kao broj značajki zato jer ako ovisi o broju potpornih vektora, a potporni vektor ovisi o broju značajki, onda i broj parametara ovisi o broju značajki.",
      "votes": {
        "upvoters": [
          "Amali (Amajli)",
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93926": {
      "poster": "johndoe12 (enaiks)",
      "content": "![](assets/2020-11-16/00019.png)\n\nKako je ovdje odg a) ? ne bi li trebalo biti c",
      "votes": {
        "upvoters": [
          "Bato (Morski Pas)",
          "Murin",
          "Yasuke (Bono)",
          "Zabe",
          "korisnickoime",
          "member"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "93957": {
      "poster": "PiqueBlinders (zisku)",
      "content": "@johndoe12#93926 ja sam a) dobio, al ko ce ga znat vise s obzirom kakav je ispit bio",
      "votes": {
        "upvoters": [
          "Ellie",
          "Vrba"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [
          "johndoe12 (enaiks)"
        ],
        "tuga": []
      }
    },
    "94003": {
      "poster": "megi7 (someone7)",
      "content": "Ono pitanje s NR i L2R s blica na moodlu je doslo u ispitu, jedan odgovor tocan na moodlu, drugi na ispitu....\n\nJe li to ja ne vidim razliku u pitanju ili su zeznuli?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "94009": {
      "poster": "Vrba",
      "content": "@megi7#94003 U ispitu _empirijska pogreška učenja ne konvergira_, a na moodleu se _zaustavio s rješenjem_",
      "votes": {
        "upvoters": [
          "Amali (Amajli)",
          "Erinon",
          "Fran_- (random_trooper)",
          "korisnickoime"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "Bananaking",
          "Stark",
          "johndoe12 (enaiks)",
          "khm19 (ajmemeni5)",
          "megi7 (someone7)",
          "moji_prsti_prsti_klize_po_njoj",
          "sane_insane"
        ]
      }
    },
    "94380": {
      "poster": "Ellie",
      "content": "Rjesavala sam ga na probnom slican zadatak kad nije bilo interakcije parova i rjesenje je bilo tocno. \n\nMoja logika je bila ovakva:\n\n [imath]7 \\ linearnih\\  znacajki\\ + {7\\choose 2} \\ interakcije \\ parova+ {7\\choose 3}\\ interakcije\\ trojki+ 7\\ kvadratnih \\ znacajki\\ = \\ 70[/imath]\n \n Ocito je kriva. Moze li mi netko objasniti kako se dolazi do 48?\n\n![](assets/2020-11-17/00002.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "94389": {
      "poster": "FICHEKK",
      "content": "@Ellie#94380 Treba primijetiti da je jedna značajka nepotrebna, a to je prosjek ocjena 4 razreda jer je to zapravo linearna kombinacija značajki 1-4. Dakle, nju ćemo izbaciti ako želimo stabilno rješenje. Preostaje 6 značajki te za njih radimo računamo: 6 linearnih + 6 kvadratnih + 6C2 parova + 6C3 trojki + 1 dummy = 48.",
      "votes": {
        "upvoters": [
          "Ellie",
          "member",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
          "setuid0"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "94390": {
      "poster": "Cubii",
      "content": "@Ellie#94380 Pretpostavljam da je ovdje trik da ne moras iskoristit sve znacajke, pa izbacimo prosjek ocjena jer je ista stvar ko i ocjene od prvog do cetvrtog (a ne zelimo multi kolinearnost).\n\nPa onda po istoj formuli dobijes 47. Nisam siguran od kud jos +1, ali mislim da je to to.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "94450": {
      "poster": "Dekan",
      "content": "@Ellie#94380 Također, problem u tom zadatku je što s polinomom 2. stupnja ne možemo dobiti trojke značajki, za trojke nam treba barem polinom trećeg stupnja. Kada sam pitao prof na ispitu kako možemo dobiti trojke iz polinoma drugog stupnja, odgovor je bio \"ma nema veze, samo uzmite trojke\" ili tako nešto, ne sjećam se više točno.",
      "votes": {
        "upvoters": [
          "Ellie"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "96659": {
      "poster": "login",
      "content": "Jel se bodovi dobiveni na moodleu sumiraju pod kontinuirano svima il samo za visu ocjenu?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "96667": {
      "poster": "Amali (Amajli)",
      "content": "@login#96659 zar nije da je to bonus svima? Mislim da pise negdje u uvodnoj prezi o predmetu, il organizacija predmeta ili stovec ima gdje",
      "votes": {
        "upvoters": [
          "Emma63194"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "96670": {
      "poster": "login",
      "content": "@Amali#96667 \n\nEvo skicnuo sam i pise ovako. Znaci, svi dobivamo.\n\n![](assets/2020-11-21/00027.png)",
      "votes": {
        "upvoters": [
          "Amali (Amajli)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "97175": {
      "poster": "moji_prsti_prsti_klize_po_njoj",
      "content": "Jel postoji netko kome još nije upisana druga lab vježba? Treban li se zabrinuti? (ispitivač Domagoj Alagić)",
      "votes": {
        "upvoters": [
          "Stark",
          "moji_prsti_prsti_klize_po_njoj"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "97205": {
      "poster": "Stark",
      "content": "@moji_prsti_prsti_klize_po_njoj#97175 Meni je upisana ali su mi krivo skalirani bodovi",
      "votes": {
        "upvoters": [
          "moji_prsti_prsti_klize_po_njoj"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "moji_prsti_prsti_klize_po_njoj"
        ]
      }
    },
    "97232": {
      "poster": "logitech",
      "content": "Zna netko koja je logika za ovaj zadatak?\n\n![](assets/2020-11-22/00042.png)\n\nJa sam gledao ovako. Ukupno imamo N=6 primjera. Max greška će biti kada svih šest krivo klasificiramo. Dva primjera mogu biti umjesto 0 označeni kao 1 tj. imamo dva primjera koji mogu biti false positive. Četiri primjera mogu biti umjesto 1 označeni kao 0 tj. imamo četiri primjera koji mogu biti false negative. I onda imamo generalno E(h|D) = 1/6 * (4 * 1 + 2 * 1/2) = 5/6. Min pogreška je 0 kada su svi točno označeni.",
      "votes": {
        "upvoters": [
          "Stark"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "97235": {
      "poster": "Emma63194",
      "content": "@logitech#97232 Je li model koji se tu koristi linearan? Jer, ako je, čini mi se da nije moguće da imaš grešku nula, ali isto tako i nije moguće uzeti hipotezu koja sve krivo klasificira.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "97255": {
      "poster": "Murin",
      "content": "@logitech#97232 \n\n![](assets/2020-11-22/00049.png)\n\n\nOvo ti je u isto vrijeme i najgori i najbolji slucaj, ako je s gornje strane granice sve klasificirano ko 1 onda imas gresku lazni pozitivni*1=0.5,  a ako je klasificirano ko 0 onda imas 4*lazni negativni+1*lazni pozitivni=4.5",
      "votes": {
        "upvoters": [
          "Dekan",
          "InCogNiTo124",
          "logitech"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "100625": {
      "poster": "AromaticConfusion (VrloZbunjen)",
      "content": "Ne kužim iz obavijesti \n\n> Popravili smo vrednovanje zadatka s dva točna ponuđena odgovora (zadatak 10 u grupi A)\n\nKoji su onda točni odgovori na kraju za taj zadatak?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "100626": {
      "poster": "Vrba",
      "content": "@AromaticConfusion#100625 Vjerojatno b) i c) s obzirom da su to isti brojevi",
      "votes": {
        "upvoters": [
          "AromaticConfusion (VrloZbunjen)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "100628": {
      "poster": "AromaticConfusion (VrloZbunjen)",
      "content": "@Vrba#100626 Eh, da sad sam skužio haha, hvala",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "103625": {
      "poster": "johndoe12 (enaiks)",
      "content": "Je li ima netko iskustva sa zamjenom termina labosa? Jel moram imati neki konkretan razlog ili? Kome se javljam?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "103714": {
      "poster": "Antuunn",
      "content": "@johndoe12#103625 ja sam se javio asistentu i rekao je da zamjena dolazi u obzir ako nademo drugog studenta koji bi se isto zamijenio",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "103969": {
      "poster": "johndoe12 (enaiks)",
      "content": "Ima li netko da bi mjenjao termin cetvrtak u 14h?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "103974": {
      "poster": "Antuunn",
      "content": "@johndoe12#103969 ja bi",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "104126": {
      "poster": "johndoe12 (enaiks)",
      "content": "@Antuunn#103974 nazalost, vec se netko javio",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "104543": {
      "poster": "doakes",
      "content": "Ima li netko da bi se mjenjao, moj termin je u petak u 8?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124",
          "Watson (112)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "104910": {
      "poster": "doakes",
      "content": "@doakes#104543  Našao.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "105313": {
      "poster": "Ziher",
      "content": "Zeli li se netko mijenjati za termin LV? Moj je u petak u 13",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "105321": {
      "poster": "Red_Baron",
      "content": "@Ziher#105313 Prodano! Čekiraj poruke",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "105529": {
      "poster": "Ziher",
      "content": "@Ziher#105313 prodano",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "106071": {
      "poster": "a_ko_si_ti",
      "content": "Zeli li se itko mijenjat za termin 3. labosa? moj je u petak u 15",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "106491": {
      "poster": "member",
      "content": "@a_ko_si_ti#106071 mogu ja",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108017": {
      "poster": "a_ko_si_ti",
      "content": "@Ziher @doakes @johndoe12 \n\nJel potvrdi asistent zamjenu na mail. Trazio sam jucer i jos uvik ne odgovara",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108139": {
      "poster": "Ziher",
      "content": "@a_ko_si_ti#108017 Ne potvrdi, samo te smjesti u tu grupu na teamsu, meni je i na kalendaru zapisano zapravo",
      "votes": {
        "upvoters": [
          "a_ko_si_ti"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "120891": {
      "poster": "Atem",
      "content": "Jesmo li obradili temu \"Probabilistički grafički modeli 2\"? (Hoće li doći u ispitu)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "121160": {
      "poster": "Koalalica (zaba)",
      "content": "@Atem#120891 Jesmo. Trebala bi. Nema video za nju, ali ima skriptica.",
      "votes": {
        "upvoters": [
          "Atem"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "121781": {
      "poster": "pepelko",
      "content": "Jel zna netko kad ce prof otvoriti Moodle kvizove za Probabilisticke modele, grupiranje itd? i oce li ih uopce bit?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "121782": {
      "poster": "Stark",
      "content": "@pepelko#121781 Danas bi trebali otvoriti prvi",
      "votes": {
        "upvoters": [
          "Ellie",
          "johndoe12 (enaiks)",
          "pepelko"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "122399": {
      "poster": "tre_besty (luk)",
      "content": "Zna li itko objasnit kako dobiju ove vrijednosti za P(w=1), P(s=1|w=1) i P(r=1|w=1), nisu mi jasne ove sume, koje se vjerojatnosti mnoze/zbrajaju:\n\nhttps://i.ibb.co/m9ZdTm6/Screenshot-from-2021-01-16-17-16-57.png\n\nOvo je graf iz kojeg iscitavaju vjerojatnosti:\n\nhttps://i.ibb.co/SXGzPhm/Screenshot-from-2021-01-16-17-17-26.png",
      "votes": {
        "upvoters": [
          "Cvija",
          "Stark"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "122907": {
      "poster": "logitech",
      "content": "Bi li netko tko ima termin labosa u četvrtak ili petak se mijenjao za utorak u 15:00h?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "122931": {
      "poster": "saitama",
      "content": "Kako se bodovi iz moodle kvizova racunaju u ukupan zbroj bodova?",
      "votes": {
        "upvoters": [
          "Anonimity (BoJack)",
          "ajkula",
          "ls_123 (KimuraKong)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "123162": {
      "poster": "tre_besty (luk)",
      "content": "Kako otvorit menti kvizove, našao sam ih u chatu na teamsu al mi pise \"The quiz is not open. Please wait for the presenter to activate it.\" npr evo link za jedan: https://www.menti.com/b8prshw5y7",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "123761": {
      "poster": "johndoe12 (enaiks)",
      "content": "vidim da na yt nema predavanja o Grupiranju, zna li netko postoji li negdje drugdje?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "123815": {
      "poster": "Cvija",
      "content": "@tre_besty#123162 To su linkovi za pristup kvizu, evo ovdje su linkovi svih kvizova nakon međuispita.. ostaje još jedan koji će biti u četvrtak\n\nhttps://www.mentimeter.com/s/88dfe4c3cff8607d81059196167adb67/2cfc18f6cee2\n\nhttps://www.mentimeter.com/s/d94ee63c4f80a568bc87e7feca6e0e54/c61654fee0f6\n\nhttps://www.mentimeter.com/s/e0565a4b75ec793bf8b024ed0dd6511f/61caee0ec16a\n\nhttps://www.mentimeter.com/s/ad638f1eaaaa77d27b8e6e7030923562/a7d6c9d3b6e5\n\nhttps://www.mentimeter.com/s/0d503045b881b974d907ff41986263e0/96b59b258557\n\nhttps://www.mentimeter.com/s/c6a5036e236e8cd492d0d4d5f82bed49/8d120c78b32c\n\nhttps://www.mentimeter.com/s/4b43f1e3a183b39f28c1886e22686e29/e6b3351bf0f6\n\nNadam se da ne fali nijedan",
      "votes": {
        "upvoters": [
          "Cubii",
          "Daorson",
          "Emma63194",
          "johndoe12 (enaiks)",
          "tre_besty (luk)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "125422": {
      "poster": "Cvija",
      "content": "@Cvija#123815 Evo i posljednji kviz\n\nhttps://www.mentimeter.com/s/35c0c5d434d650baf5f39c714bc71a80/a379ac60f264",
      "votes": {
        "upvoters": [
          "Conrad",
          "Daorson",
          "Emma63194",
          "Stark",
          "johndoe12 (enaiks)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "126429": {
      "poster": "svemia (bearyn)",
      "content": "![](assets/2021-01-22/00040.png)\n\n![](assets/2021-01-22/00041.png)\n\nJel zna netko kako se ova dva zadatka rjesavaju?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "127670": {
      "poster": "Emma63194",
      "content": "Kako izračunati ove parametre pod b)? \n\nNije mi baš sjelo kako se to radi.\n\nAlso, može možda neki hint za c) dio zadatka?\n\n>!![](assets/2021-01-23/00069.jpeg)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "127891": {
      "poster": "johndoe12 (enaiks)",
      "content": "@Emma63194#127670 \n\n![](assets/2021-01-24/00006.jpeg)\n\n![](assets/2021-01-24/00007.jpeg)\n\n![](assets/2021-01-24/00008.jpeg)\n\nBroj parametara: (br.vrijednosti koje var moze imati -1) * vrijednosti koje moze imati 1 roditelj * vrijednosti koje moze imati 2.roditelj*... \n\nnpr. uzmimo var x3 koja je binarna, dakle imamo 2-1, i ona ima 2 roditelja, x1 i x2, a oni su isto binarni pa mnozis sa 2 * 2. \n\nc) tu uvjetnu vjerojatnost prvo raspises po pravilu uvjetne vjv pa ces u brojniku imati zajednicku vjv, a u nazivniku P(x1=T, x4=3) (taj dio meni preskocen na papiru). Onda i brojnik i nazivnik raspises kao sumu i u yagradi stavis sve varijable koje imas, i onda moras iyracunati vjv za sve moguce kombinacije.",
      "votes": {
        "upvoters": [
          "Emma63194",
          "[deleted]",
          "member"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "128006": {
      "poster": "johndoe",
      "content": "Ima li netko odgovore za Kviz 6 Vrednovanje modela?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "128029": {
      "poster": "johndoe",
      "content": "@johndoe#128006 ugh, uvijek zaboravim na onaj shared dokument",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124",
          "Stark"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "138675": {
      "poster": "vidraKida (Ζ ε ύ ς)",
      "content": "Poz imam pitanje sa zadatkom s mi-ja (7.)\n\n![](assets/2021-02-06/00022.png)\n\novdje je moj pokusaj rijesavanja. Ako mi moze netko pomoc. Hvala\n\n![](assets/2021-02-06/00023.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "138799": {
      "poster": "a_ko_si_ti",
      "content": "@vidraKida#138675 Tocno si rjesio, al u zadnjem redu si krivo zbrojio sumu u zagradama. Trebas dobiti 2.1846 + |w|/2, i onda ti to da 2.69.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "138804": {
      "poster": "vidraKida (Ζ ε ύ ς)",
      "content": "@a_ko_si_ti#138799 kak dobijem ovaj |w|, jer u najboljem slucaju mi ispadne 0.56 a treba ispast po ovome 0.51",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "138805": {
      "poster": "vidraKida (Ζ ε ύ ς)",
      "content": "Jel moze pomoc oko ovog iz MI-ja (8.)\n\n![](assets/2021-02-06/00031.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "138807": {
      "poster": "a_ko_si_ti",
      "content": "Jel itko moze objasniti 10., 17. i 24. iz ovogodišnjeg MI-a kako se rade.\n\n10.\n\n![](assets/2021-02-06/00032.png)\n\nTocno je 50/32 i 25/16. Ja stalno dobivam 50/21. Za OVR uzmem N klasifikatora (4) i svaki se trenira sa 1000 primjera, sto daje za svaki Grammovu matricu od 1 000 000 elemenata, i tako puta 4 puta (4 000 000). Za OVR imam K povrh 2 (6) klasifikatora, i imaju razlicito elemenata, ovisno o broju primjera(400+400, 400+100 * 4, i 100+100). U sumi dobijem 1 680 000 elemenata u Grammovoj matrici i iz toga omjer 50:21\n\n17.\n\n![](assets/2021-02-06/00033.png)\n\nTočan je A. Ovdje sam izračunao h(x) za svaki x, po tome odredio koji je y, i onda to ubacio u formulu za ovisnost **w** i **alpha** [5. stranica ovdje](https://www.fer.unizg.hr/_download/repository/SU-2020-08-StrojPotpornihVektora.pdf). Time sam dobio sistem od 4 jednadžbe sa 4 nepoznanice (alphe) i od toga uvijek dobio da nema rješenja. Jel to točan način za radit pa ja krivo rješavam sistem jednadžbi ili ima neki drugi način?\n\n24.\n\n![](assets/2021-02-06/00034.png)\n\n    Ovdje je točno pod D. Mogu navuć brojke al mi nije baš 100% jasno. 501 parametar optimazacije mi je jasno jer imamo 500 jezgrenih funkcija (za svaki primjer jednu) u jezgrenom stroju, plus φ1=1. Al kako su došli do 3079 parametara? Sta ne bi bilo da za svaki od 38 prototipa imamo samo po jedan skalar _w_?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "138810": {
      "poster": "a_ko_si_ti",
      "content": "@vidraKida#138804 Pošto se  radi o L1 regularizaciji, |w| je apsolutna suma svih elemenata od w (osim w0 jer njega ne skaliramo). To ti daje 0.94+0.08=1.02, i onda iz formule o gubitku vidis da se ubaci |w|/2=0.56.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "138834": {
      "poster": "vidraKida (Ζ ε ύ ς)",
      "content": "Moze pomoc oko (15.) u MI-ju? \n\n![](assets/2021-02-07/00001.png)\n\nracunao sam ali dobijem da moram skalar pomozit dalje s vektorom fi(x) a ne znam dal radim nesto krivo ili?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "138837": {
      "poster": "vidraKida (Ζ ε ύ ς)",
      "content": "@vidraKida#138834 ![](assets/2021-02-07/00002.png)\n\n1.28 je rj.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "138838": {
      "poster": "vidraKida (Ζ ε ύ ς)",
      "content": "Moze pomoc oko (16.) iz MI-ja\n\n![](assets/2021-02-07/00003.png)\n\n![](assets/2021-02-07/00004.png)\n\ndobio sam da je w2 = 5 a u rj. je -5",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "138852": {
      "poster": "a_ko_si_ti",
      "content": "@vidraKida#138838 Ja sam to grafički rješio. Jednom kad nacrtas tocke, lako se vidi koja je hiperravnina, a onda i njena normala (w). \n\nAl analitički gledano: Odkud ti da je skalarni umnožak 0? Ta dva vektora nisu okomita, nego paralelna.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "138877": {
      "poster": "TentationeM",
      "content": "Zna li netko objasniti kod linearne regresije zašto je sustav nekonzistentan ako vrijedi `rang(X) < rang(X|y)`, gdje je X matrica dizajna, a y vektor oznaka?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "138906": {
      "poster": "vidraKida (Ζ ε ύ ς)",
      "content": "@a_ko_si_ti#138852 ima u formulama negdje navedeno  al moguce da sam ja sjebo \n\n![](assets/2021-02-07/00006.png)\n\n@a_ko_si_ti#138852 daj kad se vec samo ti i ja dopamo ovdje jel mo mozes pomoc oko ostalih zadataka ili ako imas postupak MI-ja da sherash tu ili nekaj",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "138912": {
      "poster": "vidraKida (Ζ ε ύ ς)",
      "content": "@a_ko_si_ti#138852 btw rijesio sam, krivo sam napisao krate se w0 i w0 i dobijes iz toga jednadzbu da je -w2 = w1",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "139010": {
      "poster": "InCogNiTo124",
      "content": "@TentationeM#138877 jedan od super primjera di znanje linearne dobro dode\n\npromatramo sljedeci slucaj: broj redaka je veci od broja stupaca (dakle imamo vise primjera nego znacajki). rang matrice je min(broj redaka, broj stupaca), dakle u ovom slucaju rang == broj stupaca. dodavanjem jednog stupca povecali smo rang za jedan.\n\ngdje tu u cijelu pricu ulazi konzistentnost? ako imamo vise redaka od stupaca, u jeziku linearne algebre to znaci kao da imas vise jednadzbi od nepoznanica (dakle mozes imat x1 .. x5, ali 20 jednadzbi) za takav sustav jednadzbi kazemo da je preodreden odnosno inkonzistentan.\n\njos jednom ukratko, ako se rang matrice poveca kad dodas stupac, znaci da ima manje stupaca od redaka, sto znaci da imas vise jednadzbi od varijabli, sto znaci da ti je sustav inkonzistentan\n\nskroz druga prica se dogada u drugom slucaju, ako je broj redaka manji od broja stupaca (imas manje primjera nego znacajki). Tada je rang matrice == broju primjera (a ne znacajki, kao gore). stoga, dodavanjem novog stupca ne mjenja se rang. u tom slucaju (ako na takvu matricu gledas kao sustav jednadzbi) sustav vise nije inkonzistentan, vec je pododređen (razmisli - ako imas 3 jednadzbe s 4 nepoznanice, ne mozes ih rjesit sve 4 nikako; zato pododreden. za takav sustav mozes nac x4 za svaki od x1..x3 koji zadovoljava sustav jednadzbi)\n\nprocitaj polako i po potrebi nacrtaj si o cem pricam, jednom kad skuzis ima savrseno smisla, makar zvuci komplicirano",
      "votes": {
        "upvoters": [
          "TentationeM"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "140218": {
      "poster": "vidraKida (Ζ ε ύ ς)",
      "content": "Nisu stavili ispit za zira u repo. Ja sam uspio par racunskih zadatka pretezito iz 1. ciklusa prepisat s ponudenim odg. Mogli bi napravit neki kolektivni dokument ili tu skupa probat rijesit te zadatke i mi i zi pa da ostane narastajima poslje nas. #djecaSUBuducnost, slikam ove zadatke pa ih naknadno tu objavim",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "140225": {
      "poster": "a_ko_si_ti",
      "content": "@vidraKida#140218 https://www.fer.unizg.hr/_download/repository/SU-2020-1IR.pdf\n\nTu je ispit",
      "votes": {
        "upvoters": [
          "Stark"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "231102": {
      "poster": "Bananaking",
      "content": "U ulaznom prostoru X = {0, 1}^3 definiramo klasif model: h(**x**; **w**) = **1**(w0 + w1*x1 + w2*x2 + w3*x3 >= 0)\n\nKoja je dimenzija prostora parametara te koliko razlicitih hipoteza postoji u ovom modelu? \n\nOdg: Dim prostora parametara je 4 (jer ih je 4), hipoteza ima manje od 256. Kako dobijem broj hipoteza? Na primjeru je riješen onaj primjer sa 2 dimenzijskim ulaznim prostorom pa je donekle jasno zašto ih je 14 a ne 16 (2 xora ne mogu pravcem klasificirati), ali kako je to matematički za više dimenzionalan ulazni prostor?",
      "votes": {
        "upvoters": [
          "vidraKida (Ζ ε ύ ς)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "231279": {
      "poster": "Bananaking",
      "content": "@Ellie#94380 Može netko ovo primjeniti na zadatak s roka?\n\nPo meni bi išlo 1 (dummy) + 6 (linearnih) + 6 (kvadratnih) + 6C2 (parovi) * 2^2 (kombinacije kvadrata nad njima npr x1x2, x1^2 x2, x1x2^2, x1^2 x2^2) + 6C3 (trojke) * 2^3 (kombinacije kvadrata) = 233 = očito krivo, treba biti A) 79.\n\nPretpostavljam da od 7 značajki jednu trebamo izbaciti jer su iz x5 x6 i x7 možemo jednu dobiti pomoću druge dvije. Također pretpostavka da štednja u kunama i devizna štednja u eurima nije isto ofc nego 2 računa.\n\n![](assets/2021-08-20/00004.png)",
      "votes": {
        "upvoters": [
          "vidraKida (Ζ ε ύ ς)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "231499": {
      "poster": "Bananaking",
      "content": "![](assets/2021-08-21/00011.png)\n\nZadnji pokušaj prije nego prestanem postati jer nitko ovo ne uči za jesen ili ne prati temu: zna netko objasniti ovaj zadatak? Odgovor je pod D), pretpostavljam da ide funkcija preslikavanja 1 jer ima kvadratne članove a podaci iz D nisu linearno odvojivi. Ali zašto Perceptron? Zato što perceptron \"kažnjava samo netočno klasificirane primjere (za razliku od regresije)\" ?\n\nTakođer: ako netko ima riješene moodle zadatke ili koji ispit bilo bi super to uploadati, bar dok su ispiti ovako više računski na zaokruživanje možda se i da nešto naučiti po primjerima a ne doktoriranjem teorije prije prelaska na zadatke.",
      "votes": {
        "upvoters": [
          "vidraKida (Ζ ε ύ ς)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "231526": {
      "poster": "nd49261 (SimpleRick)",
      "content": "@Bananaking#231499 \n\nOdgovor bi trebao biti D (perceptron i preslikavanje PHI 1 ).\n\nMoje objašnjenje je da perceptron i logistička regresija moraju imati linearno odvojive podatke kako bi učenje konvergiralo (kada podaci nisu linearno odvojivi radi se preslikavanje u prostor di su linearno odvojivi) .\n\nprvo preslikavanje ne radi ništa, samo dodaje dummy značajku i to je obična matrica dizajna.\n\nTreče preslikavanje je u prostor s 3 dimenzije (interakcijska značajka je x1x2 je uvijek nula zato što je ili x1 ili x2 u svakom primjeru nula). podaci su i dalje linearno neodvojivi.\n\nDrugo preslikavanje rješava problem linearnosti ( pretpostavljam zbog kvadratnih baznih funckija ) i onda učenje perceptrona može konvergirati.",
      "votes": {
        "upvoters": [
          "vidraKida (Ζ ε ύ ς)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "231528": {
      "poster": "Bananaking",
      "content": "@nd49261#231526 Ok ali zašto ne bi bilo LR i phi1 preslikavanje?",
      "votes": {
        "upvoters": [
          "vidraKida (Ζ ε ύ ς)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "231531": {
      "poster": "nd49261 (SimpleRick)",
      "content": "@Bananaking#231528 \n\nNisam dobro pogledao odgovore, ne znam zašto nije LR",
      "votes": {
        "upvoters": [
          "vidraKida (Ζ ε ύ ς)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "231538": {
      "poster": "nd49261 (SimpleRick)",
      "content": "Ovo je iz jednog prijašnjeg odgovora, mislim da je to odgovor na naše pitanje:\n\n\"Drugi zadatak:\n\nNadam se da si rjesavao drugi labos koji se ticao logisticke regresije, gdje si uzeo wTx i omotao to oko sigmoide. E sad, sigmoida je poprilicno nelinearna, ali granica koju si (trebao) dobiti je linearna. Zasto je tome tako? Zato jer su ti podaci unutar sigmoide linearni. Dakle, ako ti je funkcija PHI linearna, dobit ces linearnu granicu. Ako je nelinerna, i granica ce bit (perhaps surprisingly) nelinearna. Also, ako ti je PHI nelinearna, a aktivacija f linearna, ponovo granica nije linearna. Podaci su kljuc!\"\n\nZaključak: nelinearna aktivacijska funkcija(sigmoida) i nelinearni PHI daju nelinearnu granicu i zbog toga LR ne može konvergirati?",
      "votes": {
        "upvoters": [
          "vidraKida (Ζ ε ύ ς)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "231562": {
      "poster": "nd49261 (SimpleRick)",
      "content": "@Bananaking#231102 \n\nPeta bilješka na kraju skripte Osnovni koncepti:\n\n\"Razlicite hipoteze imat ce razlicite vektore parametara θ. Medutim, razliciti vektori parametara θ ne\n\nmoraju nuzno davati razlicite hipoteze. Na primjer, ako je ulazni prostor diskretan (npr. X = N\n\nn), onda mozemo imati pravce koji su malo razliciti (dakle parametri θ im se razlikuju), ali ipak daju\n\nidenticnu klasifikaciju primjera u dvije klase, tj. funkcija h je identicna (kako je uobicajeno, jednakost\n\nfunkcije ovdje definiramo ekstenzionalno: dvije funkcije su jednake ako jednako preslikavaju elemente\n\niz domene u kodomenu. To znaci da razliciti θ mogu dati identicne funkcije\"\n\nDimenzija prostora parameta je 4, ulazni prostor je diskretan (ima 8 različitih ulaza) i bitno je samo jel h(x) veći ili manji od 0(sve hipoteze koje za tih 8 ulaznih primjera daju iste klasifikacije su zapravo jedna hipoteza) \n\nU principu je pitanje na koliko načina možeš klasificirati tih 8 ulaza u dvije klase i to možeš napraviti na 2^8 načina (256)\n\nod klasifikacije 0,0,0,0,0,0,0,0 do klasifikacije 1,1,1,1,1,1,1,1  i sve između.\n\nJa sam si ovako protumačio zadatak, ne znam jel točno moje razmisljanje.",
      "votes": {
        "upvoters": [
          "vidraKida (Ζ ε ύ ς)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "231682": {
      "poster": "vidraKida (Ζ ε ύ ς)",
      "content": "@nd49261#231562 \n\n@Bananaking#231499 \n\nja radim za jesen, ako ste za imam neki online dokument pa mozemo tamo komentirat i radit",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "231683": {
      "poster": "Bananaking",
      "content": "@vidraKida#231682 Može naravno, ne znam koliko ću biti od koristi ali 3 glave bolje od jedne",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "231686": {
      "poster": "vidraKida (Ζ ε ύ ς)",
      "content": "@Bananaking#231683 https://docs.google.com/document/d/1D-WwT4eT4aakrNXwPqBoB6vshss4pHdXspTbfrpZqLM/edit",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "231687": {
      "poster": "vidraKida (Ζ ε ύ ς)",
      "content": "Planiram rjesavat moodle i ispite i onda probat zadace",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "231690": {
      "poster": "Ellie",
      "content": "@Bananaking#231279 Sorry, ne sjecam se vise kako su se rjesavali takvi zadatci. Your guess is as good as mine :/",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "231772": {
      "poster": "vidraKida (Ζ ε ύ ς)",
      "content": "@Bananaking#231279 kombinacije parova i trojki gledas ovako n!/((n-2)!x2! I za trojke n!/((n-3)!x3!",
      "votes": {
        "upvoters": [
          "Bananaking"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "231782": {
      "poster": "vidraKida (Ζ ε ύ ς)",
      "content": "@Bananaking#231279 pokusavao sam ga rijesit ali uvijek dobijem tipa 71 ili tako nesto blizu... ugl kak sam radio makno sam 2 znacajke za stednju i za preostala dugovanja. Dobio sam 5 + 5 + 1 ona klasika i 60 u kombinacijama ovakvim... sad ako ti to ista pomaze da skuzis nkj jebeno bi bilo da mi objasnis haha",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "233005": {
      "poster": "DocHoc",
      "content": "zna itko kako se rješavaju 3. i 9. zadatak iz MI-a?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "233452": {
      "poster": "vidraKida (Ζ ε ύ ς)",
      "content": "Jel mi moze netko pomoc oko 1., 4., 11. Zadatka iz zira? 11. Sam rjesavao kao i ostale tog tipa ali nikako ne mogu dobit dobre vrijednosti",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "235919": {
      "poster": "Bananaking",
      "content": "@vidraKida#231686 jel bilo šta od ovoga na kraju? još uvijek mi je access denied",
      "votes": {
        "upvoters": [
          "kix7 (Fish99)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "236455": {
      "poster": "Bananaking",
      "content": "@nd49261#231562 \n\n@Bananaking#231102 \n\nPitao sam ovdje sličan primjer ali moram priznati da mi nije jasno kako rješavati općeniti slučaj ovakvog zadatka. Evo primjer iz ovogodišnjeg međuispita. Pokušao sam razmišljati \"na koliko načina možemo klasificirati ulazne primjere\", kako je X = {0,1}^3 njih ima 2^3 = 8, mislio sam možda za prvu zagradu x1 može biti manji od fi 1,1, između ili veći, isto za drugu i treću zagradu pa nešto tipa 3C1 + 3C1 + 3C1 = 9 + ?\n\n![](assets/2021-09-12/00026.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "241723": {
      "poster": "lovro (l123)",
      "content": "Jel rješio neko 5. iz zadataka za učenje?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "241737": {
      "poster": "lincthesinc17",
      "content": "može neko stavit postupke kak je rješio?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "241821": {
      "poster": "tomekbeli420",
      "content": "@\"l123\"#p241723 \n\nove zadatke za učenje su starije kolege riješile\n\nhttps://fer.studosi.net/d/2200-struce-o-predmetu/3\n\njedino što se tiče njihovog 5a sam napisao\n\nPogreška hipoteze je očekivanje funkcije gubitka nad distribucijom primjera [imath]\\mathbf{x}[/imath] iz prostora primjera [imath]\\mathcal{X}[/imath] (dakle ne samo iz skupa primjera [imath]\\mathcal{D}[/imath] koji je statistički gledano samo uzorak iz distribucije). U praksi je problem što ta distribucija najčešće nije poznata.\n\n@\"lincthesinc17\"#p241737 \n\nKoji točno te zanima? One sve za vježbu imaš na onom google docu.",
      "votes": {
        "upvoters": [
          "lovro (l123)",
          "pingvinka"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "242040": {
      "poster": "rolotex (brr)",
      "content": "![](assets/2021-10-08/00009.png)\n\nSto znaci ova tocka pokraj X",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "242424": {
      "poster": "Bananaking",
      "content": "![](assets/2021-10-10/00005.png)\n\nKoja je razlika između A) i B)? Znači li to da za neke parametre theta ne postoje funkcije h?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "242436": {
      "poster": "InCogNiTo124",
      "content": "@\"tomekbeli420\"#p241821 sta se tice google doca, slobodno pisite u njega svoja rjesenja i pitanja, to je dobro mozda pomognete kasnije nekom ko bide imo ista pitanja",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "242443": {
      "poster": "Joji",
      "content": "@\"Bananaking\"#p242424 Mislim da je stvar u tome da je moguće za različite vrijednosti parametara ponekad dobiti iste hipoteze. Npr ako imaš model [imath]h(\\textbf x; \\boldsymbol \\theta) = |\\boldsymbol\\theta^\\top\\textbf x|[/imath], dobivaš istu hipotezu za [imath]\\boldsymbol\\theta[/imath] i [imath]\\boldsymbol-\\boldsymbol\\theta[/imath]. S druge strane, kad imaš različite hipoteze onda sigurno imaš različite parametre jer s istim parametrima moraš dobit istu hipotezu.",
      "votes": {
        "upvoters": [
          "bodNaUvidima",
          "nikace (AeIoU)",
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "242460": {
      "poster": "[deleted]",
      "content": "![](assets/2021-10-10/00008.png)\n\nmože netko objasnit što znače ove definicije? konkretno imam problema sa shvaćanjem desnog dijela svake (u zagradama)",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "242569": {
      "poster": "Rene",
      "content": "@\"Todd Chavez\"#p242460 \n\nZa hipotezu s vrijedi da je konzistentna, i da je svaka druga konzistentna hipoteza od koje je ona općenitija ili jednaka ujedno općenitija ili jednaka od s -  one su jednake. Dakle, s su najspecifičnije hipoteze. Analogno su G najopćenitije. Nekakva \"analogija\" koja bi ti možda mogla pomoć:  Neka je H multiskup (skup s dozvoljenim duplikatima), npr. H = {1, 2, 7, 5, 1, 7}.  s su oni brojevi za koje vrijedi da ako je s >= h, onda je i h >=s. Dakle to su najmanji element (ili više jednakih): S = {1, 1}. Slično bi u G završili brojevi za koje vrijedi ako je h >= g, onda je i g >= h, tj. H = {7, 7}",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "242879": {
      "poster": "rolotex (brr)",
      "content": "Jel netko rjesavao sve zadatke iz Regresije?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "243919": {
      "poster": "Bananaking",
      "content": "Ne znam jel zadan za zadaću ili ne (moodle trenutno ne radi) ali može li netko staviti rješenje 3. zadatka iz zadataka za učenje iz vježbe Regresija 1 ?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "243994": {
      "poster": "tomekbeli420",
      "content": "@\"Bananaking\"#p243919 Imaš u skripti za to gradivo, (P03 - Regresija) na stranici 12, ova 3 matematička izraza + objašnjenje ispod zadnjeg izraza.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "244030": {
      "poster": "BillIK",
      "content": "_Je li pristranost koja\n\nproizlazi iz odabira modela dovoljna za jednoznačnu klasifikaciju primjera iz D?_\n\nšto bi značila jednoznačna klasifikacija?",
      "votes": {
        "upvoters": [
          "yabk"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "244053": {
      "poster": "tomekbeli420",
      "content": "@\"BillIK\"#p244030 to pitanje je i meni bilo malo sumnjivo, jer primjeri iz [imath]\\mathcal{D}[/imath] su već klasificirani.\n\nJa mislim da se pita može li se odrediti jedinstvena hipoteza koja **ispravno** klasificira sve primjera iz [imath]\\mathcal{D}[/imath], dakle da je konzistentna i da uz to ne postoji više od jedne mogućnosti kako klasificirati neviđene primjere (primjere iz skupa [imath]\\mathcal{X} \\setminus \\mathcal{D}[/imath]).\n\nOdnosno, je li skup [imath]VS_{\\mathcal{H},\\mathcal{D}}[/imath] ima jedan ili više elemenata",
      "votes": {
        "upvoters": [
          "steker",
          "yabk"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "244162": {
      "poster": "yabk",
      "content": "Moze li netko pls objasniti 3. d) iz zadataka za ucenje (Osnovni koncepti)?\n\nGledam u ovaj doc rijesenih domacih zadaca, ali mi nije jasno kakvog smisla ima linearan model nad X = {0,1}^3 i kako dodemo do te dvije konkretne klasifikacije koje se spominju",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "244195": {
      "poster": "nikace (AeIoU)",
      "content": "@\"bruuum\"#p242040 ne znam je li netko već odgovorio.. ali je na predavanju točka pročitana kao izraz \"za koju vrijedi\" konkretnije \"x iz X za kojeg vrijedi...\"\n\nnadam se da nisam u krivu :)",
      "votes": {
        "upvoters": [
          "rolotex (brr)",
          "yabk"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "244285": {
      "poster": "Sulejman",
      "content": "@\"yabk\"#p244162 Nije ni meni bilo na prvu jasno pa sam si [geometrijski ](https://www.geogebra.org/3d/kwvvjjdc) nacrtao. Poanta je da (0, 0, 1) može pripadati bilo kojoj od dvije klase, jer postoje dvije ravnine (što su hipoteze) koje tu točku mogu smjestiti u jednu ili u drugu klasu. \n\nZnači sve točke koje su s jedne stane ravnine pripadaju prvoj klasi, a ove s druge strane ravnine pripadaju drugoj.",
      "votes": {
        "upvoters": [
          "yabk"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "244310": {
      "poster": "421blazeitfgt",
      "content": "Moze netko objasniti ovaj?\n\n![](assets/2021-10-16/00014.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "244362": {
      "poster": "tomekbeli420",
      "content": "@\"421blazeitfgt\"#p244310 \n\nu zadacima kod kojih je prostor primjera konačan, što je kod nas slučaj jer [imath]\\mathcal{X} = \\left\\{0, 1\\right\\}^3[/imath] je skup svih uređenih trojki nula ili jedinica i ima ih 8, korisno je vizualizirati kako izgledaju svi mogući primjeri u prostoru primjera.\n\nZbog toga što se radi o uređenim trojkama, jasno je da svaki primjer [imath]\\mathbf{x}[/imath] iz prostora primjera ima tri značajke: [imath]\\mathbf{x} = \\left(x_1, x_2, x_3\\right)[/imath]. Svih 8 mogućih primjera iz prostora primjera [imath]\\mathcal{X}[/imath] se tada mogu prikazati na ovakvoj kocki:\n\n![](https://i.imgur.com/EsP0S2e.png)\n\nE sad na slični način treba vizualizirati zadani model. Vidimo da se radi o binarnoj klasifikaciji, jer se koristi funkcija [imath]\\mathbf{1} \\{...\\}[/imath] sa nekim predikatom unutar vitičastih zagrada. E sad vidimo da taj predikat\n\n gleda svaku značajku posebno, i za svaku značajku testira nalazi li se unutar nekog intervala zadanih sa parametrima, i onda sve to poveže sa logičkim I (znak [imath]\\wedge[/imath]).\n\nAlso mali komentar, nije mi jasno zašto su stavili da je prostor parametara [imath]\\boldsymbol{\\theta} = \\mathbb{R}^6[/imath] a pojedine jednodimenzijske parametre označili sa dvama indeksima npr [imath]\\theta_{1,2}[/imath], no dobro... Bitno je samo da postoji 6 parametara, i svake dvije su za granice intervala jedne značajke.\n\nPretpostavimo da su parametri takvi da je interval \"ispravan\" pod navodnicima (da donje granice nisu veće od gornjih, npr da se ne desi situacija da je [imath]\\theta_{1,1} = 1.5[/imath] i [imath]\\theta_{1,2} = 0.5[/imath]), jer u suprotnom će onda takav predikat uvijek dati laž (jer evo za ove dvije thete bi se u predikat našao [imath]1.5 < x_1 < 0.5[/imath], što je naravno uvijek laž jer ne postoji nijedan realan broj koji je veći od 1.5 i manji od 0.5) i takva hipoteza onda sve primjere klasificira kao negativne (dakle sa nulama).\n\nE onda u slučaju da su parametri dobri, možemo vizualizirati model na ulaznom prostoru.\n\nPa ako malo bolje razmisliš, primijetit ćeš da se radi o kvadru koji ima sve bridove i plohe paralelne sa ovom kockom koju sam nacrtao gore na slici.\n\nEvo primjera vizualizacije jedne od hipoteza, nisam baš najvještiji u crtanju 3D oblika:\n\n![](https://i.imgur.com/EN4zDRK.png)\n\nOvo bi odgovaralo situaciji npr da su parametri ovakvi (otprilike):\n\n[math]\\theta_{1,1} = 0.8 \\quad \\theta_{1,2} = 1.5 \\quad \\theta_{2,1} = -0.5 \\quad \\theta_{2,2} = 1.8 \\quad \\theta_{3,1} = -0.2 \\quad \\theta_{3,3} = 0.25[/math]\n\nI onda se pozitivno klasificira sve što je unutar takvog kvadra. Ovakva hipoteza koju sam naveo kao primjer bi onda pozitivno klasificirala [imath](1, 0, 0)[/imath] i [imath](1, 1, 0)[/imath]. Strane i bridovi tog kvadra su paralelne sa onima iz kocke jer se testira svaka značajka pojedinačno. Jako bitno je za primijetiti da iako postoji neprebrojivo beskonačno mnogo parametara iz [imath]\\mathbb{R}^6[/imath], hipoteza postoji konačno mnogo (čak i bez ovakvog modela) jer puno različitih realnih brojeva daju efektivno iste hipoteze. Npr, ova hipoteza koju sam naveo bi bila ista kao i ona sa parametrima:\n\n[math]\\theta_{1,1} = 0.81 \\quad \\theta_{1,2} = 1.51 \\quad \\theta_{2,1} = -0.51 \\quad \\theta_{2,2} = 1.81 \\quad \\theta_{3,1} = -0.21 \\quad \\theta_{3,3} = 0.26[/math]\n\nŠto je mrvicu pomaknuti kvadar u odnosu na prošli primjer.\n\nZašto je ista? Hipoteza je u matematičkom smislu funkcija, i ona za svaki [imath]\\mathbf{x}[/imath] iz prostora primjera [imath]\\mathcal{X}[/imath] pljuje oznaku. Pa ako dvije hipoteze za svaki [imath]\\mathbf{x}[/imath] daju istu oznaku, onda su te dvije hipoteze iste i onda ih ne brojimo dvaput kod prebrojavanja hipoteza. Razlog tomu je činjenica da je prostor primjera [imath]\\mathcal{X}[/imath] konačan, pa onda postoji konačno mnogo klasifikacijskih funkcija (hipoteza) i bez modela, a model ih još ograniči s obzirom da nije moguće postići sve moguće hipoteze.\n\nDobro vizualizirali smo model (odnosno hipoteze iz tog modela) i sad treba prebrojiti koliko različitih klasifikacija postoji za takav model, odnosno koliko različitih funkcija (Booleovih funkcija u ovom slučaju jer imamo binarnu klasifikaciju) se može dobiti ovakvim modelom.\n\nE sad vizualizacija je dobra jer možemo problem svesti na sljedeći ekvivalentan problem:\n\nKoliko ima različitih \"konfiguracija\" preklapanja kocke i kvadra, gdje se naravno vrhovi kocke (primjeri iz [imath]\\mathcal{X}[/imath]) razlikuju?\n\nA ovo prebrojiti je relativno lagano jer znamo da kvadar ima bridove i plohe paralelne sa onima od kocke.\n\nPrva dva slučaja su kad se svi primjeri klasificiraju isto. Ako kvadar skroz \"fula\" kocku (ili su parametri \"neispravni\" kako smo spomenuli ranije), onda će svi primjeri biti klasificirani kao 0, a ako se kocka nalazi potpuno unutar kvadra, onda se svi primjeri klasificiraju kao 1. To su 2 hipoteze.\n\nOnda uzmimo slučaj kako kvadar u svojoj unutrašnjosti obuhvaća samo jedan vrh. Sad ovisno o tome koji vrh obuhvati, taj primjer će klasificirati sa 1, a ostale sa 0. Takvih mogućnosti imamo 8 jer ima 8 različitih vrhova, dakle to je dodatnih 8 hipoteza.\n\nOnda uzmimo slučaj kako kvadar u svojoj unutrašnjosti obuhvaća jedan brid. Pokazni primjer je jedan od takvih hipoteza. Taj brid koji obuhvaća ta 2 vrha/ulaznih primjera će klasificirati sa 1, ostale sa 0. Bridova ima 12, dakle takvih mogućnosti ima dodatnih 12, dakle dodatnih 12 hipoteza.\n\nDalje pretpostavljam kužiš kako.\n\nOnda uzmimo slučaj kako kvadar u svojoj unutrašnosti obuhvaća cijelu jednu stranu. Strana ima 6, dakle 6 dodatnih hipoteza.\n\nPozbrojimo sve to,\n\n1+1+8+12+6 = 28.",
      "votes": {
        "upvoters": [
          "123 (FERella)",
          "Arya",
          "Daho_Cro",
          "Dootz",
          "Fica (Prof)",
          "Gulbash",
          "Heklijo (Geralt of Rivia)",
          "JoKing",
          "Lyras",
          "Me1 (Me)",
          "PaleAle",
          "Pleomax",
          "Sulejman",
          "SuperSaiyano",
          "SuperSjajan3",
          "TheCrimsonChin",
          "Vonj",
          "ZekoHop",
          "boogie_woogie (nika_1999)",
          "cajaznun",
          "fooFighter",
          "gladiator",
          "indythedog",
          "jobi (azex)",
          "luka6 (ugrijaniRadijator)",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
          "sheriffHorsey",
          "spampers (majmunska boginja)",
          "steker",
          "yabk",
          "zastozato (studoš)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "244410": {
      "poster": "lincthesinc17",
      "content": "@\"tomekbeli420\"#p241821 na kojem google docu",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "244429": {
      "poster": "tomekbeli420",
      "content": "@\"lincthesinc17\"#p244410 https://docs.google.com/document/d/1POm4KkLzv3M_R1Lf7m3nnvp-2MJ-JYHRMKRJNCprJwU/edit#heading=h.jwsd30gipi6",
      "votes": {
        "upvoters": [
          "Gulbash",
          "lincthesinc17"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "244586": {
      "poster": "rolotex (brr)",
      "content": "Je li netko rj 3. zad - zad s ispita iz 03 Regresija?",
      "votes": {
        "upvoters": [
          "Ollie",
          "[deleted]",
          "cajaznun"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "244758": {
      "poster": "[deleted]",
      "content": "što je točno rang matrice i kako ga odrediti? jel prijevod toga na engleskome `span` ? jel to samo broj razlicitih stupaca 😓",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "244763": {
      "poster": "Bananaking",
      "content": "@\"Todd Chavez\"#p244758 Skripta \"3: Regresija\", Bilješke, bilješka broj 9, stranica 14. tl;dr rang matrice je broj linearno nezavisnih stupaca matrice. Ali pročitaj što piše, imaš i primjere.",
      "votes": {
        "upvoters": [
          "[deleted]",
          "[deleted]",
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "244797": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Todd Chavez\"#p244758 ~~Na engleskom se zove \"range\" ili \"column space\"~~\n\nEDIT: Na engleskom se zove \"rank\". Vezano je uz \"range\" ali nije ista stvar, pardon.",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "Sulejman"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "245619": {
      "poster": "[deleted]",
      "content": "![](assets/2021-10-21/00015.png)\n\nmoze netko objasniti ovaj?",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "245623": {
      "poster": "Emma63194",
      "content": "@\"Todd Chavez\"#p245619 \n\n@\"InCogNiTo124\"#p90565",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "245630": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Todd Chavez\"#p245619 Ja sam ovako shvatio.\n\nŠum nije relevantan za zadatak jer piše da ga je malo. Želiš minimizirat funkciju pogreške koja se računa\n\n(y+h(x))^2. E sad, tvoje oznake već približno leže na pravcu (jer je šum malen), pa je pitanje možeš li smislit drugi pravac koji daje vrijednosti koje će minimizirat fju pogreške? E to će biti pravac koji simetričan s orginalnim pravcem (tj oznakama) u odnosu na x os.\n\ntj (h(x) + y) = 1-2x -1+2x = 0\n\n>!  ![](assets/2021-10-21/00016.jpg)",
      "votes": {
        "upvoters": [
          "[deleted]",
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "245718": {
      "poster": "vidraKida (Ζ ε ύ ς)",
      "content": "@\"Todd Chavez\"#p245619 ![](assets/2021-10-22/00003.jpg)",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "245860": {
      "poster": "steker",
      "content": "![](assets/2021-10-22/00013.jpg)\n\nMi u ovom zadatku ne mozemo zakljucit nis o tome koji model bi bolje generaliziro od kojeg zbog velikog suma? Ako sam dobro skuzila",
      "votes": {
        "upvoters": [
          "Ducky"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "245872": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"steker\"#p245860 Ja bih prije rekao da ti šum govori da će model s većim kapacitetom imati manju pogrešku, a da mogu jednako loše generalizirati zato što su neodgovarajuće aproksimacije (2 je premalo, 5 je previše). Ovisno o uzorcima može ti se underfittati na [imath]H_{2,0}[/imath] i overfittati na [imath]H_{5,0}[/imath], ali bez uvida u uzorke ne možeš reći koji će otići dalje od odgovarajuće stupnja 3. Nije nužno stupanj 5 jer je za 2 udaljen od stupnja 3, dok je stupanj 2 udaljen za 1 od stupnja 3. Nit možeš predviđati da će šum bit takav da će to srediti ekstra 2 stupnja polinoma stupnja 5 bolje nego regularizacija koju pruža polinom stupnja 2.",
      "votes": {
        "upvoters": [
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "245923": {
      "poster": "steker",
      "content": "@\"BDSMićo\"#p245872 k tenks",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "246121": {
      "poster": "[deleted]",
      "content": "@\"Precious Bodily Fluids\"#p245630 možeš li još pojasniti zašto nam to točno treba biti = 0 kad zelimo minimizirati? pošto to ipak nije derivacija",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "246123": {
      "poster": "angello2",
      "content": "@\"Todd Chavez\"#p246121 zelis minimizirat pogresku, pogreska ne moze bit negativna tako da je najmanje sta moze bit = 0",
      "votes": {
        "upvoters": [
          "[deleted]",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "246826": {
      "poster": "InCogNiTo124",
      "content": "> @\"angello2\"#p246123 pogreska ne moze bit negativna tako da je najmanje sta moze bit = 0\n\nPazi samo, pogreska opcenito moze biti manja od 0.\n\nevo recimo jedan cest primjer: ucis vector embedding recimo slike, i zelis da ti model vrati slicne vektore za slicne slike, a razlicite vektore za razlicite slike. Slicnost vektora mjeris cosinusnom slicnoscu tako da MAKSIMIZIRAS cosine_similarity(model(x), y), odnosno MINIMIZIRAS -cosine_similarity(model(x), y). Ugl dogodit ce ti se da je minimum jednak -1.\n\nVecina loseva u strojnom, poput square loss iz labosa ali i l1, hinge, logistic i Mnogi Drugi imaju minimum u 0, al to nije nuzno, pa treba pazit",
      "votes": {
        "upvoters": [
          "angello2",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "247132": {
      "poster": "Artemis",
      "content": "Riješio netko 1.zad s ispita (linearni diskriminativni modeli)?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "247135": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Loki\"#p247132 Kojeg",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "247142": {
      "poster": "Artemis",
      "content": "@\"BDSMićo\"#p247135 \n\n![](assets/2021-10-27/00019.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "247146": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Loki\"#p247142 Koji je to ispit mislim",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "247148": {
      "poster": "[deleted]",
      "content": "@\"BDSMićo\"#p247146 to su skupljeni zadaci s intraneta, sekcija se zove ‘zadaci s ispita’",
      "votes": {
        "upvoters": [
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "247267": {
      "poster": "Sulejman",
      "content": "Zašto koristimo preslikavanje iz skupa primjera u skup oznaka?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "247325": {
      "poster": "steker",
      "content": "@\"Loki\"#p247142 jel tu w0 =0, w1=5, w2=-5",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "247328": {
      "poster": "Artemis",
      "content": "@\"steker\"#p247325 \n\nNe znam za w0 i w1, ali w2=-5 što je i točno rješenje zadatka.\n\nKako doći do toga?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "247346": {
      "poster": "tomekbeli420",
      "content": "@\"Sulejman\"#p247267 da bi podaci koji nisu bili linearni (odnosno nisu bili linearno odvojivi kod klasifikacije) u originalnom ulaznom prostoru postali linearni u tom preslikanom prostoru. Na taj način ne moramo mijenjati linearan model (model je i dalje linearan u težinama [imath]\\mathbf{w}[/imath]), nego potencijalnu nelinearnost možemo postići sa nelinearnim preslikavanjem [imath]\\boldsymbol{\\phi}[/imath].\n\nSkripta Regresija II, paragraf 1.2\n\n@\"Loki\"#p247142 \n\nZnamo da su težine takve da je maksimizirana udaljenost. Za predznačenu udaljenost primjera od hiperravnine granice modela znamo da se računa prema izrazu [imath]d = \\frac{h \\left(\\mathbf{x}\\right)}{\\left\\| \\mathbf{w} \\right\\|}[/imath] gdje je [imath]\\mathbf{w}[/imath] vektor težina ali sa isključenim [imath]w_0[/imath], dakle u našem slučaju [imath]\\mathbf{w} = \\left(w_1, w_2\\right)[/imath]. E ali kod nas je induktivna pristranost takva da se maksimizira udaljenost ali da se ne gleda predznak, dakle onda moramo gledati apsolutnu vrijednost: [imath]d = \\left| \\frac{h \\left(\\mathbf{x}\\right)}{\\left\\| \\mathbf{w} \\right\\|} \\right|[/imath]\n\ndakle potrebno je maksimizirati izraz (označio sam ga sa [imath]D[/imath] reda radi)\n\n[math]D = \\sum_{i=1}^{N} \\left| \\frac{h \\left(\\mathbf{x}^{(i)}\\right)}{\\left\\| \\mathbf{w} \\right\\|} \\right|[/math]\n\nE sad mi znamo da za oba primjera vrijedi [imath]y \\cdot h \\left(\\mathbf{x}\\right) = 5[/imath], pa prema tome odmah možemo saznati koliko iznose vrijednosti hipoteze (linearnog modela prije nego se uplete klasifikacija) za svaki primjer ako uvrstimo vrijednosti njihovih oznaka [imath]y[/imath] (možemo uvrstiti i vrijednosti njihovih značajki):\n\n[math]h \\left(\\mathbf{x}^{(1)}\\right) = w_1 x_1^{(1)} + w_2 x_2^{(1)} + w_0 = w_1 \\cdot 1 + w_2 \\cdot 0 + w_0 = w_1 + w_0 = 5 \\\\\nh \\left(\\mathbf{x}^{(2)}\\right) = w_1 x_1^{(2)} + w_2 x_2^{(2)} + w_0 = w_1 \\cdot 0 + w_2 \\cdot 1 + w_0 = w_2 + w_0 = -5[/math]\n\nSamo iz dobivenih linearnih jednadžbi sa nepoznanicama [imath]w_0, w_1, w_2[/imath] ne možemo odrediti jedinstveno rješenje, no moramo iskoristiti činjenicu da rješenje maksimizira udaljenosti (izraz [imath]D[/imath]) hiperravnine od primjera. E sad uvrstimo podatke (iznose hipoteza za svaka 2 primjera) u izraz [imath]D[/imath]:\n\n[math]D = \\frac{5}{\\left\\| \\mathbf{w} \\right\\|} + \\frac{5}{\\left\\| \\mathbf{w} \\right\\|} = \\frac{10}{\\sqrt{{w_1}^2 + {w_2}^2}}[/math]\n\nI to je potrebno maksimizirati uz uvjete da vrijede one 2 linearne jednadžbe gore. Pa jasno je da maksimizacija [imath]D[/imath] se svodi na minimizaciju zbroja kvadrata dvaju težina (minimizacija jer se zbroj kvadrata nalazi u nazivniku, a minimizacija korijena se svodi na minimizaciju onog unutar korijena pod uvjetom da je to unutar korijena pozitivno, što naravno jest slučaj jer se radi o zbroju kvadrata).\n\nInače bismo morali minimizirati funkciju dviju varijabli [imath]w_1[/imath] i [imath]w_2[/imath], ali one linearno ovise o [imath]w_0[/imath] (iz onih linearnih jednadžbi) pa ih možemo tako zapisati. Dakle onda se zadatak svodi na minimiziranje izraza [imath]\\left(5 - w_0\\right)^2 + \\left(-5 - w_0\\right)^2[/imath] što je dost trivijalno\n\nIzraz se svede na (kad se kvadriraju zagrade) [imath]2 {w_0}^2 + 50[/imath] što je parabola sa tjemenom u [imath]w_0 = 0[/imath]. Dakle onda je rješenje [imath]w_2 = -5 - w_0 = -5 - 0 = -5[/imath].",
      "votes": {
        "upvoters": [
          "Ardura (Maddy)",
          "Artemis",
          "Ollie",
          "SuperSaiyano",
          "Valentino",
          "WickyWinslow",
          "bodNaUvidima",
          "indythedog",
          "nnn (dinoo)",
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "247350": {
      "poster": "steker",
      "content": "@\"tomekbeli420\"#p247346 jel se moze w0=0 zakljucit iz toga da bi mozda(?) ravnina trebala prolaziti kroz ishodiste kako bi se ostvario taj maksimum udaljenosti jednog i drugog primjera",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "247353": {
      "poster": "tomekbeli420",
      "content": "@\"steker\"#p247350 Ne. Recimo da zadatak nije specificirao da mora za oba primjera vrijediti [imath]y \\cdot h \\left(\\mathbf{x}\\right) = 5[/imath] (odnosno da ne znamo da to vrijedi). Maksimum udaljenosti se onda može postići neovisno o tome koliki je [imath]w_0[/imath]. Zašto?\n\nPa ako raspišeš opet onaj izraz za [imath]D[/imath] uzimajući u obzir da je jedan primjer pozitivan drugi negativan, dobiješ\n\n[math]D = \\frac{h \\left(\\mathbf{x}^{(1)}\\right) - h \\left(\\mathbf{x}^{(2)}\\right)}{\\| \\mathbf{w} \\|} = \\frac{w_1 + w_0 - \\left(w_2 + w_0\\right)}{\\sqrt{{w_1}^2 + {w_2}^2}} = \\frac{w_1 - w_2}{\\sqrt{{w_1}^2 + {w_2}^2}}[/math]\n\nŠto ne ovisi o [imath]w_0[/imath] (jasno, ne smije se dogoditi da su težine takve da se netočno klasificiraju primjeri). Maksimizatora ovog izraza ima beskonačno, konkretno rješenje su sve težine za koje vrijedi [imath]w_2 = -w_1[/imath], što ima smisla i geometrijski ako nacrtaš primjere u prostoru primjera, pa onda da bi se maksimizirale udaljenosti, pravac granice mora biti okomit na spojnicu između dva primjera iz skupa primjera. E sad kroz koji dio spojnice prolazi je nebitno, maksimizirana udaljenost (zbroj) će biti [imath]\\sqrt{2}[/imath], pa je stoga nebitno koliki je [imath]w_0[/imath]. No ako uz to uključiš informaciju da mora vrijediti [imath]y \\cdot h \\left(\\mathbf{x}\\right) = 5[/imath], tek onda možeš sa sigurnošću reći da je [imath]w_0 = 0[/imath].",
      "votes": {
        "upvoters": [
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "248429": {
      "poster": "lucylu",
      "content": "@\"tomekbeli420\"#p247346 \n\nzašto se gleda suma d-ova kod maksimizacije?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "248432": {
      "poster": "tomekbeli420",
      "content": "@\"lucylu\"#p248429 jer zadatak kaže da model ima induktivnu pristranost tako da maksimizira udaljenost primjera od hiperravnine. Jedino što malo nije skroz jasno jest što točno se maksimizira između udaljenosti, nije eksplicitno rečeno da je zbroj.",
      "votes": {
        "upvoters": [
          "lucylu"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "248472": {
      "poster": "Bananaking",
      "content": "Linearni Diskriminativni Modeli, zadaća, 4. zadatak, \"Pozivajući se na skicu, odgovorite za koje će modele očekivanje gubitka biti veće od udjela pogrešnih klasifikacija\". Ne razumijem baš pitanje, jel odgovor isto što me pita zadnja 3 podzadatka kv gubitak jer kažnjava i točne?",
      "votes": {
        "upvoters": [
          "Sulejman",
          "WickyWinslow",
          "sheriffHorsey"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "248570": {
      "poster": "viliml",
      "content": "@\"tomekbeli420\"#p247346 @\"tomekbeli420\"#p247353 Ovo što računaš nije udaljenost primjera od hiperravnine, nego udaljenost između projekcija dva primjera na pravac okomit na hiperravninu.\n\nOno što se treba maksimizirati je minimalna udaljenost od bilo kojeg primjera do hiperravnine, dakle u našem slučaju pravac mora biti simetrala naša dva primjera.\n\nTo nam odmah daje [imath]w_0=0, w_1+w_2=0[/imath], i onda dalje lako.\n\nEDIT: ok, shvatio sam što ti je bila ideja. Ti si shvatio da se maksimizira *suma* udaljenosti primjera od hiperravnine. Ali zadatak nije tako zadan. Definicija udaljenosti između dva skupa točaka je minimum udaljenosti između bilo koje točke u prvom i bilo koje točke u drugom skupu.\n\nBespotrebno si si zakomplicirao život, ali ipak si dobio isto rješenje pa bravo.",
      "votes": {
        "upvoters": [
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "248614": {
      "poster": "tomekbeli420",
      "content": "@\"viliml\"#p248570 aha vidi stvarno, a bilo bi super kad bi u zadatku eksplicitno rekli da je minimum",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "248799": {
      "poster": "boogie_woogie (nika_1999)",
      "content": "Zna netko ovaj?\n\n![](assets/2021-11-01/00037.png)\n\nPretpostavljam da se a i b mogu eliminirati jer primjeri nisu linearno odvojivi, ali zašto je baš d, a ne c?",
      "votes": {
        "upvoters": [
          "blast"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "248950": {
      "poster": "viliml",
      "content": "@\"boogie_woogie\"#p248799 To se meni čini kao greška u zadatku, osim ako je neka kvaka s time da empirijska pogreška konvergira na neku veliku vrijednost dok još uvijek krivo klasificira.\n\nOvaj mi se isto čini kao greška. Topologija mreže je 10x4x3, Parametara je 40+12=52.\n\n![](assets/2021-11-02/00007.png)",
      "votes": {
        "upvoters": [
          "Lyras"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "249637": {
      "poster": "[deleted]",
      "content": "![](assets/2021-11-04/00000.png)\n\nmože li netko dodatno pojasniti ovaj dio? ni nakon njihovog objašnjenja mi nije jasno zašto se ovo događa *samo* za linearno odvojive probleme",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "249649": {
      "poster": "Rene",
      "content": "@\"Todd Chavez\"#p249637 Ako su primjeri linearno odvojivi, onda će se sigmoida stezati i praktički težiti prema obliku step funkcije jer možeš samo biti jako blizu nule za jednu klasu, a jako blizu jedinice za drugu klasu i strmi prelazak između njih (slika lijevo).\n\nAko nisu linearno odvojivi onda ne taj strmi prijelaz nije dobar jer će greška biti veća, pa sigmoida postaje \"blažeg\" prijelaza (slika desno)\n\n![](assets/2021-11-04/00002.png)\n\nMožda se nisam najbolje izrazio, ali mislim da je o tome Šnajder govorio na predavanju Logistička regresija 2 pred kraj.",
      "votes": {
        "upvoters": [
          "[deleted]",
          "blast"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "249659": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Todd Chavez\"#p249637 \n\nEvo wall of text struje svijesti, nadam se da pomogne.\n\nPrije svega, važno je shvatiti što se događa sa sigmoidom ako množiš njen ulaz sa faktorom alpha. Što više raste faktor alpha, to sigmoida postaje strmija ( 6. cjeline, str 2).\n\nSljedeće, potrebno je razumjeti da kada koristiš sigmoidu u logističkoj regresiji\n\n[math]\\sigma(w^Tx)[/math]\n\n w^T je ista stvar kao faktor alpha. Kako je on veći, to je sigmoida strmija.\n\nDalje je potrebno razumjeti gubitak unakrsne entropije. On kažnjava i ispravno i neispravno klasificirane primjere i  raste proporcionalno s razlikom izlaza modela i stvarne oznake primjera, tj. |[imath]y -  h \\left(\\mathbf{x}\\right)[/imath]|\n\n(vidi cjelinu 6, str 7.)\n\nAjmo sada pogledati što se događa s modelom koji već ispravno klasificira sve primjere. Dakle, primjeri su linearno odvojivi. Zašto on u daljnjoj optimizaciji nastavlja za neki faktor povećavati težine? Zato jer time ne mjenja granicu klasifikacije, a sigmoida postaje strmija. A kad sigmoida postane strmija, izlaz modela za sve pozitivne primjere pomakne se bliže 1, a za negativne bliže 0. Time se smanjuje gubitak, odnosno pogreška, a to je upravo ono što algoritam i želi.\n\nE sad, ako primjeri nisu linearno odvojivi, logička regresija neke primjere neće moći ispravno klasificirati. I sad zamisli da kreneš povećavati težine isto kao i gore. Opet bi sigmoida postala strma i davala vrijednosti blizu ili 0 ili 1. I sad recimo da postoji pozitivno označen primjer na pogrešnoj strani klasifikacijske granice. Za njega bi model dao [imath]h \\left(\\mathbf{x}\\right) \\approx 0[/imath] , što je potpuno krivo klasificirano i gubitak je velik, odnosno gubitak netočno klasificiranih primjera raste što je sigmoida strmija. U drugu ruku, pogreška za sve ispravno klasificirane primjere bi padala. Dakle kako mjenjaš strminu sigmoide, ispravno klasificiranim primjerima gubitak se smanjuje, a neispravnim se povećava.\n\nPoanta cijele priče je da kod linearno odvojih primjera funkciju pogreške uvijek možeš natjerati da teži u nula, a to postižeš jako strmom sigmoidom. Kod primjera koji nisu linearno odvojivi to ne možeš jer minimum funkcije pogreške nije 0 i težine nikad neće rasti nekontrolirano.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "Rene",
          "[deleted]",
          "angello2"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "249661": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Todd Chavez\"#p249637 Zapravo, mislim da će ti biti najjasnije ako probaš debuggat algoritam za najjednostavniji mogući linearno odvojiv i neodvojiv primjer, pa pogledaš kako se stvari ažuriraju.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "250504": {
      "poster": "viliml",
      "content": "@\"boogie_woogie\"#p248799 @\"viliml\"#p248950 \n\nMože drugo mišljenje?\n\nDa netko ili potvrditi da su zadatci krivo zadani ili objasni službeno rješenje?\n\nUskoro će rok za predaju.",
      "votes": {
        "upvoters": [
          "boogie_woogie (nika_1999)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "250510": {
      "poster": "Rene",
      "content": "@\"viliml\"#p250504 Ne bih rekao da je greška.\n\nA i B neće konvergirati jer primjeri nisu linearno odvojivi a koristi se perceptron.\n\nC ne konvergira jer logistička regresija (neregularizirana) ne konvergira za linearno odvojive primjere. Detaljnije objašnjenje:\n\nhttps://stats.stackexchange.com/questions/224863/understanding-complete-separation-for-logistic-regression\n\nD konvergira jer su primjeri linearno neodvovjivi.\n\nOvaj drugi zadatak ne dobijam ni tvoje ni njihovo rješenje, pa nisam siguran",
      "votes": {
        "upvoters": [
          "angello2"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "250527": {
      "poster": "Rene",
      "content": "@\"Rene\"#p250510 Evo, mislim da sam uspio i taj.\n\nIz modela vidimo da postoje 4 bazne funkcije [imath]\\phi_j[/imath] s tim da je [imath]\\phi_0(\\vec{x})=1 [/imath] pa ona nema parametara.\n\nOstale 3 su definirane kao [imath]\\phi_j(\\vec{x})=w_{j0} + w_{j1} x_1 + ... + w_{j10}x_{10} [/imath] dakle svaka ima 11 parametara.\n\nSvaka od 3 klase još ima svoj vektor [imath] \\vec{w_k} = (w_{k0}, w_{k1}, w_{k2}, w_{k3}) [/imath].\n\nUkupno je to onda 3 * 11 + 3 * 4 = 45 parametara.",
      "votes": {
        "upvoters": [
          "Ducky",
          "Ollie",
          "angello2",
          "matt (Matt)",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "250611": {
      "poster": "viliml",
      "content": "> @\"Rene\"#p250510 C ne konvergira jer logistička regresija (neregularizirana) ne konvergira za linearno odvojive primjere.\n\nTežine ne konvergiraju (teže beskonačnosti), ali empirijska pogreška i dalje konvergira prema nuli.\n\nAli istina da D također konvergira.\n\n@\"Rene\"#p250527 \n\nU zadatku piše da su bazne funkcije definirane kao \"skalarni produkt vektora značajki i vektora primjera\". Ako ignoriramo to što su vjerojatno htjeli reći \"vektora značajki i vektora *težina*\", to nalaže da imaju 10 parametara. Nije pisalo \"afina funkcija\". Ali ne bi me čudilo da je to isto njihova greška.\n\nTakođer nigdje nije rečeno da je nulta bazna funkcija konstanta, ali ok, recimo da je to zdravi razum kojeg ja nemam.",
      "votes": {
        "upvoters": [
          "Rene",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "250616": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"viliml\"#p250611 E ali uzmi u obzir da će ti se tih 10 značajki potencijalno proširiti dummy značajkom, takva je i praksa na predmetu. Trebalo bi urediti zadatak da ne piše ovo \"kao i na predavanju\", nego da piše konkretno o čemu se radi.",
      "votes": {
        "upvoters": [
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "250732": {
      "poster": "BillIK",
      "content": "Kako odabiremo najbolju stopu učenja? Ima čitava ona priča u skripti, ali postoji li neki kraći odgovor bez toliko teoretiziranja tipa. ona za koju će pogreška bit min/max?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "250740": {
      "poster": "Jale (čakijale)",
      "content": "Jel imao netko problema s Pycharmom i prikazivanjem grafova? Uopce mi se ne prikaze graf u Pycharmu nego se otvori novi smrznuti prozor (Not responding). Kad isti kod pokrecem u browseru, najnormalnije se prikazuje. Nisam uspio nista korisno naci na internetu",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "250766": {
      "poster": "rolotex (brr)",
      "content": "![](assets/2021-11-07/00009.png)\n\nZna netko ovaj",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "250844": {
      "poster": "jazzMassive",
      "content": "@\"Jale\"#p250740 zakomentiraj onaj red u prvoj celiji inline pylab, al nemoj zaboraviti vratiti kada pokazujes asistentu",
      "votes": {
        "upvoters": [
          "Jale (čakijale)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "250851": {
      "poster": "bodNaUvidima",
      "content": "@\"Rene\"#p250527 Gdje piše da se uzima da je nulta bazna funkcija konstantno preslikavanje značajki u 1? Ne mogu naći to u literaturi na intranetu niti se sjećam da je to naglašeno u videopredavanju.",
      "votes": {
        "upvoters": [
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "250866": {
      "poster": "BillIK",
      "content": "@\"BillIK\"#p250732  anyone?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "250870": {
      "poster": "bodNaUvidima",
      "content": "@\"BillIK\"#p250866 Idealna stopa učenja bi bila ona s kojom možeš izaći iz svakog lokalnog minimuma i uvijek završiti u globalnom, a to znati unaprijed je koliko znam nemoguće.",
      "votes": {
        "upvoters": [
          "BillIK"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "250872": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"BillIK\"#p250866 a nema bas neki definitan odgovor. Velike stope ucenja su u pravilu brze, ali ne nuzno jer ovisi kakve gradijente pogadas. Dodatno ako je stopa prevelika onda raste sansa da ti algoritam divergira. Uglavnom, trazis omjer brzine i tocnosti, koliko sam shvatio.",
      "votes": {
        "upvoters": [
          "BillIK"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "250882": {
      "poster": "Rene",
      "content": "@\"bodNaUvidima\"#p250851 nije nužno ali na ptedavanju je snajder rekao da je uobicajeno\n\nBar ja tako imam u biljeskama",
      "votes": {
        "upvoters": [
          "Ducky"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "250965": {
      "poster": "viliml",
      "content": "@\"bruuum\"#p250766 \n\n[math]-\\log(1-\\sigma(0.15+\\mathbf{w}^\\intercal\\mathbf{x}))=0.274 \\\\\n-\\log\\sigma(0.15+\\mathbf{w}^\\intercal(2\\mathbf{x})) =\\space?[/math]\n\n[math]1-\\sigma(0.15+\\mathbf{w}^\\intercal\\mathbf{x})=e^{-0.274} \\\\\n\\sigma(0.15+\\mathbf{w}^\\intercal\\mathbf{x})=1-e^{-0.274} \\\\\n\\mathbf{w}^\\intercal\\mathbf{x}=\\log(-\\frac{1-e^{-0.274}}{1-e^{-0.274}-1})-0.15 \\\\\n=\\log(\\frac{1-e^{-0.274}}{e^{-0.274}})-0.15 \\\\\n=\\log(e^{0.274}-1)-0.15[/math]\n\n[math]-\\log\\sigma(0.15+\\mathbf{w}^\\intercal(2\\mathbf{x})) = -\\log\\sigma(0.15+2\\mathbf{w}^\\intercal\\mathbf{x}) \\\\\n=-\\log\\sigma(0.15+2(\\log(e^{0.274}-1)-0.15)) \\\\\n=-\\log\\sigma(0.15+2(\\log(e^{0.274}-1)-0.15)) \\\\\n=-\\log\\sigma(2\\log(e^{0.274}-1)-0.15) \\\\\n\\approx 2.54[/math]",
      "votes": {
        "upvoters": [
          "Ollie",
          "cajaznun",
          "jobi (azex)",
          "matt (Matt)",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
          "rolotex (brr)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "251026": {
      "poster": "[deleted]",
      "content": "@\"Rene\"#p250510 možda sam ja omašio ceo fudbal, al zašto primjeri nisu linearno odvojivi? jel ih ne bi mogla odvojiti ploha y=0 npr.?",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "251033": {
      "poster": "Rene",
      "content": "@\"Todd Chavez\"#p251026 ne znam što misliš pod \"y\" ali s obzirom da je x2 = 0 onda možeš skicirat ove primjere s ovim preslikavanjima:![](assets/2021-11-07/00025.png)\n\n[imath]\\phi_0[/imath] i [imath]\\phi_2[/imath]  izgledaju kao lijeva slika (član x1x2 zanemaruješ jer je uvijek 0), a [imath]\\phi_1[/imath]  izgleda kao desna slika (zanemaruješ x2 i x2^2)",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "251035": {
      "poster": "[deleted]",
      "content": "@\"Rene\"#p251033 aa hvala ja sam gledao y kao vrijednosti i onda sam zapravo crtao (x, y) graf pa mi nije bilo jasno jer na njemu su odvojivi. tako nesto bi bilo ok za regresiju ili ni tada?",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "251044": {
      "poster": "Rene",
      "content": "@\"Todd Chavez\"#p251035 linearna odvojivost primjera == mogućnost da ih odvojiš u klase hiperravninom, nema veze s regresijom",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "251074": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "> @\"BillIK\"#p250732 ali postoji li neki kraći odgovor bez toliko teoretiziranja tipa. ona za koju će pogreška bit min/max?\n\nNe\n\nTo se, uostalom, rješava i drukčijim algoritmima, a ne samo drukčijom vrijednosti i otvoreni je (potencijalno nerješivi) problem.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "252230": {
      "poster": "nnn (dinoo)",
      "content": "Zna netko 4 i 6 iz dz08/v08?\n\n![](assets/2021-11-10/00009.png)![](assets/2021-11-10/00010.png)",
      "votes": {
        "upvoters": [
          "Sulejman",
          "angello2",
          "cajaznun",
          "cloudies",
          "netko_tamo"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "253724": {
      "poster": "angello2",
      "content": "moze bar neko objasnjenje za 6. zad iz ispita? probo sam svasta i nikad mi ne ispadne ovo njihovo rjesenje",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "253805": {
      "poster": "viliml",
      "content": "@\"dino\"#p252230 @\"angello2\"#p253724 \n\nKao prvo, u SVM-u se [imath]w_0[/imath] tretira posve zasebno od svega ostaloga. Dakle treba izbrisati prvi stupac matrice dizajna i prvu vrijednost u vektoru [imath]\\mathbf{w}[/imath].\n\nZatim treba pronaći oznake primjera i odrediti koji od njih su potporni. To se lako vidi iz vrijednosti hipoteze [imath]\\mathbf{w}^\\intercal\\mathbf{x}^{(i)}+w_0[/imath].\n\nSada samo treba primijeniti ekvivalentnost primarnog i dualnog modela [imath]\\mathbf{w}=\\sum_{i=1}^N{\\alpha_iy^{(i)}\\mathbf{x}^{(i)}}[/imath] da se nađu vrijednosti dualnih parametara.\n\nNedostatak preciznosti u zadanim vrijednostima može malo zakomplicirati stvar jer neće biti točne jednakosti, aproksimacija se može učiniti robustnijom tako da se iskoristi ograničenje [imath]\\sum_{i=1}^N{\\alpha_iy^{(i)}=0}[/imath] i tako smanji broj varijabli za 1.",
      "votes": {
        "upvoters": [
          "angello2",
          "fraki"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "Kasperinac",
          "netko_tamo",
          "pvmnt (ostaje.mi.to.sto.se.volimo)"
        ]
      }
    },
    "253878": {
      "poster": "netko_tamo",
      "content": "@\"viliml\"#p253805 ja i dalje ne dobivam njhovo rješenje, čak ni ne dobivam da mi predikcija za neke primjere daje 1 pa nemam ni potporne vektore...",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "253896": {
      "poster": "viliml",
      "content": "@\"netko_tamo\"#p253878\n\n[math]\\left(\n\\begin{array}{ccccc}\n 1 & 3 & 16 & -8 & -11 \\\\\n 1 & -5 & 4 & -8 & -7 \\\\\n 1 & 7 & -4 & 11 & 9 \\\\\n 1 & 15 & -20 & 25 & 25 \\\\\n\\end{array}\n\\right) \\left(\n\\begin{array}{c}\n 0.137 \\\\\n -0.029 \\\\\n 0.0194 \\\\\n -0.0461 \\\\\n -0.0388 \\\\\n\\end{array}\n\\right)=\\left(\n\\begin{array}{c}\n 1.16 \\\\\n 1. \\\\\n -1. \\\\\n -2.81 \\\\\n\\end{array}\n\\right)[/math]",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "253944": {
      "poster": "Rene",
      "content": "@\"dino\"#p252230 Jel iko rjesio 4.? Meni se cini da je rjesenje pod B tocno?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "253971": {
      "poster": "pvmnt (ostaje.mi.to.sto.se.volimo)",
      "content": "@\"Rene\"#p253944 povuci pravac izmedu dvije najblize tocke s lijeve strane i okomica na taj pravac s primjerom 3,3 ce ti bit u 0,0 pa ce ti 3,3 bit udaljeno od tog pravca korijen iz 3*3+3*3 sta je korijen iz 18, margina je korijen iz 18/2, sve podijeljeno s 1/2 sto je margina od prvog treniranja dobijes da je omjer korijen iz 18, sto je 3 puta korijen iz 2. pozdrav!",
      "votes": {
        "upvoters": [
          "Rene"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "253975": {
      "poster": "pvmnt (ostaje.mi.to.sto.se.volimo)",
      "content": "@\"legend649\"#p253971 ma nabijem na kurac ovaj forum ove nakosene 33 je 3 puta 3. neuk sam i nepismen. pozdrav!",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Bucc (Olive Oil)",
          "Kasperinac",
          "Lyras",
          "Tone",
          "kix7 (Fish99)",
          "pingvinka",
          "steker"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "254030": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"legend649\"#p253975 Preporučam korištenje latexa za jednadžbe, ili escape znaka `\\`",
      "votes": {
        "upvoters": [
          "matt (Matt)"
        ],
        "downvoters": [
          "Dootz",
          "Kasperinac"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254047": {
      "poster": "steker",
      "content": "Postoje neki sluzbeni materijali za neparametarske metode osim natuknica?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254218": {
      "poster": "Tompa007 (𝐓𝐇𝐄 𝐒𝐄𝐂𝐑𝐄𝐓 - 𝐂𝐋𝐔𝐁)",
      "content": "Koje je gradivo zadnje za meduispit(koja preza) (uključeno) ?",
      "votes": {
        "upvoters": [
          "Han"
        ],
        "downvoters": [
          "WP_Deva (IdeGas)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254220": {
      "poster": "[deleted]",
      "content": "@\"Tompa007\"#p254218 https://www.fer.unizg.hr/predmet/struce1/obavijesti?@=2ur9b#news_142309",
      "votes": {
        "upvoters": [
          "Tompa007 (𝐓𝐇𝐄 𝐒𝐄𝐂𝐑𝐄𝐓 - 𝐂𝐋𝐔𝐁)"
        ],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254543": {
      "poster": "boogie_woogie (nika_1999)",
      "content": "Zna netko ovaj?\n\n![](assets/2021-11-16/00008.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254642": {
      "poster": "viliml",
      "content": "@\"boogie_woogie\"#p254543 \n\nSamo izračunaš vrijednosti hipoteze i to uvrstiš u formulu za gubitak zglobnice uz [imath]\\lambda=1/C[/imath].",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254646": {
      "poster": "tomekbeli420",
      "content": "@\"boogie_woogie\"#p254543 najprije trebaš izračunati vektor težina [imath]\\mathbf{w}[/imath] kako bi mogao računati gubitke zglobnice. Taj vektor težina možeš iz vektora [imath]\\boldsymbol{\\alpha} = \\left(\\alpha_1 , \\alpha_2 , \\alpha_3\\right) = \\left(0, 0.01, 0.01\\right)[/imath] računati preko one poveznice kad se prelazilo iz primarnog problema meke margine u dualni:\n\n[math]\\mathbf{w} = \\sum_{i=1}^{N} \\alpha_i y^{(i)} \\mathbf{x}^{(i)}[/math]\n\nSve te podatke imaš, kad se uvrsti dobiješ [imath]\\mathbf{w} = \\left(0.02, 0, 0.03\\right)[/imath]\n\nEmpirijska pogreška SVM-a na skupu [imath]\\mathcal{D}[/imath] se računa kao:\n\n[math]E_{R} \\left(\\mathbf{w}, w_0 \\vert \\mathcal{D} \\right) = \\sum_{i=1}^{N} \\max{\\left(0, 1 - y^{(i)} h \\left(\\mathbf{x}^{(i)}; \\mathbf{w}, w_0\\right)\\right)} \\  + \\frac{1}{2 C} \\| \\mathbf{w} \\|^2[/math]\n\ngdje je [imath]h \\left(\\mathbf{x}; \\mathbf{w}, w_0\\right) = \\mathbf{w}^{\\mathrm{T}} \\mathbf{x} + w_0[/imath] hipoteza iz primarnog modela ([imath]w_0 = -0.8[/imath] je dan da se može izračunati). Zadatak kaže da je korištena linearna jezgra, pa kao stoga nema dodatnog preslikavanja primjera nego uz težine stoje sirovi, nepreslikani primjeri [imath]\\mathbf{x}[/imath]. Za primjere one redom iznose -1, -1, -0.87\n\nStoga njihovi gubici zglobnice iznose redom 0, 0, 1.87, dakle u zbroju daju 1.87, kvadrat L2 norme iznosi [imath] \\| \\mathbf{w} \\|^2 = 0.0013[/imath], [imath]C = 0.01[/imath] je također zadan i kad sve to uvrstiš dobiješ\n\n[imath]E_{R} \\left(\\mathbf{w}, w_0 \\vert \\mathcal{D} \\right) = 1.87 + \\frac{1}{2 \\cdot 0.01} \\cdot 0.0013 = 1.935[/imath]\n\nEdit: tek sad sam shvatio da si mogao iznose hipoteza računati i bez vraćanja nazad u primaran model, al u svakom slučaju ti trebaju težine iz primarnog modela zbog regularizacijskog faktora.\n\n\n\n\nOn topic, jel uspio netko ovaj: V10 - Jezgrene metode, zadaci s ispita 4. zadatak\n\nnikak ne mogu doći do rješenja, kod računanja hipoteze ona suma koja ima alfe, oznake i jezgrenu funkciju mi ispada reda veličine [imath]10^{-3}[/imath], nisam išao računati [imath]w_0[/imath] jer sam odustao\n\n![](https://i.imgur.com/25w5UnO.png)",
      "votes": {
        "upvoters": [
          "Ducky",
          "InCogNiTo124",
          "bodNaUvidima",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "boogie_woogie (nika_1999)",
          "gladiator",
          "matt (Matt)",
          "sheriffHorsey"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254691": {
      "poster": "[deleted]",
      "content": "![](assets/2021-11-16/00018.png)\n\nzas je ovdje prije sume minus?",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254695": {
      "poster": "tomekbeli420",
      "content": "@\"Todd Chavez\"#p254691 \n\nJer ograničenja nejednakosti u standardnom obliku su nešto <= 0, a kad ono ograničenje di imaš >= 1 preformuliraš (prebaciš 1 na lijevu stranu i pomnožiš sa -1 da obrneš nejednakost) dobiješ ovo što piše uz alfe",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254704": {
      "poster": "viliml",
      "content": "@\"tomekbeli420\"#p254646 Hipoteze za potporne vektore ispadnu -1.97711 + w0, -1.92673 + w0 i 0.0544753 + w0, a za primjer 0.00106184 + w0\n\nw0 je otprilike 0.95, pa je vrijednost hipoteze za primjer otprilike 0.95.",
      "votes": {
        "upvoters": [
          "boogie_woogie (nika_1999)",
          "tomekbeli420"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254708": {
      "poster": "Rene",
      "content": "Kako odrediti jesu li primjeri linearno odvojivi nakon preslikavanja u prostor značajki? Već nekoliko takvih zadataka sam susreo, a nije mi baš jasno s obzirom da ne mogu nacrtati ni izračunati izlaz modela da odredim oznake?\n\nNpr. V10 3. zadatak za učenje d), oke dobio sam preslikavanje [imath]\\phi(\\vec{x})=\\begin{pmatrix}x_1^2 &x_2^2 & x1x2\\sqrt2 &x_1\\sqrt2 & x_2\\sqrt2 &1\\end{pmatrix}[/imath], ali kako onda provjeriti je li XOR problem linearno odvojiv?\n>! ![](assets/2021-11-16/00019.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254722": {
      "poster": "viliml",
      "content": "@\"Rene\"#p254708 Treba pogađati i vidjeti da je [math]XOR=x_1+x_2-2x_1 x_2=\\frac{-2\\phi_3+\\phi_4+\\phi_5}{\\sqrt 2}[/math]",
      "votes": {
        "upvoters": [
          "Rene"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254724": {
      "poster": "Rene",
      "content": "@\"viliml\"#p254722 Kako? x=(1, 1) => 1 + 1 - 1 = 1, po tvom izrazu?\n\nEDIT: vidim, zapravo je [imath] x_1 + x_2 - 2x_1x_2[/imath], hvala!",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254727": {
      "poster": "tomekbeli420",
      "content": "@\"Rene\"#p254708 ja sam rekao da jest linearno odvojivi i to sam argumentirao logikom da granica\n\n[imath]h \\left(\\mathbf{x} ; \\mathbf{w}, w_0 \\right) = \\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi} \\left(\\mathbf{x}\\right) + w_0 = 0[/imath]\n\nJe krivulja drugog reda (najopćenitija bez ograničenja). Takva krivulja drugog reda je sposobna odvojiti primjere kod XOR problema.\n\nKod jezgrene funkcije [imath]\\kappa \\left(\\mathbf{x}, \\mathbf{z}\\right) = \\left(\\mathbf{x}^{\\mathrm{T}} \\mathbf{z}\\right)^2[/imath] sam pak preslikao u 3D prostor značajki sa pripadnon funkcijom preslikavanja, dobio sam\n\n[imath]\\boldsymbol{\\phi} \\left(\\mathbf{x}\\right) = \\left(x_1^2 , \\sqrt{2} x_1 x_2 , x_2^2\\right)[/imath]\n\nI tu sam našao (naprosto sam nacrtao preslikane primjere i gledao koju ravninu povući) ravninu koja razdvaja\n\n[imath]\\mathbf{w} = \\left(2, -2, 2\\right) \\qquad w_0 = -1.5[/imath]\n\nAko baš hoćeš, iste težine bi se mogle iskoristiti za naći hiperravninu kod prve jezgrene funkcije\n\nEvo nadam se da mi sve štima i da nisam neš sjebo",
      "votes": {
        "upvoters": [
          "Rene"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254741": {
      "poster": "viliml",
      "content": "@\"Rene\"#p254724 Da sori išao sam napamet i zbunio sam se, znao sam da je tako nešto otprilike.\n\n@\"tomekbeli420\"#p254727 Hoćeš reći da prostor hipoteza obuhvaća *sve* krivulje drugog reda? To je validno, ali treba obrazložiti zašto postoji krivulja drugog reda koja odvaja XOR, što je ekvivalentno originalnom problemu osim ako se to smatra opće poznatim...\n\nAli u svakom slučaju za drugu jezgru se mora naći ručno (hipoteza je isto krivulja drugog reda ali nije općenita), a pošto su za  binarne značaje [imath]x[/imath] i [imath]x^2[/imath] ista stvar, vidi se da je rješenje za jedno ujedno i rješenje za drugu.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254755": {
      "poster": "tomekbeli420",
      "content": "@\"viliml\"#p254741 da to sam mislio\n\nZašto postoji krivulja? Vizualno je trivijalno konstruirati primjer takve krivulje xD\n\nDa mi se da zajebavati sa rotacijama čak bi i složio težine koje čine elipsu oko pozitivnih primjera, al mi se ne da (a i može se napamet nešto nabosti)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254762": {
      "poster": "Rene",
      "content": "@\"viliml\"#p254704 otkud ti da je w0 otprilike 0.95?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254767": {
      "poster": "netko_tamo",
      "content": "![](assets/2021-11-16/00021.png)\n\nmoze neko objasniti? koristimo 50 znacajki ak sam dobro pobrojao",
      "votes": {
        "upvoters": [
          "batman3000"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254774": {
      "poster": "Fica (Prof)",
      "content": "@\"netko_tamo\"#p254767 Izbacuješ prosjek ocjena jer ti je savršeno koreliran sa prosjecima svake godine i onda imaš 6 linearnih i 6 kvadratnih + 1 i još za interakcijske imaš 6 povrh 2 = 15 i 6 povrh 3 = 20 i onda ispadne 6+6+1+15+20=48",
      "votes": {
        "upvoters": [
          "netko_tamo",
          "zara"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "netko_tamo"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "254776": {
      "poster": "netko_tamo",
      "content": "@\"Prof\"#p254774 tenks :D",
      "votes": {
        "upvoters": [
          "Fica (Prof)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254803": {
      "poster": "netko_tamo",
      "content": "![](assets/2021-11-16/00023.png)\n\nL2 norma je sqrt(wTw), dobivam 2.96, ocito nekaj kardinalno krivo radim pa ne vidim hahah",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254812": {
      "poster": "viliml",
      "content": "@\"Rene\"#p254762 hipoteze potpornih vektora moraju biti +-1",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254814": {
      "poster": "viliml",
      "content": "@\"netko_tamo\"#p254803 Norma gradijenta gubitka, ne norma težina.\n\nHipoteza je mrvicu veća od 0 dakle greška je mrvicu manja od 1 dakle gradijent je mrvicu manji od vektora značajki koji ima normu 2.5",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254815": {
      "poster": "Rene",
      "content": "@\"viliml\"#p254812 ne moraju ako je rijec o mekoj margini?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254880": {
      "poster": "viliml",
      "content": "@\"Rene\"#p254815 Nigdje ne piše da je meka.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254917": {
      "poster": "Banananjeros",
      "content": "Može li neko dati neki hack kako razlikovati induktivnu pristranost jezikom od induktivne pristranosti preferencijom?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254942": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Banananjeros\"#p254917 \n\nPristranosti jezikom biras model, dakle skup svih hipoteza koje dolaze u obzir. Iz tog skupa moraš nekako odabrati jednu hipotezu. To činiš funkcijom pogreške, i to je upravo pristranost preferencijom. Dodatno, ako prostor inačica ima više hipoteza (2, 3, inf), opet moraš izabrati jednu, (kod SVM-a recimo onu s najvecom marginom), to je isto pristranost preferencijom.\n\nProstor inačica je skup svih hipoteza nekog modela koje ispravno klasificiraju (tj predviđaju kod regresije) sve primjere.",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254968": {
      "poster": "Banananjeros",
      "content": "@\"Precious Bodily Fluids\"#p254942 Je li pristranost preference isključivo traženje neke hipoteze u prostoru inačica ili traženje neke hipoteze u prostoru hipoteza općenito?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254988": {
      "poster": "viliml",
      "content": "@\"Banananjeros\"#p254968 Po definiciji to bi trebalo biti u prostoru inačica, ALI...\n\nMi zapravo skoro nikad ne radimo baš s prostorom inačica.\n\nČesto je ili prostor inačica prazan, ili mi možda odaberemo hipotezu izvan prostora inačica jer ona bolje generalizira.\n\nPrisutnost šuma i mogućnosti krivo klasificiranih primjera za treniranje čine koncept prostora inačica besmislenim.\n\nJedini slučaj kad uistinu radimo direktno s prostorom inačica je SVM s tvrdom marginom.\n\nAli funkciju pogreške ipak često neprecizno nazivaju pristranost preferencije, i ako dođe takvo pitanje u ispitu trebao bi ne razmišljati preduboko o tome jer oni sigurno nisu.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255125": {
      "poster": "gladiator",
      "content": "Imam pitanje za težinski k-NN. Zašto gledamo **sve** primjere (i za sve primjere računamo k(x_i, x)) kad koristimo k-NN.\n\nTu gubimo \"k\" iz k-NN. Zar ne bi smo trebali gledati najbliže primjere i onda na temelju njih i rezultatata funkcije k(x_i, x) zaključiti kojoj klasi pripada x?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255128": {
      "poster": "Rene",
      "content": "@\"gladiator\"#p255125 mislim da se moze i jedno i drugo, na predavanju je snajder napisao da se uobicajeno koristi N ali da suma moze ic i samo za k najblizih",
      "votes": {
        "upvoters": [
          "gladiator"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255147": {
      "poster": "gad_gadski",
      "content": "![](assets/2021-11-18/00006.png)\n\nJel zna netko kako oni dobiju 2.69? Meni ispada 2.7346, kvadratna pogreska mi ispadne 2.1846 i onda to zbrojeno s 1/2 * 1.1 = 2.7346 \n\n![](assets/2021-11-18/00007.jpg)",
      "votes": {
        "upvoters": [
          "BillIK",
          "faboche (him)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255179": {
      "poster": "BillIK",
      "content": "![](assets/2021-11-18/00010.png)\n\n zna netko ovaj zadatak?",
      "votes": {
        "upvoters": [
          "blast"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255180": {
      "poster": "Amariska",
      "content": "@\"gad_gadski\"#p255147 \n\nKad se računa L1-norma uzimaju se apsolutne vrijednosti i to bez težine w0, pa imaš 0.94+0.08 = 1.02\n\n2.1846 + 1.02/2 = 2.69",
      "votes": {
        "upvoters": [
          "BillIK",
          "Ducky",
          "Upforpslone",
          "gad_gadski",
          "neksi (filip)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255190": {
      "poster": "Amariska",
      "content": "@\"BillIK\"#p255179 \n\nProsjek ocjena se treba izbaciti jer je kombinacija prve četiri značajke. \n\nlinearne: x1, x2, x3, x4, x6, x7\n\nkvadratne: x1^2, x2^2....\n\ninterakcijske: parova ima 6C2 = 15, trojki ima 6C3 = 20\n\n6 + 6 + 15 + 20 + dummy = 48\n\nRang matrice mora biti m+1=48, pa treba biti minimalno 48 primjera jer da ih je manje i rang bi bio manji, tj. bio bi max N",
      "votes": {
        "upvoters": [
          "BillIK",
          "Ducky",
          "blast",
          "sheriffHorsey"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255386": {
      "poster": "Banananjeros",
      "content": "![](assets/2021-11-19/00004.png)\n\nMože li neko objasnit ovaj?Zašto je A točno i zašto regresija nema ovaj problem, a perceptron ima?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255392": {
      "poster": "BillIK",
      "content": "@\"netko_tamo\"#p254803 možeš slikati postupak ako si riješio?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255407": {
      "poster": "SuperSjajan3",
      "content": "Moze pomoc oko ovog zadatka pls. Raspisem L tako da dobijem h(x), al onda ne znam sta bi dalje s tim\n\n![](assets/2021-11-19/00005.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255408": {
      "poster": "SuperSjajan3",
      "content": "@\"Banananjeros\"#p255386 \n\nova dva odgovora prosle generacije:\n\n@\"cotfuse\"#p93340 \n\n@\"cotfuse\"#p93390",
      "votes": {
        "upvoters": [
          "zara"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255414": {
      "poster": "BillIK",
      "content": "@\"SuperSjajan3\"#p255407 h(x) je sigmoidalna funkcija od umnoška težina i primjera + w0. Izračunaj umnožaš w^T*X i koristi za računanje novog gubitka, ali pomnoženo s 2 jer ti kaže da se značajke množe s dva i s promijenjenom oznakom",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255415": {
      "poster": "lovro (l123)",
      "content": "Jel ima neki jednostavan način za downgrade matplotlib-a bez da ga deinstaliravam pa instaliravam stariju verziju?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255417": {
      "poster": "tomekbeli420",
      "content": "@\"Banananjeros\"#p255386 zato što algoritam perceptrona se zaustavlja tek onda kad su svi primjeri ispravno klasificirani, dok gradijentni spust kod logističke regresije se zaustavlja kad se dosegne minimum funkcije pogreške (globalni, jer je konveksna), koji može biti koliki god (nikad 0 tho), pa makar i relativno visok zbog linearne neodvojivosti. To će se uvijek desiti ako se koristi linijsko pretraživanje",
      "votes": {
        "upvoters": [
          "Banananjeros"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255427": {
      "poster": "SuperSjajan3",
      "content": "@\"BillIK\"#p255392 malo je neuredno, puno brisanja je bilo, reci ak nesto ne vidis procitat\n\n![](assets/2021-11-19/00008.png)",
      "votes": {
        "upvoters": [
          "BillIK",
          "Ducky"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255433": {
      "poster": "-Ivan- (Ivančica)",
      "content": "@\"viliml\"#p250965 \n\nKako si došao do ove prve dvije formule?\n\n![](assets/2021-11-19/00009.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255438": {
      "poster": "steker",
      "content": "![](assets/2021-11-19/00010.jpg)\n\nZasto D nije tocan",
      "votes": {
        "upvoters": [
          "[deleted]",
          "sheriffHorsey"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255440": {
      "poster": "gad_gadski",
      "content": "@\"BillIK\"#p255414 ![](assets/2021-11-19/00011.jpg)\n\nMeni uporno 1.2164 ispada, je li samo treba ovaj umnozak w*x pomnoziti s dva ili? Nez di mi je greska",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255450": {
      "poster": "cloudies",
      "content": "@\"gad_gadski\"#p255440 Koliko mi se cini, ti si pomnozio sve s 2, a tamo ti se nalazi i dummy jedinica koju si pretvorio u dvojku. Msm da trebas razdvojit na w i w0.",
      "votes": {
        "upvoters": [
          "Jale (čakijale)",
          "gad_gadski"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255453": {
      "poster": "Amariska",
      "content": "@\"gad_gadski\"#p255440 \n\n>!![](assets/2021-11-19/00014.jpg)",
      "votes": {
        "upvoters": [
          "Ducky",
          "gladiator"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255459": {
      "poster": "BillIK",
      "content": "@\"gad_gadski\"#p255440 ovo što je kolega @\"cloudies\"#p255450  napisao",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255471": {
      "poster": "Banananjeros",
      "content": "![](assets/2021-11-19/00018.png)\n\nJasno mi je zašto A i B neće konvergirati, ali koja je razlika između C i D? Zašto logistička regresija s linearno odvojivim PHI1 preslikavanjem neće konvergirati?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255475": {
      "poster": "prx_xD",
      "content": "Ima netko možda ss od prošlogodišnjih moodle kvizova",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255482": {
      "poster": "sheriffHorsey",
      "content": "@\"Banananjeros\"#p255471 pa upravo je poanta u tome sto imas linearno odvojiv slucaj, algoritam ce povecavati tezine, ako tezine rastu onda raste i vrijednost umnoska [imath]\\mathbf{w}^T \\phi(\\mathbf{x})[/imath], a ako to raste onda sigmoida uvijek moze biti sve strmija i liciti sve vise na funkciju praga pa onda uvijek mozes smanjiti gubitak, a u d slucaju posto nemas linearnu odvojivost nemas taj problem i u jednom trenu neces vise moc smanjiti gubitak pa dolazi do konvergencije",
      "votes": {
        "upvoters": [
          "Banananjeros"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255485": {
      "poster": "tomekbeli420",
      "content": "@\"steker\"#p255438 Neka je zadana sljedeća situacija:\n\nNeka je [imath]\\mathcal{X} = \\mathbb{R}[/imath], odnosno [imath]n = 1[/imath], i naravno [imath]\\mathcal{Y} = \\left\\{-1, +1\\right\\}[/imath], te neka je skup primjera za učenje\n\n[imath]\\mathcal{D} = \\left\\{\\left( x^{(i)}, y^{(i)} \\right)\\right\\} = \\left\\{\\left(2, -1\\right), \\left(6, +1\\right)\\right\\}[/imath]\n\nJasno je da su primjeri linearno odvojivi.\n\nOdgovor D implicira da postoji mogućnost da će SVM izabrati ovakvu hipotezu\n\n[imath]h \\left(x ; w_1, w_0\\right) = x - 5[/imath]\n\nodnosno da je [imath]w_1 = 1[/imath] i [imath]w_0 = -5[/imath]\n\nšto bi značilo da je granica [imath]x = 5[/imath]\n\nproblem je kod ovog \"... za najbliže primjere\" jer se tu nigdje ne spominje da SVM udaljenost najbližih primjera mora maksimizirati, pa je prema tom odgovoru moguće da mu je najbliži samo jedan primjer (u našem primjeru ova šestica), i za taj primjer da vrijedi [imath]y h(x) = 1[/imath]. To naravno kod pravog SVM-a nije istina, jer on maksimizira minimalnu udaljenost, i to je sve ukodirano sa onim problemom minimizacije kvadrata vektora težina uz ograničenja da hipoteze moraju dati iznose apsolutno veće od 1.",
      "votes": {
        "upvoters": [
          "matt (Matt)",
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255501": {
      "poster": "steker",
      "content": "Koja bi bila pristranost preferencijom kod perceptrona? Jel bi to zapravo bila linearna odvojivost podataka ili to nema veze s pristranoscu",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255549": {
      "poster": "steker",
      "content": "@\"steker\"#p255501 ne znam jel ovo glupo razmisljanje, ali takoder jel se moze rec da su pristranosti jezikom kod SVM-a i perceptrona iste s obzirom da svm u primarnom obliku ima za h(x)=sgn(wtx), a perceptron h(x)=step(wtx), (znam da su step i sgn dvije razlicite func ali obje onako kako smo ih definirali na predavanjima bacaju van -1 tj1 za iste vrijednosti x)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255552": {
      "poster": "viliml",
      "content": "@\"Ivančica\"#p255433 Prvo je gubitak primjera x s oznakom 0, drugo je gubitak primjera 2x s oznakom 1. Samo sam odmah izbacio onaj dio koji se množi s 0 u općenitoj formuli, ionako je to zapravo samo te dvije formule prisilno spojene množenjem s 0/1.",
      "votes": {
        "upvoters": [
          "-Ivan- (Ivančica)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255567": {
      "poster": "viliml",
      "content": "@\"viliml\"#p255552 Zapravo možeš proći kroz cijeli izvod u poglavlju 2.1 skripte broj 6, preskočiti ono kad zamjene if/else sa potenciranjem i množenjem i izvesti funkciju gubitka\n\n[math]L(y,h(\\mathbf{x}))=\\begin{cases} -\\ln h(\\mathbf{x}) & \\text{ako }y=1 \\\\ -\\ln (1-h(\\mathbf{x})) & \\text{inače} \\end{cases}[/math]\n\nIskreno ne znam zašto su uopće radili te gluposti, ovaj oblik je puno jasniji.",
      "votes": {
        "upvoters": [
          "-Ivan- (Ivančica)",
          "Kasperinac"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255577": {
      "poster": "viliml",
      "content": "@\"steker\"#p255501 Perceptron nema nikakvu pristranost preferencijom. Svaka hipoteza koja ispravno klasificira skup za treniranje je njemu jednako dobra. To je jedan od mnogih razloga zašto je to loš algoritam.\n\n@\"steker\"#p255549 Pristranost jezikom je praktički ista stvar što i model. SVM i perceptron oboje koriste linearni model, dakle da.",
      "votes": {
        "upvoters": [
          "matt (Matt)",
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255581": {
      "poster": "steker",
      "content": "@\"viliml\"#p255577 ![](assets/2021-11-19/00036.jpg)\n\nA onda ne razumijem zasto bi ovo pod B bilo krivo, osim ako je problem u prvom navodu",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255583": {
      "poster": "viliml",
      "content": "@\"Banananjeros\"#p255471 Krivo je zadan zadatak jer **empirijska pogreška** uistinu konvergira i sa linearno odvojivim skupom podataka, stvar je da pritom **težine** divergiraju.",
      "votes": {
        "upvoters": [
          "Banananjeros"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255588": {
      "poster": "viliml",
      "content": "@\"steker\"#p255581 Rekao bih da je to samo krivo zadan zadatak. Nije ni prvi ni zadnji put.\n\nMožda općenito zovu funkciju pogreške pristranosti preferencijom, ali problem je što niti perceptron niti SVM s tvrdom marginom nisu definirani kad skup primjera nije linearno odvojiv, a perceptron samo garantira da će naći **neku** hipotezu koja ih odvaja i ništa ne kaže koju.\n\n*Mooooožda* se može reći da perceptron \"preferira\" onu hipotezu do koje ga slučajno odvede gradijentni spust? To je malo čudna preferencija, jer on niti ne zna da postoje ikoje druge, ali zapravo kad bolje razmislim, konzistentno je s rigoroznom definicijom pristranosti...",
      "votes": {
        "upvoters": [
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255592": {
      "poster": "steker",
      "content": "@\"viliml\"#p255588 vrlo moguce, ugl hvala",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255642": {
      "poster": "Banananjeros",
      "content": "Zašto u jednom zadatku s SVM-om kažemo da beskonačno mnogo hipoteza zadovoljava uvjet savršene klasifikacije, ali u zadacima iz prvih tema kažemo da je broj hipoteza konačan jer vrijedi:\n> dvije funkcije su jednake ako jednako preslikavaju elemente iz domene u kodomenu\n\nZar ne bi prema citatu trebali sve te hipoteze smatrati jednom hipotezom jer na isti način preslikavaju primjere pa bi time broj hipoteza bio 1 tj. konačan? Ali, time bi kardinalitet prostora inačica uvijek bio 1.\n\n![](assets/2021-11-20/00005.png)\n\n![](assets/2021-11-20/00006.png)",
      "votes": {
        "upvoters": [
          "gladiator"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255646": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Banananjeros\"#p255642 \n\nStvar je u tome da je u zadatku \"prostor\" primjera R^n. Iako u prostoru inačica jednako klasificiraš sve viđene primjere (tj. sve iz tvog dataseta), postoji beskonačno mnogo neviđenih primjera i svaki pravac će za njih dati malo drugačiju klasifikaciju.\n\nOvo što citiraš na početku vrlo vjerojatno opisuje klasifikaciju u diskretnom prostoru, npr {0, 1}x{0, 1}x{0, 1}, dakle vrhovi kocke. Tu imaš samo 8 mogućih primjera (označenih i neoznačenih). Ako napraviš malu modifikaciju na ravninu ona će vjerojatno i dalje sve primjere (viđene i neviđene) klasificirati kao i prije, dok u zadatku gore, gdje radimo u kontinuiranom prostoru to očito nije slučaj.",
      "votes": {
        "upvoters": [
          "gladiator",
          "viliml"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255647": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"viliml\"#p255577 \n\n@\"steker\"#p255581 \n\n> Mooooožda se može reći da perceptron “preferira” onu hipotezu do koje ga slučajno odvede gradijentni spust?\n\nTako sam i ja shvatio. Pristranost preferencijom (PP) je cijeli optimizacijski postupak, uključujući fju pogreške i algoritam (npr. gradijentni spust). PP perceptrona je uzimanje hipoteze u kojoj gradijentni spust konvergira. Po toj logici onda PP modificiraš hiperparametrom stope učenja. \n\nAl idk, možda samo filozofiram i stvarno je greška u zadatku 😅",
      "votes": {
        "upvoters": [
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255712": {
      "poster": "BillIK",
      "content": "![](assets/2021-11-20/00015.png)\n\nima li netko postupak? ne dobivam točno nikako",
      "votes": {
        "upvoters": [
          "Ardura (Maddy)",
          "matt (Matt)",
          "sheriffHorsey"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255719": {
      "poster": "faboche (him)",
      "content": "![](assets/2021-11-20/00016.png)\n\nkako dodemo do 1001 i 2829? (jezgrene metode, 1.zdk sa ispita)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255723": {
      "poster": "Rene",
      "content": "@\"faboche\"#p255719 varijanca gaussovih jezgri je hiperparametar, 28 prototipa×100znacajki + 28 pripadnih alfi + w0 je 2829 parametara, a optimiras 1000 alfi i w0",
      "votes": {
        "upvoters": [
          "faboche (him)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255724": {
      "poster": "BillIK",
      "content": "@\"faboche\"#p255719 pogledaj si formulu za gaussovu jezgru. Radiš preslikavanje jezgrenom funkcijom za svaki od N primjera pa ćeš dobiti dobiti Phi (x) = [1, k(x,x_1), k(x,x_2), ... k(x,x_N)] i svaka ta jezgrena funkcija je u biti jedna težina, dakle dobivaš N+1 parametara, tj. 1001 (ovaj +1 je za w0). Dakle imaš 1001 parametar \n\nA što se tiče naučenog modela: \n\ndobivaš 28 prototipa, svaki od njih ima 100 značajki (prema tekstu zadatka) to je 28*100 = 2800 \n\nsvakoj značajki pridružuješ jednu težinu, što znači da dobivaš 28 težina + 1 (za težinu w0)\n\ntočnije imaš 28*100 + 28 + 1 = 2829 parametara naučenog modela\n\nEDIT: kad pišem težina, mislim na alfu",
      "votes": {
        "upvoters": [
          "faboche (him)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255735": {
      "poster": "MOXY",
      "content": "![](assets/2021-11-20/00021.png)\n\nzna netko objasniti ovaj?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255738": {
      "poster": "viliml",
      "content": "@\"BillIK\"#p255712 Djelomični postupak sam napisao u @\"viliml\"#p254704 \n\nOvo između je samo množenje matrica i vektora, dobar kalkulator bi to trebao moći sve odjednom.\n\nReci koje vrijednosti hipoteza dobivaš krive.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255741": {
      "poster": "viliml",
      "content": "@\"MOXY\"#p255735 Malo kasniš? To je bilo davno za zadaću.\n\n![](assets/2021-11-20/00022.png)\n\nNema šta za objasniti, uvrštavaš u formulu.",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "AK10 (endyyyy)",
          "Bica",
          "Drmolirius",
          "Ferkonja (Ferkonja         ‮ajnokreF )",
          "Fica (Prof)",
          "Jaster111",
          "JogaBonito",
          "Kasperinac",
          "MOXY",
          "Skenk",
          "SuperSjajan3",
          "batman3000",
          "dh333",
          "matt (Matt)",
          "neksi (filip)",
          "netko_tamo",
          "sheriffHorsey",
          "stura",
          "zastozato (studoš)"
        ]
      },
      "reactions": {
        "haha": [
          "kix7 (Fish99)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "255743": {
      "poster": "Jaster111",
      "content": "@\"viliml\"#p255741 sori učiteljice",
      "votes": {
        "upvoters": [
          "Bica",
          "Ferkonja (Ferkonja         ‮ajnokreF )",
          "MOXY",
          "SuperSjajan3",
          "angello2",
          "netko_tamo",
          "stura"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "AK10 (endyyyy)",
          "Bananaking",
          "Bica",
          "BillIK",
          "Bucc (Olive Oil)",
          "Fica (Prof)",
          "Gulbash",
          "MOXY",
          "Ollie",
          "Skenk",
          "Zk6dO73 (burw0r)",
          "angello2",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "cajaznun",
          "neksi (filip)",
          "netko_tamo",
          "pingvinka",
          "rolotex (brr)",
          "steker",
          "tomekbeli420",
          "zastozato (studoš)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "255744": {
      "poster": "BillIK",
      "content": "@\"viliml\"#p255738 dobar kalkulator pa w0 ne ispadne OTPRILIKE 0.95 nego 1.46",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255749": {
      "poster": "Rene",
      "content": "![](assets/2021-11-20/00023.png)\n\nMoze objasnjenje?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255764": {
      "poster": "[deleted]",
      "content": "![](assets/2021-11-20/00024.png)\n\n zasto ovdje B ne bi bilo točno?",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255769": {
      "poster": "Dootz",
      "content": "@\"Todd Chavez\"#p255764 Za različite parametre možes dobiti i istu funkciju h",
      "votes": {
        "upvoters": [
          "[deleted]",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "viliml"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255770": {
      "poster": "bodNaUvidima",
      "content": "@\"Todd Chavez\"#p255764 jer nema garancije da će dva različita skupa parametara dati drukčiju hipotezu",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255779": {
      "poster": "[deleted]",
      "content": "![](assets/2021-11-20/00028.png)\n\na ovo ako netko zna logiku? prvo sam mislio da B uvodi nelinearnost pa zato, ali nije istina jer se koristi ova True/False fja\n\nnvm skuzio valjda, to je zato sto se u B moze dogodit da se pomnože lossevi s ispravnima i onda rezultat bude 0 sve skupa",
      "votes": {
        "upvoters": [
          "MsBrightside"
        ],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255780": {
      "poster": "viliml",
      "content": "@\"BillIK\"#p255744 Kako računaš w0?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255782": {
      "poster": "viliml",
      "content": "@\"Rene\"#p255749 Granica između klasa je krivulja definirana jednadžbom h(x)=0. Ta krivulja je pravac ako je funkcija [imath]\\phi[/imath] linearna",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255786": {
      "poster": "viliml",
      "content": "@\"Todd Chavez\"#p255779 Pitanje se svodi na to koliko najviše predikcija možeš natjerati da istovremeno budu 0 na zadanom skupu. Drugi model je zapravo skup AND-ova hipoteza iz prvog modela, pa je logično da može imati više nula (skup nula je unija)",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255791": {
      "poster": "[deleted]",
      "content": "![](assets/2021-11-20/00030.png)\n\nkako je ovdje VS veci od 1? nije li nacrtano zelenim jedina ravnina koja ce tocno odvojiti sve mogucnosti uz pretpostavku linearne odvojivosti",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255795": {
      "poster": "Arya",
      "content": "DZ1: Model takoder nazivamo prostorom inacica, a dimenzija tog prostora jednaka je cemu?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255796": {
      "poster": "[deleted]",
      "content": "@\"Arya\"#p255795 broju parametara.\n\n\n![](assets/2021-11-20/00031.png)\n\njel ne bi ovdje trebalo biti L(0, 1) > L(1, 0)? nije li redak == stvarni primjeri, a stupac predikcije",
      "votes": {
        "upvoters": [
          "Arya"
        ],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255798": {
      "poster": "Jaster111",
      "content": "![](assets/2021-11-20/00032.png)\n\nJel netko shvatio šta znači izraz \"do na ...\" jer sam vidio na hrpu mjesta da to spominju, ali uopće ne mogu skužit šta predstavlja",
      "votes": {
        "upvoters": [
          "angello2"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255803": {
      "poster": "sheriffHorsey",
      "content": "@\"Todd Chavez\"#p255791 \n\n![](assets/2021-11-20/00033.png)\n\nMislim da si samo pogrijesio u oznacavanju tocaka jer ja sam dobio ovakvu skicu u kojoj onda imas dvije hipoteze u version spaceu.",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255806": {
      "poster": "sheriffHorsey",
      "content": "@\"Todd Chavez\"#p255796 Nebi trebalo biti [imath] L(0, 1)> L(1, 0) [/imath]. U ovom zadatku treba skuzit da ti je bitnije imat sto manje false negativa jer ne zelis da tvoj klasifikator nekome ne otkrije karcinom, a zapravo ga ima. S druge strane nije ti toliko bitno ako nekom dijagnosticiras karcinom ako ga nema jer ce vjerojatno ic na neke dodatne pretrage da potvrdi to. Zbog toga zelis vise kaznjavati klasifikator ako napravi ovu prvu pogresku, a drugu manje i to upravo odgovara odnosima gubitka [imath] L(1, 0) > L(0, 1) [/imath].",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255807": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Jaster111\"#p255798 \n\nMislim da to možeš čitat kao izuzev, osim.",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255808": {
      "poster": "[deleted]",
      "content": "@\"sheriffHorsey\"#p255806 to me i muči, svjestan sam ja da je veći problem false negative, ali po ovoj matrici ne vidim kak je to ispravno rjesenje:\n\n![](assets/2021-11-20/00034.png)\n\nosim ako je svejedno kako oznacavamo retke/stupce pa zato ovo moje nije ni ponuđeno?",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255810": {
      "poster": "sheriffHorsey",
      "content": "@\"Todd Chavez\"#p255808 Ako ti redak oznacava stvarnu situaciju onda redak 1 znaci da osoba ima karcinom, a recimo da klasifikator to ne skuzi pa je stupac 0, znaci da je to L(1, 0), tj. prvo indeksiras redak pa stupac.",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255811": {
      "poster": "[deleted]",
      "content": "@\"sheriffHorsey\"#p255810 aha, zanemario sam da su 0 i 1 oznake klase, ja sam to gledao samo kao indekse u tablici :D ty",
      "votes": {
        "upvoters": [
          "sheriffHorsey"
        ],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255840": {
      "poster": "-Ivan- (Ivančica)",
      "content": "@\"tomekbeli420\"#p254646 \n\nJe li možeš molim te reći kaj točno uvrstiš ovdje jer nemrem skužit nikak\n\n(prebacivanje iz dualnog u primarni oblik)\n\n![](assets/2021-11-20/00036.png)\n\nZnači npr. kak si došo do w1? Što si točno uvrstio, koje brojeve?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255847": {
      "poster": "Rene",
      "content": "Zna netko ova dva?\n\n![](assets/2021-11-20/00037.png)\n\n![](assets/2021-11-20/00038.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255863": {
      "poster": "viliml",
      "content": "@\"Jaster111\"#p255798 \"isto do na X\" općenito znači da mijenjanjem X-a prelaziš iz jednog u drugo. Ovdje je jedan izraz jednak drugom pomnoženom s C.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255864": {
      "poster": "Fica (Prof)",
      "content": "@\"Rene\"#p255847 Imaš ovaj prvi detaljno objašnjen u snimci sa zadnje rekapitulacije na teamsima, uglavnom za ovu jezgru se izvede preslikavanje (x1^2, sqrt(2)*x1*x2, x2^2), a sa polinomom dobiješ standardno (1, x1, x2, x1^2, x1x2, x2^2) i onda ovom prvom dodaš samo 3 nule na početak jer tih članova tamo nemaš i onda ti je euklidska udaljenost zbroj kvadrata razlike za svaki od njih. Kad uvrstiš primjer dobiješ za jezgreni (0,0,0,0,1,0), a za ovaj (1,1,0,0,1,0) i onda je to korijen iz 1^2 + 1^2 i to ti je rješenje.",
      "votes": {
        "upvoters": [
          "Rene",
          "sheriffHorsey"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255865": {
      "poster": "viliml",
      "content": "@\"Ivančica\"#p255840 \n\n[math]w_1 = \\sum_{i=1}^{N} \\alpha_i y^{(i)} x^{(i)}_1[/math]",
      "votes": {
        "upvoters": [
          "-Ivan- (Ivančica)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255873": {
      "poster": "[deleted]",
      "content": "![](assets/2021-11-20/00043.png)\n\nu d) podzadatku i generalno takvim tipovima zadataka, jel uvijek gledamo s dummy jedinicom? odnosno treba li nam u slučaju bez preslikavanja 4 primjera? (3 značajke + dummy)\n\nili ako nije direktno naglašeno da je preslikavanje (1, x) ignoriramo?",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255874": {
      "poster": "viliml",
      "content": "@\"Rene\"#p255847 Za drugi zadatak, uvijek je moguće barem teoretski izračunati udaljenost\n\n[math]d(\\mathbf{x})=\\frac{h(\\mathbf{x})}{||\\mathbf{w}||}=\\frac{\\sum_i{\\alpha_i y^{(i)} \\kappa(\\mathbf{x},\\mathbf{x}^{(i)} )}+w_0}{\\sqrt{\\sum_i{\\sum_j{\\alpha_i \\alpha_j y^{(i)} y^{(j)} \\kappa(\\mathbf{x}^{(i)}, \\mathbf{x}^{(j)} )}}}}[/math]\n\nMožda se referenciraju na to da je prostor značajki za Gaussovu jezgru neprebrojivo-beskonačno-dimenzionalan pa sumiranje po dimenzijama i slične stvari nisu zapravo definirane, ali s konkretnom hipotezom će se sve događati u konačno- ili prebrojivo-beskonačno-dimenzionalnom podprostoru gdje sve sume koje nam trebaju konvergiraju...",
      "votes": {
        "upvoters": [
          "Rene"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255888": {
      "poster": "prx_xD",
      "content": "@\"viliml\"#p254704 jel možeš objasniti kako K(x, z) = (xT*z  + 2)^3 rješiš",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255912": {
      "poster": "viliml",
      "content": "@\"prx_xD\"#p255888 To je formula. Staviš brojeve unutra i izađu brojevi van.\n\nNa primjer [imath]\\kappa(\\mathbf{x}^{(3)}, \\mathbf{x})=17^3[/imath].\n\nMnoge vrijednosti će biti u milijunima ali zato su koeficijenti jako mali.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Bucc (Olive Oil)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "255948": {
      "poster": "[deleted]",
      "content": "> Prošli tjedan bavili smo se regresijom te smo razmatrali linearan model regresije, koji – uz odabir nelinearne funkcije preslikavanja – zapravo postaje nelinearan model.\n\n\n> Ovaj model sada ima linearnu granicu u prostoru značajki, ali nelinarnu granicu u ulaznom prostoru. Tehnički gledano to je i dalje linearan model, jer je linearan u parametrima.\n\nšta je od ovog onda na kraju, jel se model smatra linearnim ili ne lol",
      "votes": {
        "upvoters": [
          "MsBrightside",
          "angello2",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255964": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Todd Chavez\"#p255948 Pa... I dalje je to linearan model jer on u prostoru značajki stvara linearnu granicu. Nelinearnost nastaje prilikom preslikavanja u prostor značajki, a to nije unutar \"okvira djelovanja\" modela. Bar si ja tako tumačim...",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255968": {
      "poster": "[deleted]",
      "content": "@\"Precious Bodily Fluids\"#p255964 da, kuzim, zato me i muči ovaj \"postaje nelinearan model\" dio",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255973": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Todd Chavez\"#p255968 poanta je valjda da ako gledaš to kao crnu kutiju zaključit ćeš da je nelinearan model, al zapravo se radi o preslikavanju koje se feeda u običan linearan model. Tako da su vjerojatno oba pogleda točna, samo treba biti svjestan konteksta.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "255985": {
      "poster": "[deleted]",
      "content": "![](assets/2021-11-21/00007.png)\n\njel nije poanta doslovno bila da ako promijenimo funkciju gubitka da to više nije linearna regresija? \n\ntakođer, jel promjena aktivacijske funkcije ne spada u promjenu modela?",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256002": {
      "poster": "kix7 (Fish99)",
      "content": "@\"Todd Chavez\"#p255985 Mislim da oce rec da nas je najvise mucio onaj kvadratni gubitak i da je dovoljno da njega promijenimo da ce funkcionirat linearna regresija a uz to dolazi i promjena optimizacijskog postupka.\n\nAl da, lakse je opravdat odgovor kad znas vec rjesenje...",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256008": {
      "poster": "tomekbeli420",
      "content": "@\"Ivančica\"#p255840 e sry tek sad vidio\n\nvektorsko zbrajanje, dakle [imath]\\alpha_i[/imath] i [imath]y^{(i)}[/imath] su skalari, a [imath]\\mathbf{x}^{(i)}[/imath] su vektori\n\n[math]\\mathbf{w} = \\alpha_1 y^{(1)} \\mathbf{x}^{(1)} + \\alpha_2 y^{(2)} \\mathbf{x}^{(2)} + \\alpha_3 y^{(3)} \\mathbf{x}^{(3)} \\\\\n\\mathbf{w} = 0 \\cdot (-1) (-1, 3, 6) + 0.01 \\cdot (-1) (-4, 4, 4) + 0.01 \\cdot 1 (-2, 4, 1) \\\\\n\\mathbf{w} = (0, 0, 0) + (0.04, -0.04, -0.04) + (-0.02, 0.04, 0.01) = (0.02, 0, -0.03)[/math]\n\nI onda kako je [imath]\\mathbf{w} = \\left(w_1, w_2, w_3\\right)[/imath] odavdje iščitaš sve šta te zanima",
      "votes": {
        "upvoters": [
          "-Ivan- (Ivančica)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256012": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Todd Chavez\"#p255985 možda je greška? Valjda se implicira da se nabroje promjene koje iz lin. reg. daju perceptron, a među njima je dodavanje aktivacijske funkcije u model, čime se on mijenja.",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256040": {
      "poster": "viliml",
      "content": "@\"Todd Chavez\"#p255985 @\"Precious Bodily Fluids\"#p256012 Kad se linearna regresija koristi za klasifikaciju, odmah već mora imati aktivacijsku funkciju. U skripti se koristi funkcija [imath]\\mathbf{1}\\{\\alpha\\ge0.5\\}[/imath], ali to je ekvivalentno step funkciji do na promjenu w0 i preimenovanje izlaznih klasa iz 0/1 u =+-1 (@\"Jaster111\"#p255798 ) znači algoritam ekvivalentan perceptronu se može dobiti bez promjene aktivacijske funkcije.",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256052": {
      "poster": "Zuzu (Coffe123)",
      "content": "@\"viliml\"#p254704 možeš reć molim te kako dobiješ vrijednosti za ostale vektore, znači -1.97711, -1.92673 i 0.05117..nikako ne mogu dobiti te vrijednosti formulama pa nešto vjv krivo radim",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256057": {
      "poster": "-Ivan- (Ivančica)",
      "content": "@\"tomekbeli420\"#p254646 \n\nhvala na odgovoru, sam mi još jedna stvar nije baš jasna, kako si dobio ove brojeve (zaokruženo plavom)\n\n![](assets/2021-11-21/00021.png)\n\nnemrem skužit kaj si s čim pomnožio, što god pokušam gledajući ovu formulu ne dobim te brojeve",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256059": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "Jel zna netko ovo objasniti? Iz zadataka logističke regresije I\n> možemo li reći da je logistički gubitak konveksni surogat gubitka 0-1, i što to znači?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256064": {
      "poster": "zara",
      "content": "![](assets/2021-11-21/00024.png)\n\nJel zna netko?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256070": {
      "poster": "zastozato (studoš)",
      "content": "jel ima netko rjesen 2. iz  Jezgrenih metoda zadatci za učenje",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256071": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"zara\"#p256064 \n\nIz poznate vrijednosti fje gubitka i proizvoljne oznake y možeš izračunati izlaz modela h(x), nakon toga pomoću izračunatog h(x) i suprotne vrijednosti y izračunaš gubitak.\n\n![](assets/2021-11-21/00025.jpg)",
      "votes": {
        "upvoters": [
          "Kasperinac",
          "Ollie",
          "zara"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256072": {
      "poster": "viliml",
      "content": "@\"Precious Bodily Fluids\"#p256059 To je jedan termin koje je definiran tek kasnije, vjerojatno su ga greškom stavili u tu vježbu.\n\n![](assets/2021-11-21/00026.png)",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256075": {
      "poster": "viliml",
      "content": "@\"zara\"#p256064 Zamjena oznake znači invertiranje vjerojatnosti, pa se u gubitku zamjeni -log(p) sa -log(1-p). Dakle rješenje je -log(1-exp(-L)).",
      "votes": {
        "upvoters": [
          "zara"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256081": {
      "poster": "viliml",
      "content": "@\"Ivančica\"#p256057\n\n[imath]h \\left(\\mathbf{x}\\right) = \\mathbf{w}^{\\mathrm{T}} \\mathbf{x} + w_0[/imath]\n\nVrijednosti [imath]\\mathbf{w} [/imath] i [imath] w_0[/imath] su pokazane izračunate gore u njegovom odgovoru, a primjeri [imath]\\mathbf{x}[/imath] su zadani u zadatku.",
      "votes": {
        "upvoters": [
          "-Ivan- (Ivančica)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256088": {
      "poster": "tomekbeli420",
      "content": "@\"Ivančica\"#p256057 to su izlazi modela\n\nA s obzirom da smo našli težine [imath]\\mathbf{w} = (0.02, 0, -0.03)[/imath], i u zadatku nam je dan [imath]w_0 = -0.8[/imath], onda hipotezu možemo računati sa primarnom formulacijom modela, koji je običan linearan model\n\n[math]h \\left(\\mathbf{x}\\right) = \\mathbf{w}^{\\mathrm{T}} \\mathbf{x} + w_0[/math]\n\ntako npr za primjer [imath] \\left(\\mathbf{x}^{(3)}, y^{(3)}\\right) = \\left((-2, 4, 1) , +1\\right)[/imath] izlaz modela je\n\n[math]h \\left(\\mathbf{x}^{(3)}\\right) = (0.02, 0, -0.03) \\cdot (-2, 4, 1) - 0.8 = -0.87[/math]",
      "votes": {
        "upvoters": [
          "-Ivan- (Ivančica)",
          "matt (Matt)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256106": {
      "poster": "-Ivan- (Ivančica)",
      "content": "@\"viliml\"#p256081 \n\n@\"tomekbeli420\"#p256088 \n\nhvala puno, sad kužim",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256108": {
      "poster": "[deleted]",
      "content": "![](assets/2021-11-21/00032.png)\n\nmoze netko objasniti ovaj?",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256112": {
      "poster": "zara",
      "content": "![](assets/2021-11-21/00034.png)\n\n Jel ima netko postupak i za ovo dvoje pliz🥺",
      "votes": {
        "upvoters": [
          "Ardura (Maddy)",
          "angello2"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256113": {
      "poster": "Arya",
      "content": "![](assets/2021-11-21/00035.png)\n\nZna netko ovaj? Zasto najmanja greska nije 0, a najveca 5/6 (po formuli za empirijsku gresku, N=6 i racunamo da je sve krivo klasificirano 0.5+0.5+1+1+1+1) ?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256115": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Todd Chavez\"#p256108 objasnio je papa na predavanju, možeš pogledati na teamsu, predavanje 11.3.2021, pred kraj negdje.\n\nTLDR: aktivacijska funkcija ne utjece na linearnost granice, ono što treba zadovoljiti je da je preslikavanje obavljeno linearno, a to je slučaj za phi(x) = (1, x)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256116": {
      "poster": "[deleted]",
      "content": "@\"Arya\"#p256113 ako nacrtas vidjet ces da nisu linearno odvojivi tako da ces uvijek imati barem jedan krivo klasificirani, i uzmeš da je ovaj tip koji iznosi 0.5.\n\na za maksimalnu pogrešku: uvijek ćeš s neke strane ravnine imati barem jedan točan primjer",
      "votes": {
        "upvoters": [
          "Arya",
          "Ducky",
          "MsBrightside"
        ],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256117": {
      "poster": "Fica (Prof)",
      "content": "@\"Arya\"#p256113 Takve zadatke si moraš skicirati na kocki i onda dobiješ da su ti svi ovi pozitivni na 4 dijagonalna vrha, a negativni su na suprotnim stranama pa ih najbolje možeš odvojiti tako da ti se jedan negativni klasificira lažno pozitivno i onda je to 0.5/6, a max će ti biti kad je obrnuto i onda ćeš imati 4 lažno negativna i 1 lažno pozitivan pa dobiješ 4.5/6",
      "votes": {
        "upvoters": [
          "Arya",
          "Ducky"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256120": {
      "poster": "[deleted]",
      "content": "![](assets/2021-11-21/00036.png)\n\njel se moze ovo rijesit bez da s onog grafa usporedbe pogadas tocnu decimalnu vrijednost",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [
          "netko_tamo",
          "viliml"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "256122": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Todd Chavez\"#p256120 \n\n10-ak poruka iznad je postupak, evo\n\n@\"Precious Bodily Fluids\"#p256071",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256125": {
      "poster": "viliml",
      "content": "@\"Todd Chavez\"#p256108 pročitaj dretvu @\"viliml\"#p255782",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256130": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "![](assets/2021-11-21/00038.png)\n\nKada piše da se značajke množe sa dva, pretpostavljam da se množe sve značajke osim dummy značajke?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256134": {
      "poster": "[deleted]",
      "content": "@\"Precious Bodily Fluids\"#p256130 da, w0 tretiraš posebno",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256184": {
      "poster": "Ardura (Maddy)",
      "content": "@\"zara\"#p256112  U auditornim od logreg2 od 1:20 Snajder detaljno rj predzadnji, zadnji je nakon toga nacrtao, ali nije rekao tocan odg (njemu je bilo zacrnjeno 49 kao tocno, a nama je 45, vjv je greska).",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256185": {
      "poster": "gad_gadski",
      "content": "![](assets/2021-11-21/00043.png)\n\n Netko ima postupak?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256187": {
      "poster": "pvmnt (ostaje.mi.to.sto.se.volimo)",
      "content": "![](assets/2021-11-21/00044.png)\n\nJel zna itko ovaj?\n\nJa bi rekao da je rjesenje: najgori slucaj za OVO je kad se usporeduju 2 klase po 400. Buduci da pohranjujemo pola matrice bez dijagonale to je ukupno (400 puta 400 - 400)/2\n\nOVR uvijek ukljucuje sve elemente: dakle to je (1000 puta 1000 - 1000)/2 pohranjenih elemenata\n\nprvi broj / drugi broj je tocno 4 puta manji od pravog rjesenja... di grijesim?",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256189": {
      "poster": "steker",
      "content": "![](assets/2021-11-21/00045.jpg)\n\nKako se ovo dobije",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256195": {
      "poster": "Fica (Prof)",
      "content": "@\"legend649\"#p256187 Za OVO si uzeo samo jednu klasu, a trebalo bi usporediti dvije najveće pa dobiješ 800 primjera, a ne 400. I inače jedan tip za sve, veliku većinu tih zadataka sa ispita imate riješenih na teamsima na videu od tjedna gdje se to radilo ili na rekapitulaciji.",
      "votes": {
        "upvoters": [
          "Sulejman",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256196": {
      "poster": "steker",
      "content": "@\"legend649\"#p256187 jel nebi za ovo trebala 800x800 matica da bi bila simetricna",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256199": {
      "poster": "pvmnt (ostaje.mi.to.sto.se.volimo)",
      "content": "@\"steker\"#p256196 bravo. sjeban sam, od jutros sam za laptopom. hvala",
      "votes": {
        "upvoters": [
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256203": {
      "poster": "prx_xD",
      "content": "![](assets/2021-11-21/00047.png)\n\nMože netko dat postupak za ovaj zadatak nije mi jasno kak ga rijesit\n\nTocan odg je pod B",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256210": {
      "poster": "steker",
      "content": "@\"prx_xD\"#p256203 ![](assets/2021-11-21/00048.jpg)\n\nValjda je to to",
      "votes": {
        "upvoters": [
          "JogaBonito",
          "Smolaa",
          "cloudies",
          "pingvinka",
          "prx_xD"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "Bananaking"
        ]
      }
    },
    "256212": {
      "poster": "prx_xD",
      "content": "@\"steker\"#p256210 hvala",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256214": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"steker\"#p256210 odvratan zadatak lol",
      "votes": {
        "upvoters": [
          "-Ivan- (Ivančica)",
          "Bananaking",
          "cloudies",
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "steker"
        ]
      }
    },
    "256215": {
      "poster": "bodNaUvidima",
      "content": "@\"gad_gadski\"#p256185 predavanje na teamsu iz te lekcije Šnajder rješi isti zadatak samo druge brojke.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256220": {
      "poster": "steker",
      "content": "moze neko pametan mi malo pojasnit zasto se minimizira druga norma od W za rjesenje maksimalne margine, tj. Zasto se potporni vektori udaljavju vise od hiperravnine sto je W manji. Nikako mi to ne ide u glavu baš",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256226": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"steker\"#p256220 dosta je dense izvod, probaj shvatit svaki korak pojedinačno.\n\nKeypointovi za razumjet su (SVM 1, poglavlje 1.1):\n- definicija udaljenosti primjera od hiperravnine\n- izraz za maksimalnu marginu (minimizacija unutar argmax-a),\n- uvođenje pretpostavke da je udaljenost potpornih vektora od klasifikacijske granice jednaka 1,\n- činjenica argmax(1 / |\\|w|\\|)  ekvivalentna argmin ||w|\\|.",
      "votes": {
        "upvoters": [
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256253": {
      "poster": "viliml",
      "content": "@\"prx_xD\"#p256203 @\"steker\"#p256210 @\"Precious Bodily Fluids\"#p256214\n\nNije toliko ružno ako zguraš sve detalje linearno algebarskih računa pod tepih. Na primjer vrijednosti hipoteze za sve primjere možeš odmah izračunati trikom da daš jezgri za argument matricu umjesto vektora, i sve one sume s alfama i ipsilonima se isto mogu prikazati kao matrično množenje.\n\nOvo je isti zadatak s drugačijim krajnjim primjerom x:\n\n![](assets/2021-11-21/00054.png)",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256281": {
      "poster": "AK10 (endyyyy)",
      "content": "![](assets/2021-11-22/00000.png)\n\nkako?",
      "votes": {
        "upvoters": [
          "Arya",
          "nnn (dinoo)",
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256287": {
      "poster": "Dootz",
      "content": "![](assets/2021-11-22/00001.png)\n\nMože li netko objasniti ovo?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256296": {
      "poster": "123 (FERella)",
      "content": "Zna li netko zašto ovdje ne bi mogao bit odgovor pod D?\n\n![](assets/2021-11-22/00002.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256308": {
      "poster": "Rene",
      "content": "@\"FERella\"#p256296 jer ne znamo kako regularizacija utjece na model, mozda je s tolikim faktorom model jos jednostavniji od H2,0",
      "votes": {
        "upvoters": [
          "123 (FERella)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256309": {
      "poster": "nnn (dinoo)",
      "content": "@\"FERella\"#p256296 Neka me netko ispravi ali sjećam se da sam pričao sa asistenticom i stvar je da ne možeš ništa reći o generalizaciji hipoteza, i ovaj drugi dio \"imat će manju pogrešku na skupu za učenje\" ne mora nužno bit točno. Možda je lambda=100 prevelik faktor i greška je puno veća",
      "votes": {
        "upvoters": [
          "123 (FERella)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256315": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"endyyyy\"#p256281 \n\nIz fje gubitka izračunaš h(x). Pomoću h(x) i w0 izraziš w^T x, zatim ga pomnoži s 2 i na kraju novi h(x) = 2w^T x + w0 i obrnutu oznaku ubacuješ nazad u funkciju gubitka i to je rješenje.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256317": {
      "poster": "AK10 (endyyyy)",
      "content": "@\"Precious Bodily Fluids\"#p256315 to radim ali iz nekog razloga mi ne ispada dobro, ako mozes uslikat postupak bila bih jako zahvalnaa",
      "votes": {
        "upvoters": [
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256345": {
      "poster": "tomekbeli420",
      "content": "@\"Dootz\"#p256287 Da bi računao udaljenost od hiperravnine u prostoru značajki trebaju ti težine i preslikani primjeri. Funkcija preslikavanja je implicitno poznata iz korištene jezgre (kako se radi o algoritmu SVM onda možemo biti sigurni da je jezgra Mercerova), pa onda iz Lagrangeovih multiplikatora [imath]\\alpha_i[/imath] i preslikanih ulaznih primjera se lako dobiju težine [imath]\\mathbf{w}[/imath]. Dakle to je sve moguće, jedino problem sa Gaussovom jezgrom ili složenijim koje koriste Gaussovu jezgru kao gradivni blok jest taj da je preslikavanje beskonačno dimenzijsko, pa je to valjda (nisam ni ja siguran lmao) razlog zašto se ne može sa Gaussovom jezgrom.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256347": {
      "poster": "[deleted]",
      "content": "![](assets/2021-11-22/00006.png)\n\nmoze netko objasnit zasto je ovo C, a ne npr D",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256350": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Todd Chavez\"#p256347 \n\nD nije tocan jer se pozicija hiperravnine ne mijenja porastom vektora w, isto vrijedi i za udaljenost primjera od hipperavnine, mnozenje vektora skalarom ne utjece na udaljenost.\n\nOno što se mjenja je izlaz modela, h(x) i on raste proporcionalno porastom w.",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256375": {
      "poster": "BillIK",
      "content": "![](assets/2021-11-22/00007.png)\n\nBi li u ovome zadatku isključili x4 jer je u zavisnosti s x3 (ako dobro razmisljam)? \n\nKoliko bi bilo rješenje jer zbunjuje me ovo s parovima i trojkama kvadrata. Uzimamo li u obzir da npr. za par imamo mogucnost birati dva elementa od 6 mogucih ili od 12?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256381": {
      "poster": "angello2",
      "content": "@\"BillIK\"#p256375 to su dva odvojena racuna, nisu u ovisnosti\n\nreko bi da je x7 suvisan jer ga mozes izracunat iz ostalih podataka - imas ukupan iznos kredita i imas iznos otplacenih, ne treba ti podatak koliko je preostalih. istom logikom se moze maknut bilo koji od x5, x6, x7 - dovoljna su dva od ta tri.",
      "votes": {
        "upvoters": [
          "BillIK",
          "MsBrightside"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256384": {
      "poster": "viliml",
      "content": "@\"tomekbeli420\"#p256345 Nije strogo nužno eksplicitno izračunati težine niti koristiti funkciju preslikavanja pošto se sve može prikazati preko unutarnjih produkata/jezgri, vidi @\"viliml\"#p255874 \n\nJedino je možda problem da ta formula vrijedi *ako udaljenost postoji*, možda se može reći da udaljenost ne postoji pa ta formula iako daje neki broj nije baš rigorozno točno zvati \"udaljenost\"?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256409": {
      "poster": "gad_gadski",
      "content": "@\"endyyyy\"#p256317 ![](assets/2021-11-22/00010.jpg)\n\nPretpostavljam da nisi stavila izraz -(w*x + w0) u potenciju kod formule za h(x), vec samo -w*x",
      "votes": {
        "upvoters": [
          "AK10 (endyyyy)",
          "Jale (čakijale)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256421": {
      "poster": "matt (Matt)",
      "content": "V07 4. Zadatak: Svi poopćeni linearni modeli mogu se trenirati u “online” (pojedinačnom) načinu, primjernom\n\nalgoritma LMS. To vrijedi i za algoritam linearne regresije, za koji smo prvotno kao minimizaciju\n\nkvadrata provodili računajući pseudoinverz matrice dizajna. Jedna od prednosti algoritma LMS\n\nu odnosu na izračun pseudoinverza kod linearne regresije je manja računalna složenost LMS-a.\n\nNeka E označava broj epoha, N je broj primjera, a m broj značajki u prostoru značajki. Koja je\n\n(vremenska) računalna složenost algoritma LMS, primijenjenog na linearnu regresiju?\n\n![](assets/2021-11-22/00011.png)\n\nIz predavanja:\n\n![](assets/2021-11-22/00012.png)\n\nNije mi jasno zašto se pojavljuje izraz [imath]m[/imath], tj. koja operacija uzrokuje tu kompleksnost?\n\nJe li to operacija preslikavanja [imath]\\phi(x)[/imath] ili izračun skalarnih produkta unutar izraza za ažuriranje težine [imath]w[/imath]?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256432": {
      "poster": "RickyMorty",
      "content": "![](assets/2021-11-22/00013.jpg)\n\nKak se ovo točno radi, tojest nije mi jasno odkud mi po dvije težine za x1 itd...",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256445": {
      "poster": "Antuunn",
      "content": "@\"zara\"#p256112 Jel pronadeno rjesenje za ova dva zadatka?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256447": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"RickyMorty\"#p256432 Ovaj zadatak je detaljno objašnjen na teams predavanjima.",
      "votes": {
        "upvoters": [
          "RickyMorty",
          "kix7 (Fish99)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256452": {
      "poster": "gad_gadski",
      "content": "![](assets/2021-11-22/00018.png)\n\nOvo je mi s materijala na studosima, u rj pise da je A tocno, jel to krivo ili? Ja nevidim kako nije C",
      "votes": {
        "upvoters": [
          "angello2"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256457": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"gad_gadski\"#p256452 \n\nGrafički se lagano rijesi\n\n![](assets/2021-11-22/00020.jpg)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256458": {
      "poster": "jobi (azex)",
      "content": "@\"BillIK\"#p256375 \n\n@\"angello2\"#p256381 \n\nmozete li napisat postupak pls, ne kuzim nikako kako doc do rjesenja",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256463": {
      "poster": "bodNaUvidima",
      "content": "@\"tomekbeli420\"#p254646 Je si li uspio ovdje doci do njihovog rjesenja? Takoder mi h(x) ispada 1.06 x [imath]10^{-3}[/imath] + w0, a w0 mi ispada 0.999999 po njihovoj formuli.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256475": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"bodNaUvidima\"#p256463 rješio ga je Šnajder na predavanju",
      "votes": {
        "upvoters": [
          "bodNaUvidima"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256476": {
      "poster": "steker",
      "content": "![](assets/2021-11-22/00023.jpg)\n\nKako se ovo rjesava",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256477": {
      "poster": "[deleted]",
      "content": "@\"steker\"#p256476 značajke == klasifikatori, i onda kad napravis K, odnosno K povrh 2 vidiš da OVO ima duplo, a OVR lošije radi na \"znanost\" kategoriji jer je osjetljiv na class imbalance, odnosno ima jako malo primjera pa ih \"trpa\" u većinsku klasu i tako griješi",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "steker"
        ],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256479": {
      "poster": "steker",
      "content": "@\"Todd Chavez\"#p256477 aha zbunjivalo me taj dio sa znacajkama",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256487": {
      "poster": "batman3000",
      "content": "E zna netko?\n\n![](assets/2021-11-22/00024.png)",
      "votes": {
        "upvoters": [
          "ZliDule"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256491": {
      "poster": "viliml",
      "content": "@\"matt\"#p256421 Za svaku epohu, za svaki primjer, svaka težina se mora ažurirati.\n\nOvo u tvojoj drugoj slici je vektorska jednadžba koja je skraćeni prikaz za [imath]m[/imath] zasebnih operacija.",
      "votes": {
        "upvoters": [
          "matt (Matt)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256494": {
      "poster": "viliml",
      "content": "@\"bodNaUvidima\"#p256463 [imath]h(\\mathbf x)[/imath] ti je točan, kako računaš w0 da ti ispadne krivo? Trebao bi takav da je [imath]h(\\mathbf x^{(i)})=y^{(i)}[/imath]. Što dobiješ za [imath]h(\\mathbf x^{(i)})[/imath]?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256523": {
      "poster": "bodNaUvidima",
      "content": "@\"viliml\"#p256494 w0 pokušavam izračunati preko ove formule iz P08 pdfa\n\n![](assets/2021-11-22/00029.png)\n\nAko uzmem 3. potporni vektor sa oznakom y=+1, uvrstim u tu formulu dobijem w0 = 0.999998026, svakako predaleko od 0.95.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256526": {
      "poster": "bodNaUvidima",
      "content": "@\"bodNaUvidima\"#p256523 Da odgovorim sam sebi, imao sam brain fart i nisam koristio kernel funkciju nego samo mnozio dva vektora.",
      "votes": {
        "upvoters": [
          "Me1 (Me)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256528": {
      "poster": "ppooww (pp)",
      "content": "![](assets/2021-11-22/00031.png)\n\nJel ovo znaci da mozemo 2 pitanja krivo zaokruzit bez negativnih? Ili tocni odgovori nose 35/20 bodova, a krivi odgovori -1/3 * 35/20 ?",
      "votes": {
        "upvoters": [
          "ZliDule",
          "gad_gadski"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256535": {
      "poster": "Reznox",
      "content": "![](assets/2021-11-22/00032.png)\n\nIma ko postupak?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256545": {
      "poster": "boogie_woogie (nika_1999)",
      "content": "@\"Reznox\"#p256535 \n\n![](assets/2021-11-22/00033.jpg)",
      "votes": {
        "upvoters": [
          "Reznox"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256547": {
      "poster": "boogie_woogie (nika_1999)",
      "content": "Zna netko ovaj?\n\n![](assets/2021-11-22/00034.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256558": {
      "poster": "Sulejman",
      "content": "@\"pp\"#p256528 Rekao bih da dobiješ negativne ako krivo odgovoriš, sumnjam da bi ti dopustili da dvaput besplatno pogađaš. Ali da, ak odgovoriš na 21 pitanje točno onda možeš fulat jedno 😅",
      "votes": {
        "upvoters": [
          "ppooww (pp)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256559": {
      "poster": "angello2",
      "content": "![](assets/2021-11-22/00036.png)\n\n![](assets/2021-11-22/00037.png)\n\n![](assets/2021-11-22/00038.png)\n\nmoze neki savjet oko ovakvih zadataka di se trazi broj parametara? krenem ja tu nesto brojat i dobijem neki broj al nikad ne dobim tocno rjesenje tak da mi ocito nesto fali",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256572": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"angello2\"#p256559 zadnji je rješio Medić, na identičan način se rješi i prvi. drugi je rješio tatica, isto na predavanju.\n\nUostalom, većina zadataka je rješena na predavanjima, a ona su sva su dostupna u repozitoriju na teamsu.",
      "votes": {
        "upvoters": [
          "kix7 (Fish99)",
          "viliml"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256578": {
      "poster": "zastozato (studoš)",
      "content": "![](assets/2021-11-22/00041.png)\n\novaj? zašto?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256579": {
      "poster": "zastozato (studoš)",
      "content": "@\"studoš\"#p256578 točan odg je C",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "Kaiser"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256583": {
      "poster": "steker",
      "content": "@\"studoš\"#p256578 ja sam to shvatila ovako: model ima 6 svojih wj tezina. Imamo 5 fi j parametriziranih baznih funkcija gdje svaka ima n+1 tezina tj svaka ima 101 tezinu, jos dodatno imamo i tu fi 0 baznu funkciju koju na predavanju nismo parametrizirali pa onda imamo: 5*101 tezina baznih funkcija + 6 tezina modela= 511. Valjda...",
      "votes": {
        "upvoters": [
          "sheriffHorsey",
          "zastozato (studoš)"
        ],
        "downvoters": [
          "matt (Matt)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256592": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"steker\"#p256583 Znači imamo 5 baznih funkcija koje preslikavaju na način da implementiraju logističku regresiju, dakle svaka 100 + 1 parametara? A što točno predstavlja [imath]\\phi_0[/imath]? Jel to simulira dummy značajku, as in [imath]\\phi(x) = 1[/imath], pa nema parametara? Ima smisla, al ne vidim tu pretpostavku nigdje u skripti predavanja tho...",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256596": {
      "poster": "viliml",
      "content": "@\"angello2\"#p256559 vidi @\"Rene\"#p255723",
      "votes": {
        "upvoters": [
          "angello2"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256597": {
      "poster": "steker",
      "content": "@\"Precious Bodily Fluids\"#p256592 nez ja znam da je snajder reko kad je rjesavo slican zadatak da je to dogovorno jednako 1. Ali ocito bi trebalo simulirat nesto valjda kao prag u neuronskoj mrezi lmao",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256641": {
      "poster": "InCogNiTo124",
      "content": "@\"Precious Bodily Fluids\"#p256592 phi_0 je po dogovoru uvijek 1 to se sjecam da je nama govorio",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256652": {
      "poster": "zastozato (studoš)",
      "content": "![](assets/2021-11-22/00052.png)\n\na ovaj? odg je a",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256654": {
      "poster": "steker",
      "content": "@\"studoš\"#p256652 valjda jer je sum malen mozes rec da je y= -1+2x\n\nZa optimizacijski postupak znas da zelis minimizirat (y+2h(x))^2 pa izjednacis sa 0 valjda\n\nKorjenujes tu jednadzbu i dobijes 2h(x)=-y\n\n2h(x)=-(-1+2x)\n\nh(x)=1/2 -1x",
      "votes": {
        "upvoters": [
          "Tone"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256657": {
      "poster": "gladiator",
      "content": "@\"Precious Bodily Fluids\"#p256592 phi_0 je 1, to je jasno kao dan, ali upitan je w0. Zato taj 101. parametar",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256658": {
      "poster": "Arya",
      "content": "![](assets/2021-11-23/00000.png)\n\nKako se racuna klasifikacija za 3-NN?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256670": {
      "poster": "tomekbeli420",
      "content": "@\"steker\"#p256654 oof, ne znam je li to legitimno objašnjenje lmao al ispada dobro\n\nanyways\n\n@\"studoš\"#p256652 \n\nznači ako gledaš taj krivi gubitak i malo ga prepraviš\n\n[math]L \\left(y, h \\left(\\mathbf{x}\\right) \\right) = \\left(y + 2 h\\left(\\mathbf{x}\\right)\\right)^2 = 4\\left(\\frac{y}{2} +  h\\left(\\mathbf{x}\\right)\\right)^2 = 4 \\left(- \\left(-\\frac{y}{2}\\right) + h\\left(\\mathbf{x}\\right)\\right)^2 = 4 \\left( h\\left(\\mathbf{x}\\right) - \\left(-\\frac{y}{2}\\right)\\right)^2[/math]\n\nprimijećujemo da kada bi svaka oznaka [imath]y[/imath] bila pretvorena u [imath]-\\frac{y}{2}[/imath] da bi onda takva \"kriva\" funkcija odgovarala funkciji kvadratnog gubitka i onda bi takav prema metodi najmanjih kvadrata vratio očekivane parametre. Ovaj faktor 4 nije bitan zbog optimizacijskog postupka.\n\nZaključak: ako uzmemo podatke, transformiramo svaki tako da [imath]\\mathbf{x}[/imath] ostane kakav je, ali [imath]y[/imath] promijenimo u [imath]-\\frac{y}{2}[/imath], onda je implementirana funkcija kvadratni gubitak i onda će naučiti parametre za generiranje transformiranih podataka.\n\nDakle samo se oznaka mijenja, kako je originalno bilo da su [imath]y[/imath] uzorkovane iz [imath]\\mathcal{N} \\left(-1 + 2x, \\sigma^2\\right)[/imath], onda skaliranjem se samo mijenja očekivanje, prema tome [imath]-\\frac{y}{2}[/imath] onda dolazi iz distribucije [imath]\\mathcal{N} \\left(\\frac{1}{2} - x, \\sigma^2\\right)[/imath]\n\nI iz toga je vidljivo da ćemo onda dobiti težine [imath] \\left(w_0, w_1\\right) = \\left(\\frac{1}{2}, -1\\right) [/imath]",
      "votes": {
        "upvoters": [
          "Jale (čakijale)",
          "Rene",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "funky_funghi",
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256682": {
      "poster": "Rene",
      "content": "@\"tomekbeli420\"#p256670 drugi način, više računski a manje kreativan, je da napisete funkciju pogreške [imath]E(w|D)=(2X\\vec{w}+\\vec{y})^T(2X\\vec{w}+\\vec{y}) [/imath] i izvedete rjesenje najmanjih kvadrata gdje cete dobit [imath] \\vec{w}' = \\frac{-1}{2}w [/imath] pa isti zakljucak kao kolega",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Unity (Sgt. Forge)",
          "kix7 (Fish99)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "256684": {
      "poster": "viliml",
      "content": "@\"tomekbeli420\"#p256670 @\"Rene\"#p256682 Isusek mileni koji je ovo autizam.\n\nTreba se riješiti zadatak, ne napisati znanstveni rad pobogu.\n\nVidiš da algoritam ima gubitak [imath](y-g(\\mathbf x))^2[/imath] gdje je [imath]g(\\mathbf x)=-2h(\\mathbf x)[/imath]. [imath]g(\\mathbf x)[/imath] će konvergirati na [imath]-1+2x[/imath] jer je to ispravna linearna regresija, pa se onda vidi [imath]h(\\mathbf x) = -\\frac{1}{2}g(\\mathbf x)=\\frac{1}{2}-x[/imath] i gotov si.",
      "votes": {
        "upvoters": [
          "Unity (Sgt. Forge)",
          "cajaznun"
        ],
        "downvoters": [
          "Diego"
        ]
      },
      "reactions": {
        "haha": [
          "Fica (Prof)",
          "Lyras",
          "Rene",
          "Sulejman",
          "Unity (Sgt. Forge)",
          "angello2",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "kix7 (Fish99)",
          "member",
          "steker"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "256693": {
      "poster": "tomekbeli420",
      "content": "@\"viliml\"#p256684 doslovno si istu stvar napravio, a ja sam onda autist lmao",
      "votes": {
        "upvoters": [
          "JustinCase"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Ducky",
          "Fica (Prof)",
          "Lyras",
          "Rene",
          "angello2",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "matt (Matt)",
          "member",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
          "steker"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "256745": {
      "poster": "Rene",
      "content": "Jel iko uspio onaj s kreditima potrosio sam 15 minuta i nikako dobit\n\nMoja logika je 6 nezavisnih varijabli\n\n6 linearnih + 6 kvadrata + 4 kombinacije za svaku od 15 dvojki + 8 kombinacija za svaku od 20 trojki",
      "votes": {
        "upvoters": [
          "Bananaking"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256748": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Rene\"#p256745 dvije su multikolinearne barem, starost<- >prihodi i neplaceni-placeni krediti, al idk nisam uspio rjesit",
      "votes": {
        "upvoters": [
          "Rene"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256751": {
      "poster": "Rene",
      "content": "@\"Precious Bodily Fluids\"#p256748 isprobao sam i za 5 i za 4 nezavisne i svejedno nisam dobia nista od ponudenog, tako da mislim da je greska u prebrojavanju\n\nCak sam probao i izbacit dvojke i trojke di su sve kvadratne opet nista",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256752": {
      "poster": "angello2",
      "content": "@\"Rene\"#p256745 ja sam nekak zakljucio da je 5 varijabli, s obzirom da za 6 varijabli ispadnu puno puno veci rezultati od ponudenog, pa je valjda x2 suvisan jer kao iz stanja racuna mozes dobit prinose na racun (ne bas al aj...) \n\nnaravno problem je bio da cak i za 5 varijabli dobim 130 znacajki a najvise ponudeno je bilo 92\n\nza 4 varijable sam dobio 64 sta isto nije ponudeno\n\nsve u svemu wtf",
      "votes": {
        "upvoters": [
          "PaleAle",
          "Rene",
          "[deleted]",
          "cajaznun",
          "miss_anthropocene (neunist.iva)",
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256763": {
      "poster": "Rene",
      "content": "@\"angello2\"#p256752 evo ako nesto masno ne grijesim, isprobao sam programski sve moguce kombinacije (broj nezavisnih varijabli, broj dvojki koji se uzima u obzir za neke dvije znacajke, broj trojki koji se uzima u obzir za neke tri znacajke) i jedina moguca rjesenja da se dobije nesto od ponudenog su 6 nezavisnih gdje uzimamo samo po 1 dvojku i 1 trojku (npr samo linearne) ili 7 nezavisnih s 2 dvojke i 1 trojkom\n\nStvarno ne kuzim",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256772": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Rene\"#p256763 svima je priznat ako sam dobro procitao obavijest. Krivo su rjesenja zadali\n\nJoš da nisam potrošio 30 min na njega al aj 😄",
      "votes": {
        "upvoters": [
          "AK10 (endyyyy)",
          "Gulbash",
          "InCogNiTo124",
          "Jale (čakijale)",
          "Lyras",
          "Rene",
          "[deleted]",
          "angello2",
          "at5611",
          "gladiator",
          "joj (poni)",
          "kix7 (Fish99)",
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256786": {
      "poster": "grga_it_is (it_is_what_it_is)",
      "content": "Znači grupa A, zašto bi bio krivi 3. B)?\n\nI grupa A, zašto nije 15. C) (jer sam doslovno uvrštavao brojke i provjeravao)?",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256789": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"it_is_what_it_is\"#p256786 isto sam dobio 3. B, nije mi jasno...",
      "votes": {
        "upvoters": [
          "grga_it_is (it_is_what_it_is)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256791": {
      "poster": "tomekbeli420",
      "content": "15 također zbunjen kako jbt, uvrstavao brojke lemao\n\nA kod treceg je opceniti linearan model i granicu se moze fino naginjati tako da ispravno klasificira ona 3 preslikana primjera ali da ima razlike u ostalima iz prostora značajki, pa je zato version space veći od 1",
      "votes": {
        "upvoters": [
          "grga_it_is (it_is_what_it_is)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256795": {
      "poster": "grga_it_is (it_is_what_it_is)",
      "content": "@\"tomekbeli420\"#p256791 ali zar ne možeš to reći i za recimo koji su oni označili kao točan. To bi bio odsječak na y osi, pa možeš dobiti koliko hoćeš odsječaka?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256797": {
      "poster": "tomekbeli420",
      "content": "@\"it_is_what_it_is\"#p256795 da ali za bilokoji takav odsjecak klasifikacije svih primjera u prostoru znacajki su iste, to je ona situacija da razliciti parametri daju istu hipotezu, kod opcenitog linearnog modela pak dolazi do razlike",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256799": {
      "poster": "grga_it_is (it_is_what_it_is)",
      "content": "@\"tomekbeli420\"#p256797 \n\nAli ja mogu kontra primjer dati. Recimo ti si rekao da dobijem različite modele ovisno kako je pozicioniran pravac koji ih razdvaja, a recimo kod odsječka ja isto mogu pozicionirati ga kako želim, pa isto tako dobijem koliko hoćeš modela koji savršeno klasificiraju model.\n\nZanči različiti parametri za odječak daju različiti model po toj logici.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256801": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"it_is_what_it_is\"#p256786 jesi gledao apsolutno povećanje il omjer prije-poslije?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256802": {
      "poster": "tomekbeli420",
      "content": "@\"it_is_what_it_is\"#p256799 uzmi u obzir da je prostor primjera parovi cijelih brojeva, ako i dalje si uvjeren okej možeš li dati dvije hipoteze koje nisu iste a dobro klasificiraju ulazne primjete",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "grga_it_is (it_is_what_it_is)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256805": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "Evo ovak sam rješavo 3. u grupi A, ne ispada mi nigdje |Vs| = 1 . Vidi itko grešku?\n\n![](assets/2021-11-23/00012.jpg)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "256806": {
      "poster": "grga_it_is (it_is_what_it_is)",
      "content": "@\"tomekbeli420\"#p256802 JAOO TO NISAM VIDIO 😂😂😂",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "tomekbeli420"
        ]
      }
    },
    "256807": {
      "poster": "tomekbeli420",
      "content": "@\"it_is_what_it_is\"#p256806 eh da, vrag je u detaljima prijatelju moj",
      "votes": {
        "upvoters": [
          "grga_it_is (it_is_what_it_is)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "InCogNiTo124",
          "steker"
        ]
      }
    },
    "257573": {
      "poster": "viliml",
      "content": "@\"Precious Bodily Fluids\"#p256805 Hipoteze se ne broje po parametrima nego po različitim outputima.\n\nZa D, granica zapravo odvaja [imath]x_2[/imath] vrijednosti, i točno je jedna granica koja točno klasificira primjere: ona između 0 i 1.",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "262435": {
      "poster": "Daeyarn",
      "content": "postoji li skripta za neparametarske metode?",
      "votes": {
        "upvoters": [
          "matt (Matt)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "262445": {
      "poster": "steker",
      "content": "@\"Daeyarn\"#p262435 ne, imaju samo natuknice i video",
      "votes": {
        "upvoters": [
          "Daeyarn",
          "matt (Matt)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "262487": {
      "poster": "Daeyarn",
      "content": "@\"steker\"#p262445 hvalaa, e a gdje ima video?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "262492": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Daeyarn\"#p262487 na stranici predmeta je link takelabovog yt kanala, na njemu su sva videopredavanja",
      "votes": {
        "upvoters": [
          "Daeyarn"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "262509": {
      "poster": "Daeyarn",
      "content": "@\"Precious Bodily Fluids\"#p262492 ok, hvala:D",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "264038": {
      "poster": "steker",
      "content": "U ovoj zadaci iz neparametarskih u 3. Zadatku (zadatci s ispita) jel se “lj” uzima kao dva slova ili kao jedno slovo",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "264046": {
      "poster": "angello2",
      "content": "@\"steker\"#p264038 trebalo bi kao jedno",
      "votes": {
        "upvoters": [
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "264099": {
      "poster": "[deleted]",
      "content": "jel uspio netko rijesiti 3. iz zadataka s ispita?",
      "votes": {
        "upvoters": [
          "MsBrightside",
          "rolotex (brr)"
        ],
        "downvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "266193": {
      "poster": "Artemis",
      "content": "Može netko riješiti 5.zadatak (zadatci s ispita) iz V16 Bayesov klasifikator II? Rješavao se na rekapitulaciji, ali ga zapravo nije riješio jer računski dugo traje.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "266272": {
      "poster": "batman3000",
      "content": "@\"Artemis\"#p266193 pogledaj tu https://fer.studosi.net/d/2793-struce-zavrsni-ispit-20202021, imaš dokument https://docs.google.com/document/d/15drigevvwo3wOvZ3uFZgCAO2hgEHdCUa-a1DTMWV7_k/edit#heading=h.me2a23xgw8o koji su prošle godine napravili, tamo je također riješen i ovaj zadatak, možda pomogne",
      "votes": {
        "upvoters": [
          "Artemis"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "266895": {
      "poster": "rolotex (brr)",
      "content": "Jel nekto rj V13 zad za ucenje 1.zad",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "267172": {
      "poster": "viliml",
      "content": "Kako se modelira zavisnost između kontinuirane i kategoričke značajke u Bayesovom klasifikatoru?\n\nNe znam riješiti zadnji zadatak iz DZ 15-16\n\nNaivni model bi trebao imati 14 parametara i polunaivni 18, ali kako dobiti 32 za generalni?\n\nAko ignoriram tu miješanu zavisnost dobijem 29.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "267383": {
      "poster": "maraska",
      "content": "Je li netko rijesio 5. zadatak s ispita za V15?\n\nNikako ne dobivam točan rezultat a ne znam što točno radim krivo.\n\nPrvo sam izračunala zajedničku kovarijacijsku matricu i dobila dijagonalnu matricu s 6, 2 i 4 na dijagonali (to su kvadrirane sigme).\n\nOnda sam isla izračunati h1(x) s uvrštavanjem 1 za mi(1,1), 0 za mi (2,1) i -2 za mi (3,1) te 0,2 za P(y = 1).\n\nJel nešto krivo uvrštavam ili?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "267438": {
      "poster": "viliml",
      "content": "@\"maraska\"#p267383 Nešto si krivo uvrstila. Ne mogu reći što kad nisi rekla koji si rezultat i međurezultate dobila.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "267773": {
      "poster": "matrica",
      "content": "![](assets/2022-01-03/00005.jpg)\n\nJe li mozda netko rjesio ovaj zadatak pod a i b?",
      "votes": {
        "upvoters": [
          "blast"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "268010": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "Koliko su 15. i 16. cjelina povezane cjelinama nakon?",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "268501": {
      "poster": "gad_gadski",
      "content": "![](assets/2022-01-06/00012.png)\n\nPGM2 skripta druga stranica, zna netko kako ovo izračunati?",
      "votes": {
        "upvoters": [
          "Bananaking"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "268867": {
      "poster": "Stoja_9 (Bije_san_u_autobusu)",
      "content": "@\"gad_gadski\"#p268501 Za svaku od vrijednosti c, s i r (svaka je binarna varijabla, ukupno 8 mogucih kombinacija) racunas zajednicku vjerojatnost pomocu izraza `P(c, s, r, w = 1) = P(c)P(s|c)P(r|c)P(w=1|s,r)` pomocu vjerojatnosti iz tablica na slici di je prikazana Bayesova mreza s tim da uvijek vrijedi da je w=1 i dobivene izraze pozbrajas. Znaci za c=0, s=0, r=0 dobijes izraz `P(c=0)P(s=0|c=0)P(r=0|c=0)P(w=1|s=0,r=0) = 0.5*0.5*0.8*0.0` i to zbrojis sa svim ostalim mogucim kobinacijama varijabli c, s i r.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [
          "dodo",
          "neksi (filip)"
        ],
        "tuga": []
      }
    },
    "269000": {
      "poster": "blast",
      "content": "@\"matrica\"#p267773 jel netko uspio",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "269053": {
      "poster": "ana (anci)",
      "content": "@\"blast\"#p269000 rjeseno je na recapu u srijedu negdje oko 1:15 na snimci",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "270928": {
      "poster": "RickyMorty",
      "content": "Zadatak za zadacu V19 s ispita 4. K-medoida PAM algoritam pls objašnjenje ovog drugog dijela algoritma di se odreduju medoidi ovo s predavanja mi fkt nije jasno kaj bi trebao racunat... skroz kontradiktorno je napisano...",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "271214": {
      "poster": "netko_tamo",
      "content": "![](assets/2022-01-16/00028.png)\n\nkojoj formuli odgovara računanje P(x1=Istra | y=1)?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "271308": {
      "poster": "Ducky",
      "content": "Jel zna tko objasnit ovo rješenje iz zadaće?\n\n ![](assets/2022-01-16/00032.png)\n\n![](assets/2022-01-16/00033.png)\n\nLL* je prosječna log-izglednost za taj model na kraju izvodenja EM-algoritma.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "271419": {
      "poster": "gad_gadski",
      "content": "![](assets/2022-01-17/00024.png)\n\nKako oni uspiju dobiti 0.56? Nez sta krivo radim\n\n![](assets/2022-01-17/00025.jpg)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "271422": {
      "poster": "Valentino",
      "content": "@\"gad_gadski\"#p271419 Imaš slični zadatak objašnjen u predavanju na Teamsima od prošlog ponedjeljka, pokaže jednostavno kako se b računa",
      "votes": {
        "upvoters": [
          "Ducky",
          "gad_gadski"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "271744": {
      "poster": "zastozato (studoš)",
      "content": "![](assets/2022-01-18/00012.png)\n\nneko ovaj?",
      "votes": {
        "upvoters": [
          "gad_gadski"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "271964": {
      "poster": "gad_gadski",
      "content": "@\"studoš\"#p271744 mene isto zanima, snajder je na teamsu predavanjima pricao o tim zadatcima, kao da je to najlaksa verzija zadataka s randovim indeksom, al kako je on to jednostavno prebrojavo u svom primjeru na ovom nikako da dobijes dobro rjesenje",
      "votes": {
        "upvoters": [
          "SuperSaiyano"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272065": {
      "poster": "Bananaking",
      "content": "Može netko objasniti proces razmišljanja za ovaj zadatak? Jel se radi samo o napiši formulu za log izglednost i vidi kako utječe kad se smanji N zbog manjeg podskupa i da je varijanca u nazivniku pa kad je nepristrana procjena je veća varijanca pa će log izglednost izgl biti manja? I što mi ovih 0,1 za parametre znači?\n\n![](assets/2022-01-19/00018.png)",
      "votes": {
        "upvoters": [
          "Jaster111",
          "Lyras",
          "Tootha",
          "neksi (filip)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272297": {
      "poster": "dora (AE)",
      "content": "![](assets/2022-01-20/00007.png)\n\nJel bi mogao netko napisat postupak za ovaj zadatak?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272299": {
      "poster": "Bananaking",
      "content": "@\"AE\"#p272297 Ne znam treba li nekako pametnije/kompliciranije ali ja sam tocno rjesenje dobio tako da sam sa stranice 13 skripte procjena parametara II uzeo formulu za MAP procjenitelj, izracunas prvo za 1. korak onda za 1.+2. korak (zbrojis M i n), oduzmes ovaj drugi izracun od prvog i voila. Vjerojatno bi trebao tu formulu izvest kao oni u skripti par koraka ranije ali eto, valjda pomaze",
      "votes": {
        "upvoters": [
          "dora (AE)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272377": {
      "poster": "Daho_Cro",
      "content": "Zna li netko imaju li negdje riješeni zadatci s ispita koji se nalaze u vježbama za pojedinu lekciju?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272390": {
      "poster": "Bananaking",
      "content": "@\"Daho_Cro\"#p272377 Nema, što je šteta jer bi se dalo napraviti od toga kuharicu za ispite i sigurno ima ljudi koji su ih sve riješili",
      "votes": {
        "upvoters": [
          "Daho_Cro",
          "Ducky",
          "Gulbash",
          "JeleeII",
          "[deleted]",
          "angello2",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "cajaznun"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "Daho_Cro",
          "Ducky",
          "Gulbash",
          "cajaznun",
          "soplagaitas (sopla)"
        ]
      }
    },
    "272471": {
      "poster": "Ducky",
      "content": "jel ima ko snimljenja predavanja koja nisu izbrisana na teamsu?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272675": {
      "poster": "Dootz",
      "content": "![](assets/2022-01-22/00001.png)\n\n F1 vrijednosti sam si cak uspio objasniti, ali ove lambda nemogu smisliti. Jel zna netko zasto je C tocno?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272707": {
      "poster": "Rene",
      "content": "Može mi netko objasnit kako su došli do ovog broja parametara (17 odnosno 59)?\n\n![](assets/2022-01-22/00002.png)",
      "votes": {
        "upvoters": [
          "Daho_Cro"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272708": {
      "poster": "JoKing",
      "content": "@\"Ducky\"#p272471 Zar nisu sva predavanja dostupna kada odabereš General -> Files -> Recordings",
      "votes": {
        "upvoters": [
          "Ducky",
          "bodNaUvidima",
          "kix7 (Fish99)",
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272712": {
      "poster": "InCogNiTo124",
      "content": "Tko bumpa dokument SU2020/21 Zadaci iz zadaca i kvizova i zasto tocno?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "angello2",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "272715": {
      "poster": "vidraKida (Ζ ε ύ ς)",
      "content": "@\"AE\"#p272297 ![](assets/2022-01-22/00003.jpg)\n\n Eo lepi",
      "votes": {
        "upvoters": [
          "dora (AE)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272716": {
      "poster": "vidraKida (Ζ ε ύ ς)",
      "content": "@\"Bananaking\"#p272065 dobro bi nam to doslo. Ja sam raspisao formule i uvrstio sve, ali nisam bas nesto previse shvatio. Tako da mislim da mora bit neka glupa caka.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272717": {
      "poster": "vidraKida (Ζ ε ύ ς)",
      "content": "Ima netko postupak za ovaj zadatak? \n\n![](assets/2022-01-22/00004.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272726": {
      "poster": "gladiator",
      "content": "@\"Ζ ε ύ ς\"#p272717 \n\novaj video  https://ferhr.sharepoint.com/sites/SU120212022/Zajednicki%20dokumenti/Forms/AllItems.aspx?id=%2Fsites%2FSU120212022%2FZajednicki%20dokumenti%2FGeneral%2FRecordings%2FGeneral%2D20211220%5F102034%2DMeeting%20Recording%2Emp4&parent=%2Fsites%2FSU120212022%2FZajednicki%20dokumenti%2FGeneral%2FRecordings\n\nna 4:20 ga asistent jako dobro objasni :)\n\nako ne mozes otvorit link onda nadi na teamsu snimku od predavanja 20.12 (ponedjeljak) pa pogledaj",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272763": {
      "poster": "Bananaking",
      "content": "![](assets/2022-01-22/00008.png)\n\nU ovom zadatku kaže nam da je na lijevoj slici izglednost klase, na desnoj aposteriorne vrijednosti\n\nBayesovo pravilo -> P(y|x) = P(x|y) * P(y) (uz izostavljeno P(x) jer je konstanta)\n\nNas zanimaju apriorne vjerojatnosti klasa odnosno P(y). Iz Bayesovog pravila P(y) = P(y|x) / P(x|y) odnosno aposteriorna vjerojatnost kroz izglednost, za sliku to je onda P(y) = desna slika / lijeva.\n\nIz danih varijanci i slike izglednosti zaključim da je plava linija \"srednja\" odnosno odgovara klasi y = 1 jer je za nju varijanca 3, između 5 (široke, crvene) i 2 ( zelene, uske)\n\nDakle idem gledati za plavu liniju desna slika / lijeva. Odaberem recimo x = -10, na lijevoj slici imam otprilike 0.105, na desnoj slici imam otprilike 0.95.  Desna / lijeva je onda 0.95/0.105 = 9.04, a ne 0.1 koliko bi trebalo biti (lijeva / desna daje 0.1). Očito P(y = 1) ne može biti 9.04 ali gdje griješim u logici rješavanja?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272771": {
      "poster": "InCogNiTo124",
      "content": "@\"Bananaking\"#p272763 zasto dijelis desnu sliku kroz lijevu?\n\nUgl desna slika je nastala kao [imath]\\frac{f_i(x)}{f_1(x)+f_2(x)+f_3(x)}[/imath], gdje je i=1 plava klasa, i=2 crvena, a i=3 zelena. funkcije su svaki od grafića. dakle, nazivnik bayesa je suma tih funkcija. Nacrtas si u https://desmos.com/calculator ako zelis vizualizaciju kaj se dogada\n\nEvo primjer: lijeva strana x=15, crveni gaus ima f(x) od 0.03 dok zeleni za isti x ima f(x) oko 0.02. plavi x je dovoljno malen da cu ga smatrat 0\n\nSta to znaci za desnu sliku: za x=15 gledaju se omjeri. Crvena boja je 0.03/(0.02+0.03) sto je oko 0.6, zelena ima 0.02/0.05 sto je oko 0.4, plavo je 0",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272775": {
      "poster": "InCogNiTo124",
      "content": "@\"Bananaking\"#p272763 cekaj ova slika nema smisla. Postoji pogreska u grafovima, nije velika ali je dovoljna da je krivo",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272776": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"InCogNiTo124\"#p272775 di",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272778": {
      "poster": "InCogNiTo124",
      "content": "@\"Precious Bodily Fluids\"#p272776 na x=15 lijevo je zeleno vece, a na x=15 desno je crveno vece i to nije dobro\n\nevo tu sam nacrtao\n\nhttps://www.desmos.com/calculator/cd8ietcxvg",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272781": {
      "poster": "Mikki",
      "content": "@\"InCogNiTo124\"#p272771 Nije li da bi ovo tvoje vrijedilo da je lijevi graf zajednička gustoća p(x,y), a ne p(x|y)?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272782": {
      "poster": "InCogNiTo124",
      "content": "@\"Mikki\"#p272781 vidi graf iznad",
      "votes": {
        "upvoters": [
          "Mikki"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272806": {
      "poster": "Cubi",
      "content": "@\"Bananaking\"#p272763 Mucio sam se s ovim neko vrijeme i mislim da sam skuzio, pa da pokusam objasniti.\n\nZnaci prva stvar, izostavio si p(x) na pocetku, a kasnije si trazio tocnu vrijednost od p(y|x) sto ne ide. P(x) se izostavlja pri maksimizaciji i usporedbi koja je vrijednost veća jer onda ta konstanta nema utjecaja.\n\nE sad kako naci p(y). Kad bi sve vjerojatnosi p(y) bile jednake, onda bi desna slika izgledala drugacije. Plavi i crveni graf bi se sjekli na istom mjestu na lijevom i na desnom grafu. Npr. s lijeve slike se vidi da je vjerojatnost za primjer x=-4 veci za plavi graf. Međutim, da bi crvena klasa bila vjerojatnija tu, to mora znaciti da se plavi primjeri generiraju rijeđe, tj. da je p(y=plava) < p(y=crvena). Slicno i za ostale. \n\nKako naci tocne vrijednosti. Ja sam gledao sjecista na desnom grafu. Vidimo da se plavi i crveni graf sijeku u x=-5, tj. tu su jednako vjerojatni. Na lijevoj slici je vrijednost za x=-5 za plavi oko 0.7, a za crveni 0.1. Da bi bili jednako vjerojatni u x=-5 to znaci da crvena klasa mora biti 7 puta vjerojatnija od plave. I vec tu se otkriva odgovor, jedino moguce je 0.7 za crvenu i 0.1 za plavu. Isto razmisljanje moze se ponoviti za crveni i zeleni graf. Kod x=10 vidimo da je vrijednost za zeleni graf oko 0.17, a za crveni oko 0.5, znaci malo vise od 3 puta manja. Znaci vjerojatnost za crvene primjere mora biti malo vise od 3 puta vjerojatnija od zelenog. 0.7 i 0.2 odgovaraju tome.\n\nMalo je objasnjenje zbrda zdola, al nadam se da ce pomoci. Mogu pokusati objasniti neki detalj ako bude potrebno.",
      "votes": {
        "upvoters": [
          "Jale (čakijale)",
          "Skenk",
          "cile",
          "zastozato (studoš)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272829": {
      "poster": "gad_gadski",
      "content": "@\"studoš\"#p271744 snajder pojasnio zadatak na predavanju u srijedu ovu zadnju, imas na temsu",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272837": {
      "poster": "Heklijo (Geralt of Rivia)",
      "content": "![](assets/2022-01-22/00014.png)\n\nJel se kod odredivanja topoloskog uredaja gleda po razinama? Npr. prvo nodes koji nemaju parent (w,y) pa zatim njihova djeca (z, x) itd...\n\nJel bi onda TU bio : W, Y, X, Z?",
      "votes": {
        "upvoters": [
          "Bananaking"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272839": {
      "poster": "Bananaking",
      "content": "@\"Heklijo\"#p272837 Tako je",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272840": {
      "poster": "Tompa007 (𝐓𝐇𝐄 𝐒𝐄𝐂𝐑𝐄𝐓 - 𝐂𝐋𝐔𝐁)",
      "content": "![](assets/2022-01-22/00015.png)\n\nzasto je tu D tocno ? a ne C?",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "WP_Deva (IdeGas)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272841": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Tompa007\"#p272840 na predavanju je rješen, ugl imaš rubni slučaj kada je N=2, i tada  slijedi mi1 = mi3, dakle ne vrijedi stroga nejednakost",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272842": {
      "poster": "Tompa007 (𝐓𝐇𝐄 𝐒𝐄𝐂𝐑𝐄𝐓 - 𝐂𝐋𝐔𝐁)",
      "content": "@\"Precious Bodily Fluids\"#p272841 ahaaaa, mislis kada je N =1 ? pa onda moze biti da je mi1 = mi2 ili mi1 = mi3 ovisno koja realizacija se dogodila",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "WP_Deva (IdeGas)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272844": {
      "poster": "Bananaking",
      "content": "@\"Tompa007\"#p272840 map procjena je ( Nk + Alfak - 1 ) / (Suma-po-k(Nk + Alfak) - K)\n\nN1 = 0, N2 = 1/2 N, N3 = 1/2 N\n\nAlfa1 = 2, Alfa2 = 2, Alfa3 = 1\n\nKad uvrstiš dobiješ \n\nmi(MAP, 1) = 1 / N + 2\n\nmi(MAP, 2) = 0.5 N + 1 / N + 2\n\nmi(MAP, 3) = 0.5N / N + 2\n\nN je broj primjera, Nk je broj nastupanja k-te vrijednosti. I sad u N staviš najmanji mogući N=1 i dobiješ za 1 -> 1/3, za 2 -> 1/2, za 3 -> 1/6\n\nPod c) kaže da je mi3 uvijek veći od mi1 (što vidiš gore da nije točno)\n\nPod d) kaže da je:\n* mi1 od 0 do 1/3 (za N = 1 je 1/3, za veći N se smanjuje)\n* mi2 između 0.5 i 1 (za N = 1  je 0.5, za veći raste ali ne može biti veći od jedan jer se N u brojniku dijeli sa 2 a u naz ne)\n* da je mi2 uvijek veći od mi3 i da su između 0 i 1 ( mi2 ima taj \"+1\" u brojniku)\n\nVoila.",
      "votes": {
        "upvoters": [
          "Jale (čakijale)",
          "Tompa007 (𝐓𝐇𝐄 𝐒𝐄𝐂𝐑𝐄𝐓 - 𝐂𝐋𝐔𝐁)",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272854": {
      "poster": "Dootz",
      "content": "@\"Bananaking\"#p272844 Nije li najmanji mogući broj N=2, jer kak ćeš podijelit 1 primjer na x2 i x3?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272859": {
      "poster": "Bananaking",
      "content": "Može netko raspisati ovaj, čini se jednostavno ali ne znam kako tretirati ovu prvu i zadnju uvjetnu nezavisnost\n\n![](assets/2022-01-22/00017.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272878": {
      "poster": "tomekbeli420",
      "content": "jel uspio netko? nema šanse da ikako pogodim tih 64\n\n![](https://i.imgur.com/yrXAEzS.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272889": {
      "poster": "sheriffHorsey",
      "content": "@\"tomekbeli420\"#p272878 \n\nznaci za [imath]\\mathcal{H}_0[/imath] imas dio koji se odnosi na numericke znacajke: [imath]\\frac{2 \\cdot 3}{2} + 2\\cdot 2 + 2 - 1 = 8[/imath]\n\nu tom izrazu redom imas parametre za dijeljenu kovarijacijsku matricu (cuvas dijagonalu i jedan trokut), za svaku klasu po jedan vektor sredine [imath]\\mathbf{\\mu}_k[/imath] i jos [imath]K - 1[/imath] parametar za apriorne vjerojatnosti\n\nnakon ovoga ide dio za kategoricke znacajke: [imath] (3\\cdot 2\\cdot 2 - 1) \\cdot 2 = 22[/imath]\n\novdje moras isprobat sve kombinacije vrijednosti svih znacajki osim jedne koju mozes dobit iz uvjeta da se vjerojatnosti zbrajaju u 1 i to jos pomnozit s brojem klasa\n\nsljedeci je model [imath]\\mathcal{H}_1[/imath]:\n\nkod numerickih znacajki se sad mijenja prica jer je sad kovarijacijska matrica dijeljena ali i dijagonalna pa ti vise ne treba trokut, ali ostalo ostaje isto: [imath] 2 + 2 \\cdot 2 + 2 -1 = 7 [/imath]\n\nkod kategorickih znacajki su sad [imath]x_1[/imath] i [imath]x_4[/imath] združene pa se smanjuje broj kombinacija: [imath] [(3\\cdot 2 - 1) + (2-1)]\\cdot 2 = 12[/imath]\n\ni na kraju [imath]\\mathcal{H}_2[/imath]:\n\nopet vrijedi isto za numericke kao i u prethodnom modelu: [imath] 2 + 2 \\cdot 2 + 2 -1 = 7 [/imath]\n\nkod kategorickih se pak jos vise smanjuje broj parametara: [imath] [(3-1) + (2-1) + (2-1) ] \\cdot 2 = 8 [/imath]\n\nNa kraju kad se sve sumira: [imath] (8+22) + (7+12) + (7+8) = 64 [/imath] parametra",
      "votes": {
        "upvoters": [
          "Cubi",
          "Jale (čakijale)",
          "js51856 (6ak)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272894": {
      "poster": "cajaznun",
      "content": "Je li ima netko raspisano rjesenje ovog zadatka (zavrsni prosle godine) sa uzajamnom informacijom? U predavanju asistent samo preleti preko njega,  a u onom doc fileu su napisana gotova rjesenja. Takoder postupak rjesavanja  i brojevi od asistenta sa predavanja se razlikuje od onih iz doc filea za vjerojatnost x2.\n\n ![](assets/2022-01-23/00000.png)",
      "votes": {
        "upvoters": [
          "SuperSaiyano",
          "gad_gadski"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272920": {
      "poster": "Tompa007 (𝐓𝐇𝐄 𝐒𝐄𝐂𝐑𝐄𝐓 - 𝐂𝐋𝐔𝐁)",
      "content": "![](assets/2022-01-23/00005.png)\n\njel bi znao neko ovog objasnit ? nemogu si namapirat ove h1,h2,h3 na one kruznice",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "WP_Deva (IdeGas)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272922": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Tompa007\"#p272920 ima ga na predavanju ako se ne varam.\n\nUkratko, u predavanju postoje formule za izračun parametara za razne varijante bayesovog klasifikatora. U te formule uvrštavaš podatke iz zadataka, pa tako usporediš broj parametara.\n\nŠto se tiće složenosti, može se zaključiti da je model H2 je jednostavniji od H1 jer je on, prema njegovov definiciji, podskup modela H1. O odnosu H1 i H2 naspram H3 ne možemo baš pametno zaključiti jer oni nisu podskup modela H3.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272924": {
      "poster": "Rene",
      "content": "@\"Tompa007\"#p272920 ne znam o kakvim kruznicama govoris, ali izracunas broj parametara za svaku model:\n\nH1: dijagonalna kovarijacijska za svaku klasu\n\nn * K + n*K + K-1 = 109\n\nH2: izotropna ali nije dijeljena \n\nK + n*K + K-1 = 69\n\nH3: dijeljena\n\nn(n+1)/2 + n*K + K-1 = 74\n\nJedini odgovor koji se s ovim poretkom slaze je A",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272925": {
      "poster": "Heklijo (Geralt of Rivia)",
      "content": "![](assets/2022-01-23/00006.png)\n\nŠto predstavlja nazivnik i kako se odredi?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272931": {
      "poster": "tomekbeli420",
      "content": "@\"Heklijo\"#p272925 Dakle kod naivnog Bayesovog klasifikatora vrijedi\n\n[math]P \\left(\\mathbf{x}, y\\right) = P \\left(x_1, x_2, x_3, x_4, y\\right) = P (y) P (x_1 \\mid y) P (x_2 \\mid y) P (x_3 \\mid y) P (x_4 \\mid y)[/math]\n\nE a sad ove vjerojatnosti [imath]P (x_k \\mid y)[/imath] su vjerojatnosti kategoričke (ili Bernoullijeve ako su moguće samo 2 vrijednosti) slučajne varijable [imath]X_k[/imath], koje se procjenjuju Laplaceovim zaglađivanjem (MAP procjena sa onim alfama 2).\n\nU nazivnik ide\n\nukupan broj primjera sa oznakom [imath]y[/imath] (kako nas zanima [imath]y=1[/imath] gledaš ukupan broj primjera koji imaju tu oznaku, dakle ima ih 3)\n\nplus\n\nukupan broj različitih vrijednosti varijable [imath]x_k[/imath]. Ova dva koja si zaokružio su faktori koji predstavljaju [imath]P(x_1 = \\text{\"Istra\"} \\mid y=1)[/imath] i [imath]P(x_2 = \\text{\"ne\"} \\mid y=1)[/imath]\n\nDakle za ovaj prvi faktor [imath]x_1[/imath] ima moguće 3 vrijednosti (Kvarner Dalmacija Istra) zato još stoji +3 a za ovaj drugi faktor [imath]x_2[/imath] ima moguće 2 vrijednosti (da ne).\n\n\n\n\n@\"sheriffHorsey\"#p272889 \n\nUm.... Jasno mi je da je to točno rješenje ali\n\nNeka mi netko objasni kako, na koji način, ovaj komad o [imath]\\mathcal{H}_0[/imath] ne uvodi pretpostavku o uvjetnoj nezavisnosti????????????????????\n\nOvim postupkom koji si ti naveo bi se združena vjerojatnost faktorizirala kao\n\n[math]P \\left(x_1, x_2, x_3, x_4, x_5, y\\right) = P (y) P (x_2 ,x_3 \\mid y) P (x_1, x_4, x_5 \\mid y)[/math]\n\nDakle ove brojke koje si naveo, 1, 8, 22, su upravo brojevi parametara za procjenu ovih faktora\n\nŠto **nije** potpuna faktorizacija bez nekih uvjetnih nezavisnosti. Konkretno ovdje bi onda, čisto gledajući samo ovu faktorizaciju vrijedilo [imath]\\{x_2, x_3\\} \\perp \\{x_1, x_4, x_5\\} \\mid y[/imath]\n\nI kako je sad to točan odgovor ako kaže da [imath]\\mathcal{H}_0[/imath] ne uvodi nikakve pretpostavke o uvjetnoj nezavisnosti???? Ono što sam ja dobio isprve pa sam bio u čudu je broj parametara samo za [imath]\\mathcal{H}_0[/imath] je prelazio sve ove odgovore.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272939": {
      "poster": "Bananaking",
      "content": "Glupo pitanje ali kod algoritma k-sredina, kriterijska funkcija J je L2-norma, ako je centroid (1,2) a primjer x = (1,1), koliko iznosi J za taj primjer?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272940": {
      "poster": "soplagaitas (sopla)",
      "content": "@\"Bananaking\"#p272859 ![](assets/2022-01-23/00007.jpg)\n\nsori ako nije čitljivo",
      "votes": {
        "upvoters": [
          "Bananaking",
          "Sulejman"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272941": {
      "poster": "Bananaking",
      "content": "@\"sopla\"#p272940 Super, hvala, ovo p(z|v,w,x,y) me zanimalo, imamo da je P(z|w,x,y) = P(z|x,y) ali nije mi bilo jasno jel možemo i v izbaciti",
      "votes": {
        "upvoters": [
          "soplagaitas (sopla)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272942": {
      "poster": "tomekbeli420",
      "content": "@\"Bananaking\"#p272939 zbroj kvadriranih euklidskih udaljenosti (euklidska udaljenost = L2 norma vektorske razlike), dakle ne korjenuje se zbroj kvadrata po dimenzijama.\n\n[imath]\\boldsymbol{\\mu} = (1, 2) \\qquad \\mathbf{x} = (1, 1) \\\\\n{\\Vert \\mathbf{x} - \\boldsymbol{\\mu} \\Vert} ^2 = (1-1)^2 + (2-1)^2 = 1[/imath]",
      "votes": {
        "upvoters": [
          "Bananaking"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272944": {
      "poster": "sheriffHorsey",
      "content": "@\"tomekbeli420\"#p272931 Ima smisla ovo sto kazes, ali nije mi uopce jasno kako iskombinirati onda numericke i kategoricke znacajke",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272951": {
      "poster": "Dootz",
      "content": "@\"sheriffHorsey\"#p272944 \n\n@\"tomekbeli420\"#p272931 \n\nImate to rješeno https://docs.google.com/document/d/15drigevvwo3wOvZ3uFZgCAO2hgEHdCUa-a1DTMWV7_k/edit#\n\nŠnajder je i potvrdio da je točno i dobro objašnjeno, makar ja osobno ne razumijem neke dijelove objašnjenja.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272952": {
      "poster": "Bananaking",
      "content": "@\"tomekbeli420\"#p272942 Pokušavam ovaj riješiti, nisam vidio na predavanjima da su riješili baš iteracije k-sredina, ovako sam ga ja shvatio ali očito nisam dobro\n\n![](assets/2022-01-23/00008.png)\n\n![](assets/2022-01-23/00009.jpg)\n\nEDIT: ups, u centriranju za mi 2 je u nazivniku 2 a ne 3 hahah, sad sve valja, nek ostane post na moju sramotu ali možda nekom pomogne",
      "votes": {
        "upvoters": [
          "tomekbeli420"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272960": {
      "poster": "tomekbeli420",
      "content": "@\"sheriffHorsey\"#p272944 e dakle ja sam inicijalno ovako to iskombinirao\n\ndakle hoćemo računati neke uvjetne vjerojatnosti i moramo onda parametre procijeniti\n\ne sad je pitanje kako faktorizaciju napraviti, jer nije moguće kao u onom primjeru u skripti napraviti sve moguće kombinacije vektora [imath]\\mathbf{x} = (x_1, x_2, x_3, x_4, x_5)[/imath] jer takvih ima neprebrojivo mnogo zbog [imath]x_2[/imath] i [imath]x_3[/imath]\n\nE sad sam si razmisljao okej kako napraviti (potpunu) faktorizaciju a da opet mogu prikazati sve to u konačnom broju parametara\n\nPa mislim si okej ne mogu imati situaciju gdje u uvjetnom dijelu imam neku kontinuiranu varijablu, jer onda opet imam beskonačno kombinacija\n\nDakle onda moram imati faktor [imath]P(x_2, x_3 \\mid x_1, x_4, x_5, y)[/imath] i ovo se da u konačno mnogo parametara prikazati. Za svaku kombinaciju ovih uvjetnih varijabli imat ću jedan 2D Gausijan\n\nOnda konačna faktorizacija bi bila\n\n[math]P(x_1, x_2, x_3, x_4, x_5, y) = P(y) P(x_1, x_4, x_5 \\mid y) P(x_2, x_3 \\mid x_1, x_4, x_5, y)[/math]\n\nZa računati prvi faktor treba 1 parametar\n\nZa drugi treba 22, kao što i kod tvog postupka (3\\*2\\*2-1 kombinacija za svaku klasu)\n\nE a za ovaj treći, treba nam dakle 3\\*2\\*2\\*2 = 24 dvodimenzionalnih Gausijana za svaku kombinaciju\n\nDakle 24 dvodimenzionalnih lokacija + jedna dijeljena kovarijacijska matrica od 3 parametra = 24*2+3 = 51 parametar\n\n1+22+51 = 74\n\nI to je samo za [imath]\\mathcal{H}_0[/imath]\n\nEvo ovo je moje mišljenje što bi trebalo biti dobro, ovo njihovo fakat nema smisla...",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272964": {
      "poster": "tomekbeli420",
      "content": "@\"Dootz\"#p272951 di točno, ja ne vidim da uopće se pojavljuje taj zadatak u tom docu\n\nedit: nvm našao sam sad gledam\n\nma kakvi nema šanse da je ovo objašnjenje dobro, čini mi se da je i on uključio uvjetne nezavisnosti, da ima vremena popričao bih s njim, uvjeren sam 100% da je sjebao",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272968": {
      "poster": "Dootz",
      "content": "@\"tomekbeli420\"#p272960 Stranica 16",
      "votes": {
        "upvoters": [
          "tomekbeli420"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272974": {
      "poster": "Cubi",
      "content": "@\"cajaznun\"#p272894 Znaci polunaivan bayesov klasifikator je slican kao naivan Bayesov klasifikator. Znaci rastavlja [imath]P(x_1, x_2, x_3, y) = P(y) P(x_1 \\mid y) P(x_2 \\mid y) P(x_3 \\mid y)[/imath] ako su sve varijable uvjetno nezavisne. Ako nisu, onda te zdruzi, npr recimo da su [imath]x_2[/imath] i [imath]x_3[/imath] uvjetno zavisne onda bi zajednicka vjerojatnost bila: [imath]P(x_1, x_2, x_3, y) = P(y) P(x_1 \\mid y) P(x_2, x_3 \\mid y)[/imath]\n\nProblem koji se dogadja je da je tesko odrediti koje su varijable uvjetno zavisne i onda postoje razliciti postupci objasnjeni u skripti da sad ne ulazim u detalje. U zadatku kaze da su varijable uvjetno zavisne ako je [imath]I(x_i, x_j) > 0.01[/imath] tj. uzajamna informacija veca od 0.01. Sad samo treba izracunati [imath]I[/imath] za svaki par i vidjeti koji su parovi zavisni. Njih cemo zdruziti u faktorizaciji [imath]P(x_1, x_2, x_3, y)[/imath] poslije.\n\n[imath]I(X, Y)[/imath] se računa kao [imath]\\sum{P(X, Y) ln \\frac{P(X, Y)}{P(X)P(Y)}}[/imath] za svaku vrijednost X i Y.\n\nPrimjerice: \n\n[imath] I(x_1, x_2) = P(x_1=0, x_2=0)  ln \\frac{P(x_1=0, x_2=0)}{P(x_1=0)P(x_2=0)}  +P(x_1=0, x_2=1)  ln \\frac{P(x_1=0, x_2=1)}{P(x_1=0)P(x_2=1)} +P(x_1=1, x_2=0)  ln \\frac{P(x_1=1, x_2=0)}{P(x_1=1)P(x_2=0)}+P(x_1=1, x_2=1)  ln \\frac{P(x_1=1, x_2=1)}{P(x_1=1)P(x_2=1)} [/imath]\n\nNe da mi se sad raspisivat dalje detaljno, al iz tablice se mogu odrediti [imath]P(x_1, x_2)[/imath] za svaki par vrijednosti. Npr za (0, 0) samo zbroji kad su [imath]x_1, x_2[/imath] = 0 za svaki [imath]x_3[/imath]. Slicno i za [imath]P(x_1)[/imath], samo se zbroje sve vrijednosti za svaki [imath]x_2[/imath] i [imath]x_3[/imath]\n\nAko je ovo isti zadatak kao u zadacima za vjezbu, dobije se:\n\n[imath] I(x_1, x_2)=0.00513164[/imath]\n\n[imath] I(x_1, x_3)=0.03[/imath]\n\n[imath] I(x_2, x_3)=0.00513[/imath]\n\niz cega slijeda da su varijable [imath]x_1[/imath] i [imath]x_3[/imath] zavisne i faktorizacija je [imath]P(x_1, x_2, x_3, y) = P(y) P(x_2 \\mid y) P(x_1, x_3 \\mid y)[/imath]",
      "votes": {
        "upvoters": [
          "Ducky",
          "cajaznun",
          "plavisnajper",
          "sheriffHorsey"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272986": {
      "poster": "prx_xD",
      "content": "![](assets/2022-01-23/00012.png)\n\n![](assets/2022-01-23/00013.png)\n\nmoze netko objasnit ova 2 zadatka iz zzv-ova Bayesov Klasifikator 2 \n\nimaju rjesenja u [docu](https://docs.google.com/document/d/15drigevvwo3wOvZ3uFZgCAO2hgEHdCUa-a1DTMWV7_k/edit#heading=h.me2a23xgw8o) al mi nisu jasna\n\nsada sam vidio da je @\"Cubi\"#p272974 ovjasnio 5 zadatak al moze netko 4",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273003": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"prx_xD\"#p272986  jos skrolanja 5 sekundi gore i nasao bi 4.  @\"sheriffHorsey\"#p272889",
      "votes": {
        "upvoters": [
          "prx_xD"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273123": {
      "poster": "matrica",
      "content": "[OVDJE](https://drive.google.com/file/d/1WsLuEwz9FNfZaBUoj9qTGJWfTwSGMdJm/view?usp=sharing) mozete pronaci sve zadatke sa ispita za ZI i neke za ucenje.\n\nMozda nekome pomogne pri sutrasnjem ponavljanju ili kasnije... 😅 \n\nako nekome treba dio za MI bacite kom",
      "votes": {
        "upvoters": [
          "2more (Shooshur)",
          "AK10 (endyyyy)",
          "Ardura (Maddy)",
          "Bananaking",
          "BrownPerson (Bambi)",
          "Daho_Cro",
          "Dootz",
          "Ducky",
          "ErnestHemingway (Alfetta)",
          "Fica (Prof)",
          "Han",
          "JBear",
          "Jale (čakijale)",
          "JeleeII",
          "JoKing",
          "Mario1",
          "Me1 (Me)",
          "MsBrightside",
          "Ollie",
          "Ryder (Pepper)",
          "Smolaa",
          "Stark",
          "Sulejman",
          "SuperSaiyano",
          "SuperSjajan3",
          "Uchenikowitz (Učečuču)",
          "Unity (Sgt. Forge)",
          "Upforpslone",
          "YenOfVen",
          "Zuzu (Coffe123)",
          "angello2",
          "blablajar",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "djeno",
          "dora (AE)",
          "eaypeasy (easypeasy)",
          "gladiator",
          "idontwannabemyself",
          "indythedog",
          "jobi (azex)",
          "kix7 (Fish99)",
          "matej1423",
          "matt (Matt)",
          "miss_anthropocene (neunist.iva)",
          "neksi (filip)",
          "plavisnajper",
          "soplagaitas (sopla)",
          "spampers (majmunska boginja)",
          "swish41 (PlavušaSFilozofskog)",
          "tempest"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273125": {
      "poster": "Bananaking",
      "content": "@\"matrica\"#p273123 <3",
      "votes": {
        "upvoters": [
          "matrica"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273165": {
      "poster": "matrica",
      "content": "@\"matrica\"#p273123 \n\nupdate: [evo](https://drive.google.com/file/d/1xX65XuS85S8VhQFQHE61auqYJpLrdwYV/view?usp=sharing) i za MI, nije toliko sredeno u nedostatku vremena, ali nadam se da ce pomoci 😊",
      "votes": {
        "upvoters": [
          "AK10 (endyyyy)",
          "Bananaking",
          "BillIK",
          "Daho_Cro",
          "Ducky",
          "Han",
          "Jale (čakijale)",
          "JoKing",
          "Ollie",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "djeno",
          "eaypeasy (easypeasy)",
          "indythedog",
          "matt (Matt)",
          "plavisnajper"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273167": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "![](assets/2022-01-24/00001.png)\n\nZna netko objasnit ovaj? S prošlogodišnjeg ZI, rješenjima piše da je A točno, ali ako je tema članka već brexit, riječ brexit sama po sebi nebi trebala dodatno utjecati na pojavu riječi UK.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273168": {
      "poster": "matrica",
      "content": "@\"Precious Bodily Fluids\"#p273167 obzirom da ocekujes rijec brexit u clancima u brexitu, bas kao i uk - to bi znacilo da te dvije rijeci nisu uvjetno nezavisne u kateogriji clanaka koji pricaju o brexitu, to sto je tema brexit ne utjece. c i d su cini mi se jasni sami po sebi. b nije jer ne ocekujemo da ce se rijec brexit pojavljivati u clancima cija tema nije brexit, a u takvim clancima bi se recimo mogla pojaviti rijec konzum, tj skup tema koje se ne bave brexitom smo dodatno suzili na skup tema koje u sebi imaju i konzum. dakle ako nista, metodom eliminacije dolazis do istog odg",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273169": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"matrica\"#p273168 \n\n![](assets/2022-01-24/00002.png)\n\n> obzirom da ocekujes rijec brexit u clancima u brexitu, bas kao i uk - to bi znacilo da te dvije rijeci nisu uvjetno nezavisne u kateogriji clanaka koji pricaju o brexitu\n\nsori, nije mi baš jasno i dalje... Ovaj zadatak je riješen na predavanju i ovdje A nije točan odgovor \"jer riječ pandemija u temama o covidu ne pridonosi nikakvu informacijsku dobit\". \n\nKako se ova situacija razlikuje od prethodnog zadatka? Ako je tema o brexitu, pretpostavljam da pojava riječi brexit neće ništa promijeniti u vjerojatnostima?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273171": {
      "poster": "matrica",
      "content": "@\"Precious Bodily Fluids\"#p273169 okej, kuzim, kontradiktorno je samo po sebi. Jedino bih i dalje ostala pri tom odgovoru zbog drugih ponudenih. Ako ovo dode na ZI, lagani prigovor setacu.",
      "votes": {
        "upvoters": [
          "Ducky",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273199": {
      "poster": "-Ivan- (Ivančica)",
      "content": "@\"Precious Bodily Fluids\"#p273167 Taj zadatak je odbačen prošle godine tj. rekli su da nije imao točno rješenje",
      "votes": {
        "upvoters": [
          "Ardura (Maddy)",
          "Ducky",
          "Sulejman",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273217": {
      "poster": "Sulejman",
      "content": "@\"matrica\"#p273168 Zasto je d) jasan po sebi. Da je tema brexita i da se spominje konzum rekao bi da ce se ostatak teksta odnosit na utjecaj brexita na trgovine u hrvatskoj i mozda se uopce ne spomene uk. Sam ja neš krivo skužio ili?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273227": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Sulejman\"#p273217 U zadatku se dosta da filozofirati pa su ga vjerojatno zato maknuli. Pod D) se gleda koja je vjerojatnost da se u clanku koji prica o brexitu spominje rijec \"Ujedinjeno Kravljevstvo\". IMO, kad je vec tema brexit to ce imate toliko veliki utjecaj na pojavu rijeci \"Ujedinjeno Kraljevstvo\" da utjecaj uvjeta \"Konzum\" postaje zanemariv.",
      "votes": {
        "upvoters": [
          "Sulejman"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273232": {
      "poster": "BillIK",
      "content": "![](assets/2022-01-24/00008.png)\n\nMože pomoć s ovim? Ne dobivam ni jedan od ponuđenih rezultata",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273235": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"BillIK\"#p273232 Mislim da dobiš \"točan\" odgovor kada pribrojiš onu konstantu s pi. Iz nekog razloga su je uvrstili.",
      "votes": {
        "upvoters": [
          "BillIK"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273239": {
      "poster": "Sulejman",
      "content": "@\"BillIK\"#p273232 ![](assets/2022-01-24/00009.jpg)",
      "votes": {
        "upvoters": [
          "BillIK"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273250": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Ducky\"#p271308 jesi skužio? Ne mogu shvatit LL2* >= LL3*",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273253": {
      "poster": "Rene",
      "content": "@\"Precious Bodily Fluids\"#p273250 pa oba modela imaju isti broj komponenti i dijeljenu kov. Matricu. Jedino u cemu se razlikuju je to sto H2 koristi k-means za inicijalizirati sredista a H3 to radi nasumicno. Moze se diskutirati da nasumicno mozda bas pogodi savrseno, ali u zadatku pise da ponavljas to 100 puta i uzmes prosjek, pa je za ocekivati da ce H2 biti bolji od H3 tj. LL2*>=LL3*",
      "votes": {
        "upvoters": [
          "Ducky",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273261": {
      "poster": "Tootha",
      "content": "@\"Bananaking\"#p272763 Možeš rješiti i računanjem apriornih vjerojatnosti iz sjecišta desnog grafa.\n\nU x  = -5 vrijedi P(y=1)P(x=-5 | y=1) = P(y=2)P(x=-5 | y = 2). Nazivnik se ovdje može maknuti jer je jednak za oba slučaja.\n\nU x = 10 vrijedi P(y=2)P(x=10 | y=2) = P(y=3)P(x=10 | y=3). Izglednosti u ovim jednadžbama možeš dobiti egzaktno preko formule za Gaussovu distribuciju.  Kada dobiješ te vrijednosti možeš izraziti P(y=1) i P(y=3) preko P(y=2). Iskoristi još P(y=1) + P(y=2) + P(y=3) = 1 da dobiješ P(y=2) i preko toga dobiješ P(y=1) i P(y=3).\n\nNapomena:\n\nU zadatku koriste notaciju N(mi, sigma)] umjesto N(mi, sigma^2).",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273264": {
      "poster": "Me1 (Me)",
      "content": "![](assets/2022-01-24/00014.png)\n\nzasto tu ne dodaju K-1 u broj parametara u svakom retku, a kod pojednostavljenja koja se nalaze odma iznad tablice dodaju? npr. ovaj zadnji redak odgovara 3.pojednostavljanju ali broj parametara nije isti",
      "votes": {
        "upvoters": [
          "Sulejman",
          "Unity (Sgt. Forge)",
          "plavisnajper"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273275": {
      "poster": "Ducky",
      "content": "![](assets/2022-01-24/00017.png)\n\njel kuži tko zašto je mijenja za 12?\n\noke, imam teoriju, al neznam jel točna...",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273282": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Ducky\"#p273275 ima dosta primjera gdje se za bayesovu mrežu prebrojavaju parametri, pogledaj jedan od njih. Treba samo raskinut, odnosno dodat vezu pa usporediti broj parametra s orginalom, trivijalan zadatak\n\nOvdje ima primjer sličnog @\"matrica\"#p273123",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273286": {
      "poster": "Rene",
      "content": "@\"Ducky\"#p273275 to je opet prebrojavanje parametara za bayesov klasifikator, imas slicnih vec rjesenih u temi\n\nMoze se rjesit preko formula za broj parametara, ali meni je lakše ić po mreži:\n\nH1: treba ti 2 parametra za P(y), za svaku od 3 vrijednosti y ti treba po 2 parametra za x1, x2 i x3, dakle ukupno [imath] 2 + 3 \\cdot 2 + 3\\cdot 2 + 3\\cdot2 = 20 [/imath]\n\nH2: treba ti 2 parametra za P(y), za svaku od 3 vrijednosti y ti trebaju 2 parametra za x1 i x2, a za svaku od 9 kombinacija y i x2 ti trebaju 2 parametra za x3: [imath] 2 + 3\\cdot 2 + 3\\cdot 2 + 9\\cdot 2 = 32 [/imath]\n\nH3: treba ti 2 parametra za P(y), za svaku od 3 vrijednosti y ti trebaju 2 parametra za x1, za svaku od 9 kombinacija x1 i y ti treba 2 parametra za x2 i za svaku od 9 kombinacija x2 i y ti treba 2 parametra za x3: [imath] 2 + 3\\cdot2 + 9 \\cdot 2 + 9\\cdot 2 = 44 [/imath].\n\n\"Parametar\" ovdje znači definirati vjerojatnost za pojedinu kombinaciju.",
      "votes": {
        "upvoters": [
          "Ducky",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273293": {
      "poster": "Ducky",
      "content": "@\"Rene\"#p273286 hvala na postupku, ali nije još mi nije jasno teoretski odkud ti parametri dolaze.\n\nRecimo, na primjer, da su klase true (T), false (F) i ne znam (IDK). Zašto mi trebaju 2 parametra za P(y)?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273295": {
      "poster": "sheriffHorsey",
      "content": "@\"Ducky\"#p273293 treci parametar uvijek mozes dobit preko uvjeta [imath] \\sum_{i=1}^{K} P(y = i) = 1 [/imath] pa zapravo trebas zapamtiti samo dva",
      "votes": {
        "upvoters": [
          "Ducky"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273296": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Ducky\"#p273293 ak imas kategoricu varijablu s 3 stanja, možeš ju modelirat s dva parametra, recimo\n\nP(y=T) = 0.2\n\np(y=F) = 0.3\n\np(y=Ne znam) = 1 - P(y=T) - P(y=F)   jer suma mora biti 1\n\nTo bi bilo da varijable nije ovisna ni o kojoj drugoj. A recimo da sad y ovisi o x, koji ima 2 stanja. Pa imaš\n\nP(y=T | x=0) = ...\n\np(y=F | x=0) = ...\n\np(y=Ne znam  | x=0) = ovaj dobivaš od prethodna dva\n\nP(y=T | x=1) = ...\n\np(y=F | x=1) = ...\n\np(y=Ne znam  | x=1) = ovaj dobivaš od prethodna dva\n\ndakle broj parametara koliko bi ti trebalo od y apriorno * broj stanja od x.\n\nNe znam jel ova logika korektna, al uspio sam tako rješiti većinu sličnih zadataka",
      "votes": {
        "upvoters": [
          "Ducky"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273297": {
      "poster": "-Ivan- (Ivančica)",
      "content": "![](assets/2022-01-24/00020.png)\n\nOvo je super ezi zad al opet negdje griješim mada nekužim di.\n\nP(y|x=0, z=1) = P(x=0) * P(y=0) * P(z=1|x=0, y=0) + P(x=0) * P(y=1) * P(z=1|x=0, y=1)\n\nP(...) = 0.8 * 0.7 * 0.9 + 0.8 * 0.3 * 0.8 = 0.696\n\n500 * 0.696 = 348\n\nNije ponuđeno?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273298": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Ivančica\"#p273297 ovo je broj prihvaćenih uzoraka, trebaš to oduzet od 500, jer se traži broj odbačenih",
      "votes": {
        "upvoters": [
          "-Ivan- (Ivančica)",
          "Ducky",
          "MsBrightside"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273300": {
      "poster": "-Ivan- (Ivančica)",
      "content": "@\"Precious Bodily Fluids\"#p273298 \n\neh kad ne čitam podrobno\n\ntenkju",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273301": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "ima netko tumačenje ovoga?\n\n![](assets/2022-01-24/00022.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273304": {
      "poster": "-Ivan- (Ivančica)",
      "content": "@\"Precious Bodily Fluids\"#p273301 \n\nAko se dobro sjećam, Šnajder je rješavao ovaj zadatak i rekao da se ne slaže s rješenjem, točnije s ovim ha2>ha3\n\nKao da to ne možemo zapravo znati je li taj dio vrijedi",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273305": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Ivančica\"#p273304 mene taj dio isto buni, ali bi mi imalo smisla da je obrnuto, onda bi se grupe mogle fino poravnati",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273311": {
      "poster": "sheriffHorsey",
      "content": "![](assets/2022-01-24/00024.png)\n\nsto je s ovom usporedbom modela, ulazi li to u zavrsni i otkud bi to uopce trebalo ucit?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273327": {
      "poster": "Reznox",
      "content": "![](assets/2022-01-24/00030.png)\n\nMoze neko pojasnit?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273333": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Reznox\"#p273327 znaci da ispobavas svaku kombinaciju C i gamma. resetka as in kartezijev produkt",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "273336": {
      "poster": "Rene",
      "content": "@\"Reznox\"#p273327 \n\n10 puta vrtis vanjsku petlju, ali u 9 slucajeva ce svaki primjer biti u train setu a jednom u test setu\n\nSvaki put u vanjskoj petlji prolazis sve hiperparametre dakle 31 (linearna jezgra) + 31×31 (rbf)\n\nU unutarnjoj petlji 4 od 5 puta ce biti u train setu, a jednom u validation\n\nNa kraju vanjske petlje jos istreniras s najboljim parametrima i testiras\n\n[imath] 9 \\cdot ((31 + 31^2) \\cdot 4 + 1) = 35721 [/imath]\n\nAko nisi baš skužio pogledaj pseudokod ove ugniježđene provjere pa si probaj vizualizirat",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "276096": {
      "poster": "branimir1999",
      "content": "Postoji neka preporuka kako uciti za rok? Ishodi ucenja, skripta, stari zadaci i videopredavanja?",
      "votes": {
        "upvoters": [
          "Asdf",
          "Daho_Cro",
          "Gulbash"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "276532": {
      "poster": "Bananaking",
      "content": "Rješavam ZI pa bi stavio rješenja za sve osim gradivo koje nisam prošao (Bayesov klasifikator, ako netko je može staviti svoja rješenja), došao sam do ovog zadatka. Jel bi ga mogao netko raspisati? Ne znam jel dobro raspisujem nezavisnosti odnosno kako se dekomponiraju kad imam skupove npr {v, w}. Ima mi smisla da se to rastavi na dvije, v / y | x i w / y | x ali mi malo mršavo izgleda pool nezavisnosti s kojima bi pojednostavio zajedničku distribuciju.\n\n![](assets/2022-02-02/00025.jpg)",
      "votes": {
        "upvoters": [
          "Daho_Cro",
          "Vuk99"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "276581": {
      "poster": "tomekbeli420",
      "content": "@\"Bananaking\"#p276532 Ja sam to išao rješavati tako da sam direktno pokušao konstruirati Bayesovu mrežu tako da sam obrnutim postupkom primjenjivao uređajno Markovljevo svojstvo (UMS) nad svakom varijablom:\n\n[math]x_k \\perp \\operatorname{pred}(x_k) \\setminus \\operatorname{pa}(x_k) \\mid \\operatorname{pa}(x_k)[/math]\n\nTopološki uređaj je zadan u zadatku, stoga nam je  [imath]\\operatorname{pred}(x_k)[/imath] poznat za sve varijable.\n\nNpr. [imath]\\operatorname{pred}(y) = \\{v, w, x \\}[/imath].\n\nKako?\n\nPa prvo kreneš od Bayesove mreže koja nema nikakvih uvjetnih nezavisnosti, dakle to je usmjereni aciklički graf sa svim mogućim bridovima, pritom imajući na umu topološki uređaj.\n\nI onda uzmeš ove uvjetne nezavisnosti koje imaš i gledaš onu varijablu koja se pojavljuje sama, odnosno ne u parovima varijabli. Npr. uzmeš ovu prvu uvjetnu nezavisnost [imath]\\{v, w\\} \\perp y \\mid x[/imath]\n\nI vidiš da se [imath]y[/imath] pojavljuje sam, i onda što napraviš jest probaš skužiti iz uređajnog Markovljevog svojstva za varijablu [imath]y[/imath] koji su roditelji od [imath]y[/imath] odnosno kakav je [imath]\\operatorname{pa} (y)[/imath]. Pa čini se da je samo [imath]x[/imath], što ima smisla jer [imath]\\operatorname{pred} (y) \\setminus \\operatorname{pa} (y)[/imath] ispadne stvarno [imath]\\{v, w\\}[/imath], što odgovara onda ovoj uvjetnoj nezavisnosti koja je zadana. Dakle onda pobrišeš bridove [imath]vy[/imath] i [imath]wy[/imath].\n\nNa istu foru se za drugu uvjetnu nezavisnost zaključi da je [imath]\\operatorname{pa}(z) = \\{w, y\\}[/imath] pa onda samo te bridove koje vode do [imath]z[/imath] sačuvaš (odnosno pobrišeš bridove [imath]xz[/imath] i [imath]vz[/imath] ).\n\nI onda iz dobivene mreže lako iščitaš faktorizaciju:\n\n[math]p (v, w, x, y, z) = p(v) p(w \\mid v) p(x \\mid v, w) p (y \\mid x) p(z \\mid w, y)[/math]\n\nDistribucija [imath]p(v)[/imath] ima 3-1=2 parametra\n\nDistribucija [imath]p(w \\mid v)[/imath] ima 3\\*(2-1)=3 parametra\n\nDistribucija [imath]p(x \\mid v, w)[/imath] ima 3\\*2\\*(3-1)=12 parametara\n\nDistribucija [imath]p (y \\mid x)[/imath] ima 3\\*(2-1)=3 parametra\n\nDistribucija [imath]p(z \\mid w, y)[/imath] ima 2\\*2\\*(2-1)=4 parametra\n\nSve skupa 24 parametara",
      "votes": {
        "upvoters": [
          "Jale (čakijale)",
          "Stark",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "276937": {
      "poster": "Bananaking",
      "content": "Zašto su B) C) i D) krivi?\n\n![](assets/2022-02-04/00000.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "276938": {
      "poster": "sheriffHorsey",
      "content": "@\"Bananaking\"#p276937 \n\nb) newtonov postupak moze koristit l2 regularizaciju (skripta, log reg 2, str. 5)\n\nc) ovaj odgovor je otrovan jer ti u pitanju kaze \"konkretno kod logisticke regresije\" a pise da gradijentni spust moze zaglaviti u lokalnom optimumu sto nije istina jer je funkcija pogreske za logisticku regresiju konveksna i onda te gradijentni spust mora dovesti do globalnog minimuma uz razumnu stopu ucenja\n\nd) u drugom dijelu odgovora kaze \"kod l2-regularizirane regresije ne konvergira ako primjeri nisu linearno odvojivi\" sto nije istina, sjeti se da logisticka regresije ne konvergira za linearno odvojive primjere ako NE koristis regularizaciju dok u slucaju regularizirane verzije ce povecanje tezina u odredenoj iteraciji povecat vrijednost funkcije pogreske umjesto smanjiti i time doc do konvergencije",
      "votes": {
        "upvoters": [
          "Bananaking"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "276951": {
      "poster": "Heklijo (Geralt of Rivia)",
      "content": "Ima koji dokument sa skupljenim zadacima (i postupak) sa MI i ZI?",
      "votes": {
        "upvoters": [
          "Gulbash"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277048": {
      "poster": "Asdf",
      "content": "![](assets/2022-02-04/00006.png)\n\nmoze netko objasnit postupak rješavanja ovog zadatka?\n\nIzracunam gubitke ali ne dobijem odg pod A",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277051": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"sheriffHorsey\"#p276938 u b nije krivo što newtonov postupak isto može koristiti l2, nego što će on isto divergirati za preveliku stopu učenja",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277074": {
      "poster": "sheriffHorsey",
      "content": "@\"Mićo Simpćo\"#p277051 nisam ni rekao da je krivo sto MOZE koristit l2 regularizaciju jer odgovor kaze \"za razliku od newtonovog postupka, gradijentni spust moze se koristiti za l2 regulariziranu logisticku regresiju\" sto bi znacilo da se newtonov postupak NE MOZE koristiti za l2 regulariziranu logisticku regresiju sto je odmah krivo pa dalje ni ne trebas citat, a ovaj dio s divergiranjem u odgovoru se odnosi na gradijentni spust",
      "votes": {
        "upvoters": [
          "Daho_Cro"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277085": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"sheriffHorsey\"#p277074 Da ali piše ti \"dok Newtonov postupak nema taj problem\", koji se vjv odnosi na divergenciju, pošto je to konkretan problem",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277436": {
      "poster": "Bananaking",
      "content": "Koja je razlika između 3-NN i težinskog k-NN? U zadatku izračunam sličnost između riječi i za 3-NN uzmem 3 najveće sličnosti, pogledam njihove oznake (recimo 1, 1, 0) i zaključim da je oznaka primjera 1. Kako za težinski?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277461": {
      "poster": "sheriffHorsey",
      "content": "@\"Bananaking\"#p277436 \n\n![](assets/2022-02-05/00016.png)\n\ndrzis se ove formule kad racunas tezinski, zapravo to vec i radis kod 3-NN ali su uvijek vrijednosti jezgrene funkcije iste pa ti ne utjece na argmax",
      "votes": {
        "upvoters": [
          "Bananaking"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277492": {
      "poster": "Bananaking",
      "content": "![](assets/2022-02-06/00002.png)\n\nKako se ovdje dobije D) 79? Po meni od 7 značajki jednu odbacujemo (jer je x7 = x5 - x6) pa ih imam 6. Znači 6 nekvadriranih, 6 kvadriranih, 6C2 parova puta 2^2 kombinacije kvadrata, 6C3 trojki puta 2^3 kombinacije kvadrata = 232",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277493": {
      "poster": "Jaster111",
      "content": "@\"Bananaking\"#p277492 \n\nJa sam tu stavio da treba 5 značajki, jer x4 ti ne treba jer je kolinearan sa x3, i još izbacimo bilo šta od x5 ili x6 ili x7. Dakle 5 značajki imamo. Stoga imamo 5 nekvadriranih, 5 kvadriranih, 40 interakcijskih parova (10C2 = 45, ali oduzimamo sve one parove koji su sami sa sobom na kvadrat, dakle x1 i x1^2 npr, pa je takvih 5, dakle imamo 40), i još 30 interakcijskih trojki (svaku od nekvadriranih značajki moramo spojiti sa parom kvadriranih značajki, ali moramo paziti da ne spojimo x1 i x1^2 i x3^2 npr... kvadriranih parova ima 10, ali za x1 moramo oduzeti 4 koja imaju x1 u sebi... to nas dovodi do 6 mogućih parova, dakle 6 parova kvadriranih * 5 nekvadriranih značajki = 30)\n\nTo sve skupa daje 80 pa sam nekako zaključio da je 79 najbliži broj pa je točno lol, ne znam jel to ima logike.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277495": {
      "poster": "sheriffHorsey",
      "content": "@\"Bananaking\"#p277492 ovo mi izgleda kao zadatak s meduispita koji je bio ponisten pa vjerojatno tocan odgovor nije ni bio ponuden",
      "votes": {
        "upvoters": [
          "Jaster111"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277498": {
      "poster": "Bananaking",
      "content": "@\"sheriffHorsey\"#p277495 u obavijestima na ferwebu kažu da je poništen 15. u A grupi a ovo je 6. u A grupi\n\nAko je i ovaj poništen ok ali jel mi dobar način razmišljanja? Primjer u zadacima za vježbu nema kvadrate u interakcijskim",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277500": {
      "poster": "sheriffHorsey",
      "content": "@\"Bananaking\"#p277498 \n\n![](assets/2022-02-06/00003.png)\n\nprvo su 6. ponistili, a 15. naknadno\n\ne jebiga davno je ovo gradivo bilo tak da ti ne mogu rec jel dobro, ali ovo kaj si napisao mi ima smisla i tako bi ga i ja rijesio\n\nmislim da ovo kaj je kolega poslije iskomentirao za x3 i x4 nije dobro jer brijem da ukupna ustedevina nije isto sto i ukupna devizna ustedevina, ali nemam pojma o financijama",
      "votes": {
        "upvoters": [
          "Bananaking"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277562": {
      "poster": "Daeyarn",
      "content": "ima negdje dokument sa zadacima iz prvog ciklusa ko kak ima ovaj doc za drugi ciklus? https://docs.google.com/document/d/15drigevvwo3wOvZ3uFZgCAO2hgEHdCUa-a1DTMWV7_k/edit#heading=h.me2a23xgw8o",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277569": {
      "poster": "Bananaking",
      "content": "@\"Daeyarn\"#p277562 imaš ovdje @\"matrica\"#p273123 @\"matrica\"#p273165  bilježnice sa riješenim svim zadacima s ispita iz vježbi, i prvi i drugi dio gradiva",
      "votes": {
        "upvoters": [
          "Daeyarn"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277576": {
      "poster": "Daeyarn",
      "content": "@\"Bananaking\"#p277569 hvala!!",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277580": {
      "poster": "cajaznun",
      "content": "![](assets/2022-02-06/00018.png)\n\nJe li itko zna postupak rjesavanja ova tri zadatka s MI od ove godine.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277605": {
      "poster": "Bananaking",
      "content": "Kako iz alfa pročitam da su primjeri potporni? U skripti kaže da su svi primjeri za koje je Alfa >= 0 potporni, one za koje je alfa 0 zanemarujem, koja je razlika između Alfa = 1 i 0 < Alfa < 1? Jesu ti između unutar meke margine?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277614": {
      "poster": "Bananaking",
      "content": "@\"cajaznun\"#p277580 17. ne znam, pogledat ću ga još pa ako riješim stavim a ovdje su 18. i 19. Malo neuredno jbg ali mislim da se kuži. Kad znaš ovo što sam ispod napisao onda znaš da su 2, 3 i 4 primjer potporni na margini, ostalo je sve računanje dijagonala i euklidske udaljenosti primjera. U 18. se pojavljuje taj izraz k(x, z) = (x^T Z)^2 i to je u primjeru u skripti izvedeno tako, asistent je kad je rješavao sličan zadatak isto otvorio skriptu i prepisao.\n\n![](assets/2022-02-06/00023.jpg)\n\n\n\n@\"Bananaking\"#p277605 da odgovorim sam sebi, 0 < Alfa <= C su potporni, Alfa < C su na margini, Alfa = C mogu ležati i unutar margine",
      "votes": {
        "upvoters": [
          "cajaznun"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277619": {
      "poster": "Han",
      "content": "Netko postupak za ovaj mozda? Dobim krivo uvijek nezz kaj krivo radim\n\n![](assets/2022-02-06/00026.png)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277622": {
      "poster": "Bananaking",
      "content": "@\"Han\"#p277619 I ja sam uvijek na tome krivo dobio (2.4242), on je što se mene tiče krivo zadan, ako sam u krivu nek me ispravi netko",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277627": {
      "poster": "Bananaking",
      "content": "Evo moja rješenja ovogodišnjeg MI-a, nisu baš svi zadaci rješeni (npr 11.) i malo je neuredno, planirao sam u ponavljanju opet ga riješiti na čistim papirima da bude lijepo za slikanje ali nema se vremena. \n\n![](assets/2022-02-06/00027.jpg)![](assets/2022-02-06/00028.jpg)![](assets/2022-02-06/00029.jpg)![](assets/2022-02-06/00030.jpg)![](assets/2022-02-06/00031.jpg)![](assets/2022-02-06/00032.jpg)![](assets/2022-02-06/00033.jpg)![](assets/2022-02-06/00034.jpg)![](assets/2022-02-06/00035.jpg)![](assets/2022-02-06/00036.jpg)![](assets/2022-02-06/00037.jpg)![](assets/2022-02-06/00038.jpg)![](assets/2022-02-06/00039.jpg)![](assets/2022-02-06/00040.jpg)",
      "votes": {
        "upvoters": [
          "Daeyarn",
          "Daho_Cro",
          "Jale (čakijale)",
          "cajaznun",
          "krokodil",
          "miss_anthropocene (neunist.iva)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277629": {
      "poster": "Jaster111",
      "content": "@\"Han\"#p277619 ![](assets/2022-02-06/00041.jpg)\n\nMeni je ovako ispalo, jedino sta sam dugo vremena gubio da skuzim da w0 se ne regularizira, a ja sam to redovno radio.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277630": {
      "poster": "Jaster111",
      "content": "![](assets/2022-02-06/00042.png)\n\nKoji bi bio postupak rješavanja ovakvog zadatka?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277632": {
      "poster": "miss_anthropocene (neunist.iva)",
      "content": "![](assets/2022-02-06/00043.png)\n\nzas su TP ovdje [math] 3 * {2 \\choose 2} [/math] a ne [math] 4 * {2 \\choose 2} [/math]?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277877": {
      "poster": "Tompa007 (𝐓𝐇𝐄 𝐒𝐄𝐂𝐑𝐄𝐓 - 𝐂𝐋𝐔𝐁)",
      "content": "![](assets/2022-02-07/00027.png)\n\njel neko ovog skuzio ?",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "WP_Deva (IdeGas)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "278193": {
      "poster": "Jaster111",
      "content": "@\"Vamonos\"#p278188 kakva su generalno pitanja i šta se smatra osnovama nekog pitanja? Jesu teorijska čisto ili?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "278252": {
      "poster": "Vamonos (Idemo)",
      "content": "@\"Jaster111\"#p278193 esejska, da ti nešto što ste obrađivali tipa jezgrine funkcije i ti objašnjavaš o čem se radi, napišeš na ploču povezane izraze tako to. Ako nešto bitno izostavis pitati će te podpitanja, ako vidi da baš ne znaš pokušati će te voditi do odgovora. A što je osnovno ti ne znam ovako genericno reci.",
      "votes": {
        "upvoters": [
          "Haki"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "278321": {
      "poster": "Han",
      "content": "@\"Vamonos\"#p278252 Je li medu izvucenim pitanjima moguce izvuc neki racunski zadatak ili su svi esejskog tipa? Takoder prolaizi li se po pismenom dijelu ispita kojeg smo pisali i onda kao sto je bilo krivo sto tocno ili se samo izvlace ta nova pitanja?\n\nMalo sam u strahu pa zato pitam, tenks na info unaprijed.",
      "votes": {
        "upvoters": [
          "Jaster111"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "278366": {
      "poster": "Vamonos (Idemo)",
      "content": "@\"Han\"#p278321 kod mene i ostalih četvero koji su bili u isto vrijeme u prostoriji nije bilo zadatka. Pitanja nemaju veze s ispitommizvlacis na slijepo",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "278387": {
      "poster": "Tompa007 (𝐓𝐇𝐄 𝐒𝐄𝐂𝐑𝐄𝐓 - 𝐂𝐋𝐔𝐁)",
      "content": "@\"Vamonos\"#p278188 Teoretski ako sada imam trojku, jel me moze rusit snajder ili mi moze samo spustit na 2 ? :D",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "WP_Deva (IdeGas)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "278394": {
      "poster": "Vamonos (Idemo)",
      "content": "@\"Tompa007\"#p278387 a ako još uvijek piše da je potreban prag na usmenom uvijek te može srušiti. Dal hoće ne znam",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "278395": {
      "poster": "Tompa007 (𝐓𝐇𝐄 𝐒𝐄𝐂𝐑𝐄𝐓 - 𝐂𝐋𝐔𝐁)",
      "content": "@\"Vamonos\"#p278394 Pise da nema praga, odnosno 70% je ispit 30%labosi, ovo je citat za usmeni s preze \"usmenom ispitu koji sluˇzi kao dodatna provjera i za\n\neventualnu korekciju ocjene u graniˇcnim sluˇcajevima \"",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "WP_Deva (IdeGas)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "278417": {
      "poster": "Vamonos (Idemo)",
      "content": "@\"Tompa007\"#p278395 aha, ja sam bio na roku gjde je zapravo nosio bodove. Iz ovog što piše rekao bih da te neće rušiti ako znaš barem nešto, ali ono to je totalni guessworj",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "306662": {
      "poster": "Vuk99",
      "content": "![](assets/2022-06-29/00010.png)\n\njel bi znao netko ovo rijesit",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "306729": {
      "poster": "Reznox",
      "content": "Postoji li gdje dokument sa risenim zadacima za ispite sa vjezbi (postupci)?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "306877": {
      "poster": "Reznox",
      "content": "![](assets/2022-06-30/00010.png)\n\nZna netko ovaj, ispada mi 2.41",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "306894": {
      "poster": "Daeyarn",
      "content": "@\"Reznox\"#p306877 ![](assets/2022-06-30/00022.png)\n\nedit: tu sam izostavio dvojku, mb\n\n![](assets/2022-06-30/00023.png)",
      "votes": {
        "upvoters": [
          "Reznox"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "306940": {
      "poster": "Reznox",
      "content": "![](assets/2022-06-30/00031.png)\n\nZasto se tu rjesava na ovakav nacin? Malo zbunjujuce",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "306980": {
      "poster": "snowman",
      "content": "@\"Reznox\"#p306940 sta ti tu nije jasno? to je samo x.T * w",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "307017": {
      "poster": "Reznox",
      "content": "@\"snowman\"#p306980 Da al zar nije definicija h = wTx haha",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "307050": {
      "poster": "snowman",
      "content": "@\"Reznox\"#p307017 pa isto ti dode na kraju za ovakve zadatke. ili dobijes 1x3 ili 3x1 matricu.",
      "votes": {
        "upvoters": [
          "Reznox",
          "djeno"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "307056": {
      "poster": "djeno",
      "content": "@\"Reznox\"#p306940 to sam i ja razmisljao -- mozes i po formuli wT x wT je redak matrica od w i X je znacajke po stupcima \n\nnajbitnije ti je da ti pase matricno mnozenje i da ima smisla da sve dimenzije ulaza mnozis sa svakom tezinom a sad oces rjesit ovako ili onako nije bitno",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "307060": {
      "poster": "djeno",
      "content": "![](assets/2022-07-01/00019.png)\n\nOvaj zadatak mi je mal nejasan. Ako dobijem Fi_k gore i popunim ga s nulama u lijevo kako se dobije\n\n(0 0 0 0 1 0) a ne (0 0 0 1 0 0) jer bi valjda x_1^2 trebo bit 1 a ne ovaj drugi koef?\n\nEDIT: skuzio sam treba se poredat kao i funkcija preslikavanja pa zapravo su (0, 0, 0, root(2)x1x2, x1^2, x2^2)",
      "votes": {
        "upvoters": [
          "Daeyarn",
          "Reznox"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "307090": {
      "poster": "Zuzu (Coffe123)",
      "content": "@\"Vuk99\"#p306662 ako otkriješ javi😅",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    }
  }
}