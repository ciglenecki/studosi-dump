{
  "title": "[NEINR] 5. domaća zadaća - 2020/2021",
  "creator": "Hus",
  "slug": "neinr-5-domaca-zadaca-20202021",
  "tags": [
    "FER",
    "Neizrazito, evolucijsko i neuro računarstvo",
    "Zadaće"
  ],
  "posts": {
    "106149": {
      "poster": "Hus",
      "content": "Ljudi moji, ja ću se propucat. No to trenutno nije bitno.\n\nTrenutno je bitan M kao broj reprezentativnih točaka geste. Koji M da uzmem kako bi mogao najbolje naučit neuronsku mrežu?",
      "votes": {
        "upvoters": [
          "[deleted]",
          "in1",
          "lucija (luc)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Cvija",
          "Emma63194",
          "FICHEKK",
          "miki123",
          "shade"
        ],
        "wtf": [],
        "tuga": [
          "carrieb",
          "in1"
        ]
      }
    },
    "106157": {
      "poster": "lkm",
      "content": "Do kad treba rjesiti 5. domacu zadacu?",
      "votes": {
        "upvoters": [
          "addisonRae"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "106165": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "> @Hus#106149 Koji M da uzmem kako bi mogao najbolje naučit neuronsku mrežu?\n\nbeskonačno\n\nza dobar omjer performansi i throughputa, i 30 će biti dovoljno.",
      "votes": {
        "upvoters": [
          "Hus",
          "in1"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "106185": {
      "poster": "Hus",
      "content": "@lkm#106157 \n\nMislim da treba do sljedećeg petka.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "106203": {
      "poster": "lkm",
      "content": "@Hus#106185 11.12 ili 18.12?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "106238": {
      "poster": "Hus",
      "content": "@lkm#106203 \n\n11.12. \n\nNo ponavljam da nisam siguran.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "106823": {
      "poster": "Marenix",
      "content": "@lkm#106203 \"Rok je standardni, do početka idućeg predavanja\" - Čupić, 4.12.2020. xD",
      "votes": {
        "upvoters": [
          "Hus"
        ],
        "downvoters": [
          "luba"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "Hus",
          "Juren",
          "MJ3"
        ]
      }
    },
    "106845": {
      "poster": "Hus",
      "content": "Ovo je moj rezultat za onaj jednostavan primjer iz uputa ( f(x) = x^2 ):\n\nSETUP:\n\nmreža - 1x6x6x1\n\nstopa učenja - 0.5\n\nmetoda - stohastička\n\nbroj iteracija - 800000\n\nraspon početnih težina - [ -1.2, 1.2]\n\nREZULTAT:\n\nx = -1.0 -> f(x) = 0.9399\n\nx = -0.8 -> f(x) = 0.6765\n\nx = -0.6 -> f(x) = 0.3183\n\nx = -0.4 -> f(x) = 0.1410\n\nx = -0.2 -> f(x) = 0.0780\n\nx =  0.0 -> f(x) = 0.0591\n\nx =  0.2 -> f(x) = 0.0672\n\nx =  0.4 -> f(x) = 0.1240\n\nx =  0.6 -> f(x) = 0.3233\n\nx =  0.8 -> f(x) = 0.6953\n\nx =  1.0 -> f(x) = 0.9191\n\nAko može samo netko usporedit sa svojim rezultatima zato što nemam pojma ako je ovo dobro ili loše",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "107135": {
      "poster": "moji_prsti_prsti_klize_po_njoj",
      "content": "@Hus#106845 stavia san sve tvoje postavke samo mi epohe idu do 60 000 jer 800000 bi mi se izvodilo 20 minuta , ovako samo 2 :lol:\n\n (x, y) = (-1, 1) => f(x) = 0.99841342\n\n (x, y) = (-0.8, 0.64) => f(x) = 0.64009418\n\n (x, y) = (-0.6, 0.36) => f(x) = 0.35957731\n\n (x, y) = (-0.4, 0.16) => f(x) = 0.1611458\n\n (x, y) = (-0.2, 0.04) => f(x) = 0.03518987\n\n (x, y) = (0, 0) => f(x) = 0.01350174\n\n (x, y) = (0.2, 0.04) => f(x) = 0.03408994\n\n (x, y) = (0.4, 0.16) => f(x) = 0.16216243\n\n (x, y) = (0.6, 0.36) => f(x) = 0.35835155\n\n (x, y) = (0..8, 0.64) => f(x) = 0.64109768\n\n (x, y) = (1, 1) => f(x) = 0.99098108\n\ntestni primjeri (why not)\n\n(x, y) = (0.7, 0.49) => f(x) = 0.47077921\n\n(x, y) = (0.5, 0.25) => f(x) = 0.26002767\n\n(x, y) = (-0..9, 0.81) => f(x) = 0.91751392\n\n(x, y) = (2, 4) => f(x) = 0.99999829",
      "votes": {
        "upvoters": [
          "Hus",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "107147": {
      "poster": "Hus",
      "content": "@moji_prsti_prsti_klize_po_njoj#107135 \n\nhmm... To je puno bolje nego moje...\n\nZanima me što se kod tebe događa u jednoj epohi kod stohastičkog spusta.\n\nNapraviš li ti propagaciju kroz mrežu za jedan primjer i onda osvježiš sve težine i to je kraj jedne epohe ili \n\nnapraviš propagaciju + osvježavanje za svaki primjer i onda tek završiš jednu epohu?",
      "votes": {
        "upvoters": [
          "moji_prsti_prsti_klize_po_njoj"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "107150": {
      "poster": "moji_prsti_prsti_klize_po_njoj",
      "content": "kod mene je jedna epoha prolazak kroz sve primjere a osvjezavanje tezina je nakon svakog primjera",
      "votes": {
        "upvoters": [
          "Hus"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "107152": {
      "poster": "Hus",
      "content": "@moji_prsti_prsti_klize_po_njoj#107150 \n\nOk, hvala!\n\nMeni je jedna epoha jedan primjer, sad ću to promijenit pa bi mi trebao rezultat biti sličniji tvojem (nadam se...)",
      "votes": {
        "upvoters": [
          "moji_prsti_prsti_klize_po_njoj"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "107164": {
      "poster": "Simpy",
      "content": "U kojem jeziku mislite da je najbolje pisat ovaj labos?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "107181": {
      "poster": "Hus",
      "content": "@Simpy#107164 \n\nPredlažem javu.",
      "votes": {
        "upvoters": [
          "Simpy"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "107183": {
      "poster": "saitama",
      "content": "@Simpy#107164 takoder predlazem Javu jer ako si mislio mozda python tkinter i kivy su odvratni naspram vec odvratnog swing",
      "votes": {
        "upvoters": [
          "Hus",
          "Simpy"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "107212": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "> @saitama#107183 tkinter je odvratan\n\nlol\n\nnapišeš u čemu znaš pisati, džaba ti GUI ak ti implementacija ne radi",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "107258": {
      "poster": "Atem",
      "content": "Radimo li i sa slobodnim težinskim faktorima (w0) i ako radimo, moramo li i njih korigiravati ?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "107261": {
      "poster": "Hus",
      "content": "@Atem#107258 \n\nMislim da ne radimo sa bias težinama (w0)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "107298": {
      "poster": "FICHEKK",
      "content": "Kako pametno inicijalizirati početne težine i biase neuronske mreže? Trenutno generiram random broj u intervalu [0, 1], jel ima nešto što je generalno bolje?",
      "votes": {
        "upvoters": [
          "Cvija"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "107322": {
      "poster": "Joji",
      "content": "@FICHEKK#107298 Najbolje mi je radila [Glorot](https://jamesmccaffrey.wordpress.com/2017/06/21/neural-network-glorot-initialization/) inicijalizacija. Najčešće mi u oko 10 epoha kvadratna pogreška bude ispod 0.03 (mreža je 100x6x5, batch size je 25, stopa učenja 0.5, a u skupu za učenje imam ukupno 125 redaka s po 50 reprezentativnih točaka).",
      "votes": {
        "upvoters": [
          "Daorson",
          "FICHEKK",
          "guzmanov_brk (aybe)",
          "in1",
          "menace_master"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [
          "in1"
        ],
        "tuga": []
      }
    },
    "107331": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@FICHEKK#107298 Ono što želiš da su sve težine blizu nula, ali da se bira po nečemu sličnom normalnoj ili uniformnoj distribuciji\n\nKaiming He je jedna od najčešćih inicijalizacija u praksi ali za ove probleme je inicijalizacija generalno nebitna jer imate koliko toliko lijepe loss hiperplohe. For all you care sempliranje iz [imath]U(-10^{-3}, 10^{-3})[/imath] je dovoljno dobro\n\nMOD EDIT: Ispravio grešku u eksponentu",
      "votes": {
        "upvoters": [
          "FICHEKK",
          "guzmanov_brk (aybe)"
        ],
        "downvoters": [
          "in1"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "107856": {
      "poster": "[deleted]",
      "content": "@moji_prsti_prsti_klize_po_njoj#107135 Jel koristiš bias?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "107872": {
      "poster": "ssbb",
      "content": "jeste implementirali algoritme matricno ili \"neuron po neuron\"? I forward pass i backprop\n\nTakodjer, jel treba implementirati mrezu za samo 1 skriveni sloj ili za proizvoljno skrivenih slojeva (onu generaliziranu formulu)?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "107915": {
      "poster": "ssbb",
      "content": "@Marenix#106823 hmm, meni jos nije otvoren upload na ferku",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "107990": {
      "poster": "[deleted]",
      "content": "Što bi mogao biti povod da mi težine rastu po apsolutnoj vrijednosti sve dok ne dobijem math error u sigmoidi, loss pada tako ne bih rekao da divergira, a i sa vrlo malim stopama učenja počnu rasti",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "107991": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@gentleman#107990 Kriva implementacija (u smislu kad se to dogodi: krivo učiš)\n\nnormalna pojava za sigmoidu, predugo treniraš, ili baci normalizaciju ili clippaj težine.",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "107992": {
      "poster": "[deleted]",
      "content": "@micho#107991 No shit sherlock, možda se netko susreo s nečime što mu je radilo takav problem\n\nEdit: Ok, probat ću s tim",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108018": {
      "poster": "PrisonMike (Števo)",
      "content": "@moji_prsti_prsti_klize_po_njoj#107135 jesu i ostalima ovakvi rezultati za te parametre? \n\nnajbolje što sam nekad uspio dobit za -1 ili 1 je 0.97, al vecinom bude jos manje",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108043": {
      "poster": "Atem",
      "content": "Što koristite za crtanje ako koristite python",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108045": {
      "poster": "Cvija",
      "content": "@Atem#108043 Ja sam koristio pyglet ... Ostalo sa IRG-a haha",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Atem"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "108291": {
      "poster": "Atem",
      "content": "> Algoritam koristi grupno\n\nučenje: težine se ažuriraju isključivo nakon što su viđeni svi uzorci te je na temelju njih\n\nizračunat pravi gradijent.\n\nSmatra li se pod \"svi uzorci\" sve koordinate jedne geste pa onda ažuriramo težine nakon svake geste ili se misli na sve geste(sve koordinate svih gesti) pa ažuriramo težine nakon svih gesti?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108293": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@Atem#108291 svi svi",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108308": {
      "poster": "AromaticConfusion (VrloZbunjen)",
      "content": "Smijemo koristiti np.savez za spremanje dataseta?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108394": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@AromaticConfusion#108308 ak misliš na numpy serijalizaciju, ne gledaju to. Ali kad si već tu možeš ti serijalizirati i cijeli objekt  s pickleom, pa loadaš sve odjednom, nema zajebancije, možda ti overhead bude nešto veći fajl, ali potencijalno i manji, komprimira se to.\n\nMislim ja sam radio neku svoju custom serijalizaciju, bože sačuvaj, ali ispitivanje ti se svelo na \"upalite program\", učitao sam podatke koje nije ni pitao kak učitavam, rekao \"ajde napišite neki znak\", ja napisao, pokazao sam mu za kakve znakove je overfittano, pokazao sam mu što buni mrežu i to je to bilo. To je bilo prošle godine, btw.",
      "votes": {
        "upvoters": [
          "AromaticConfusion (VrloZbunjen)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108466": {
      "poster": "luba",
      "content": "@Marenix#106823 ![](assets/2020-12-10/00012.png)\n\nSmilovao se malo čupić",
      "votes": {
        "upvoters": [
          "Hus",
          "[deleted]",
          "in1",
          "saitama"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Njet",
          "Svenchi",
          "data"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "108530": {
      "poster": "carrieb",
      "content": "u kojoj mjeri moram obracati paznju na proslogodisnju prezentaciju za ovu temu (s obzirom da ovogodisnja nije odabrana i da nisam gledala predavanje)\n\ntj.\n\nkoliko ovo mora biti \"pametno\" u smislu azuriranja tezina (vidim tu neke strategije koje ukljucuju udaljenosti i susjede) - je li dovoljno odraditi samo skolski kak smo radili na umjetnoj\n\ntj. tj. \n\nkako ovo napraviti najbezbolnije",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108611": {
      "poster": "saitama",
      "content": "@gentleman#107992 jesi uspio rijesit jer meni se dogada ista stvar ?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108661": {
      "poster": "PrisonMike (Števo)",
      "content": "@luba#108466 jel zna netko jel ovo zabuna ili je stvarno rok do 23:59?",
      "votes": {
        "upvoters": [
          "Cvija",
          "miki123"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108707": {
      "poster": "Daorson",
      "content": "Jel netko imao problem da mu neuronska mreza nakon dovoljno iteracija za bilo kakav ulazni primjer izbacuje isti izlaz, i taj izlaz je tocno jednak aritmetickoj sredini svih izlaza iz pocetnog izlaznog seta?",
      "votes": {
        "upvoters": [
          "[deleted]",
          "[deleted]",
          "dante_zu"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108708": {
      "poster": "dante_zu",
      "content": "@Daorson#108707 \n\nImam isti problem i neznam ga rjesit...",
      "votes": {
        "upvoters": [
          "Daorson"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108726": {
      "poster": "Luka14 (Kekec)",
      "content": "@Daorson#108707 \n\n@dante_zu#108708 \n\nImam isti problem, jeste li mozda dosli do kakvog zakljucka?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108745": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@Luka14#108726 Zvuči kao da imate loše definiran problem a može biti radi svačega\n\n- loša funkcija gubitka\n- sjeban dataset\n- prevelika regularizacija (ako je koristite)\n- manjak kapaciteta mreže\n  - radi mrtvih neurona (ako se npr. koristi relu)\n  - već na početku premali kapacitet\n  - zeznuta forward propagation implementacija\n  - krivo inicijalizirane težine</LI>\n\nTreba malo unit testirati to",
      "votes": {
        "upvoters": [
          "Daorson"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108775": {
      "poster": "[deleted]",
      "content": "@saitama#108611 Jesam, imao sam pogrešku u back passu, koristio sam krivi index u izračunu pogreške neurona skrienih slojeva, za jedan \"unatrag\"",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108791": {
      "poster": "Daorson",
      "content": "@micho#108745 tezine inicijaliziram random u intervalu [0,1>, jel to moze imati utjecaj?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108795": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@Daorson#108791 kao što je gore napisano, to je loša inicijalizacija, ali ne nešto što bi sigurno uzrokovalo probleme\n\nImate alate kao tensorflow i pytorch pa lijepo usporedite, morali bi vam svi međurezultati za istu arhitekturu, podatke i isto inicijalizirane težine biti skroz jednaki",
      "votes": {
        "upvoters": [
          "Daorson",
          "in1"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108860": {
      "poster": "dirtyharry",
      "content": "Jel ikome to radi kako spada kada treba klasificirati testne utorke, ili na nekom \"validation\" skupu?\n\nMeni se fita bez problema 100% na trainu ali na validationu max 80%, a na nove uzroke je sramotno loš...",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108870": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@dirtyharry#108860 Overfittao si mrežu, možda najbolje da proučiš materijale strojnog učenja po pogledu toga ili tipa [ovo](https://medium.com/predict/what-overfitting-is-and-how-to-fix-it-887da4bf2cba)\n\nPoanta učenja neuronskih mreža je maksimizirati performanse na validacijskom skupu, ne na skupu za učenje, a to će se po klasičnoj teoriji strojnog učenja dogoditi prije nego što dobiješ 100% na skupu za učenje.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108871": {
      "poster": "dirtyharry",
      "content": "Jasno mi je to, znam sve, ali u premalo iteracija mi se overfitta. Nije poanta mog pitanja jeli dobiti 100% na trainu loše, nego radi li to ikome na validationu i testu dobro.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108894": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "> @dirtyharry#108871 u premalo iteracija mi se overfitta\n\nKolika ti je mreža?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108908": {
      "poster": "dirtyharry",
      "content": "2,3 skrivena sloja sa po 200/100/50 neurona, nije ni važno, svakako se to događa",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108911": {
      "poster": "[deleted]",
      "content": "@Atem#108043 prošle godine sam radio taj labos koristeći pygame, dosta sam brzo sve nakuckao",
      "votes": {
        "upvoters": [
          "Atem"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108912": {
      "poster": "[deleted]",
      "content": "@Daorson#108791 za ovaj problem koji ste dobili, to će ti biti u redu. iako, ima boljih načina za inicijalizirati mrežu, a dovoljno su jednostavni za implementaciju. pogledaj si Xavier/Glorot inicijalizaciju",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108916": {
      "poster": "[deleted]",
      "content": "@Daorson#108707 Ovo mi \"smrdi\" na ponašanje slično asocijativnoj memoriji. Zanemari to i probaj debugirrati na jednostavnijim problemima\n\nMožete provjeriti rad svoje mreže na \"dummy\" regresijskim i klasifikacijskim problemima, evo nekih primjera:\n\ni) generiraj [imath]D = \\{x^{(i)}, y^{(i)} \\}_{i=1}^{N}[/imath], gdje je [imath]y = f(x) = \\sum_{ = 0}^{n}w_ix^i + N(0, \\epsilon)[/imath], zatim provesti regresiju nad takvim jednostavnijim problemom; na primjer neka je f(x) = a + bx + cx^2 \n\nii) za klasifikacijske probleme možeš pogledati https://machinelearningmastery.com/generate-test-datasets-python-scikit-learn/\n\nLakše će te debugirati backprop na takvim problemima, nego li na cijelom zadatku.",
      "votes": {
        "upvoters": [
          "Daorson"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108984": {
      "poster": "dante_zu",
      "content": "@Daorson#108707  Ono sto sam otkiro u problemu je sljedece:\n\nU postupku backpropagation-a se tezine mijenjaju tako da se tezina za neurone u izlaznom sloju mijenja puuuno brze nego tezine u sakrivenim slojevima. To uzrokuje da se izlazni sloj prilagodava izlazima sloja prije njega- koji su u njegovom svijetu brojeva prakticki isti(vrlo vrlo slicni). \n\nEfektivno to znaci da se ponisti utjecaj input-a na cijelu mrezu i samo se prilagodi na outpute.\n\nZasto je to tako i koja je konkretna greska u kodu.. neznam",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "108994": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@dirtyharry#108908 Po meni je to dosta preveliki model za dataset, mreža ti lagano naštreba primjere. Ili smanji mrežu (i to značajno, za ovakav problem je sve više od 2 skrivena sloja teški overkill), ili uvedi dodatna ograničenja kao regularizaciju. Jer trenutno učiš 100 slijedova točaka na nekoliko desetaka tisuća parametara, što nikako nema smisla. Ti doslovno možeš sve svoje točke napisati u tu mrežu (tj. njene težine), zato ti se vjerojatno tako brzo overfitta.\n\nSve ovo je uz pretpostavku da imaš ispravnu implementaciju, jel",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    }
  }
}