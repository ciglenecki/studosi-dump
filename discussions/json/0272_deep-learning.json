{
  "title": "Deep learning ",
  "creator": "[deleted]",
  "slug": "deep-learning",
  "tags": [
    "Opušteno"
  ],
  "posts": {
    "9100": {
      "poster": "[deleted]",
      "content": "Mislim da nas ima nekoliko na forumu sto se manje ili vise bavimo dubokim ucenjem i stvarima vezanim uz njega (cv, nlp), pa otvaram ovu temu da mozemo ovdje razmjenjivati iskustva.\n\nTipa ako netko zna za neki zanimljivi paper i sl, da moze ovdje objaviti itd.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "allophone",
          "member",
          "miss_anthropocene (neunist.iva)",
          "rija"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "9104": {
      "poster": "InCogNiTo124",
      "content": "Obligatory https://github.com/deeppomf/DeepCreamPy\n\nA sad za stvarno, zadnji put dok su jos postojala IRL predavanja, prof. Šegvić je spomenuo novo istraživanje o jednom fenomenu koji je primjećen.\n\nNaime, poznato je da je greška u učenju velika kad je model male kompleksnosti i da pada, no nakon jednog trenutka ponovo kreće rasti (zbog overfita).\n\nNo zapažen je fenomen di nakon neke granice greška ponovo počne padati, čak i ispod prethodne najniže razine. Fenomen su nazvali \"double dip\" odnosno \"double descent\". U budućnosti bi trebalo doći još poveznih istraživanja na tu temu.\n\nTakođer su pokazali da double descent nije ovisan samo o kompleksnosti vec i o drugim varijablama. Evo abstract:\n\n> We show that a variety of modern deep learning tasks exhibit a \"double-descent\" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.\n\nhttps://arxiv.org/abs/1912.02292",
      "votes": {
        "upvoters": [
          "Bisenberg (DonaldPump)",
          "Filemon",
          "Jimothy",
          "Lyras",
          "NekocBraca",
          "Piki",
          "Worly",
          "[deleted]",
          "mAcaLukas (mAca Lukas)",
          "member",
          "rija",
          "v-v"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "9121": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#9104 Čini mi se da je jedan razlog za ovo to što loss ne prikazuje vjerno generalizacijsku sposobnost modela, tj. jedan paper koji sam čitao je argumentirao da područje većeg lossa od minimalnog, čak i na validacijskom skupu, može ukazivati na tranzicijski period do boljeg platoa.\n\nNe sjećam se točno koji je to paper bio, ali znam da sam naišao na to kad sam tražio po netu zašto se kod early stoppinga uzima posljednji model, a ne onaj s najmanjim lossem.",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "9124": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "Neke stvari koje su meni bile izrazito interesantne i mislim da su good read za intermediate dubokoučitelje:\n\n\n[Averaging Weights Leads to Wider Optima and Better Generalization](https://arxiv.org/pdf/1803.05407)\n>! Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and ShakeShake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.\n\n[The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/pdf/1803.03635v5)\n>! Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance.\n\nWe find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.\n\nWe present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "NekocBraca",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "9371": {
      "poster": "InCogNiTo124",
      "content": "> @micho#9121 Čini mi se da je jedan razlog za ovo to što loss ne prikazuje vjerno generalizacijsku sposobnost modela, tj. jedan paper koji sam čitao je argumentirao da područje većeg lossa od minimalnog, čak i na validacijskom skupu, može ukazivati na tranzicijski period do boljeg platoa.\n\nMeni se to čini jako logično. Loss je samo mjera koliko model odstupa od train seta (a onda inherentno i svih njegovih biasa)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "9373": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#9371 E vidiš zapravo, s obzirom na to da bi u nekom više dimenzionalnom svijetu ta neka točka težina mreže zapravo radila penjanje po vertikalnom zidu plohe gubitka, tzv. wall climb, onda bi se taj fenomen trebao nazvati:\n\nhttps://i.kym-cdn.com/entries/icons/facebook/000/029/310/cover4.jpg",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "Jimothy",
          "[deleted]",
          "xeqte"
        ],
        "downvoters": [
          "Lyras"
        ]
      },
      "reactions": {
        "haha": [
          "InCogNiTo124",
          "Jimothy",
          "Lyras",
          "Piki",
          "danko (nerim)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "10908": {
      "poster": "tom32",
      "content": "Što mislite o tržištu rada vezano uz ml u hr? 80% IT tržišta čine developeri, ne vidim nešto previše velikih imena u ml-u, naravno cast izuzetcima ali govorim o nekim vecim prilikama.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "10930": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@tom32#10908 Nije nešto razvijeno ali ima firma tu i tamo\n\nSpecifično u firmi u kojoj radimo ja i @Delpins je super za hrvatske pojmove, ali sad jel bi nekom preporučio da proučava ML i DL da bi lako našao posao, pa ne znam, vjerojatno ne bi, potražnje ima zato što je ponuda mala.\n\nAli onda opet ML i DL su samo mali dio svega, otprilike isto kao što je i npr. javascript mali dio ebaya.",
      "votes": {
        "upvoters": [
          "[deleted]",
          "tom32"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "10933": {
      "poster": "InCogNiTo124",
      "content": "Jel ima uopce u Hrvatskoj posla _samo_ sa ML? Za jednu firmu znam da svi koji su bili ML inzenjeri su morali znat backend i nitko nije bio cisti ML. Gideon Brothers isto, oni pak imaju neki robotski OS na koji ide model, s kojim moras znat baratat, dakle opet nije _samo_ ML.",
      "votes": {
        "upvoters": [
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "10935": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#10933 Možda takelab",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "Lyras",
          "[deleted]",
          "boss15",
          "member"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "11093": {
      "poster": "korisnickoime",
      "content": "Što bi preporučili nekom tko bi se htio zaposliti u industriji (u hr). Na koje tehnologije bi se trebao fokusirati, što se traži konkretno.",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "11096": {
      "poster": "tata (NISAM ASISTENT)",
      "content": "Vi koji ste u firmama, s kakvim podacima radite? Koliko se oni razliku od onih datasetova koji su npr. na Kaggleu?Pretpostavljam dosta, tamo mi se čini da je nekako sve pripremljeno za tebe. Jesu li uopće te stvari s Kagglea stvaran prikaz jednog vašeg zadatka u firmi?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "11098": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@tata#11096 O konkretnim podacima se baš ne smije pričati, al to ionako dosta ovisi, nemaju svi podaci isti izvor\n\ngeneralno se stvari testiraju na poznatijim datasetovima da znaš usporediti rezultate s postojećim.\n\nZbog toga što recimo kod nas ne radiš na nečemu iz nule onda imaš pripremljene datasetove na kojima su se prethodno stvari učile, tak da mogu biti sređeni, ali i ne moraju za tvoju mrežu (nemaju sve mreže isti unos). Neki put su stvari toliko loše sređene da ćeš morati raditi preprocesiranje iz nule (osobno sam morao sređivati dataset jer nam baza nije bila u nijednoj normalnoj formi), neki put je plug and play.\n\nAl forma datasetova ti je najmanja briga, po meni je Kaggle dobar u smislu da se riješavaju slični problemi (relevantni problemi), ali Kaggle je možda presintetičan. Ak radiš na Kaggleu nećeš imati nikakvih problema u firmi, ja iskreno ne znam jel itko od nas interna prije zaposlenja koristio Kaggle pa su se svi dobro snašli. Najviše vremena ti treba da se uhodaš u taj način razmišljanja, traženja članaka i arhitektura i optimizacije mreže, a jednom kad to imaš je sve to klasičan codemonkey posao.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]",
          "member",
          "tata (NISAM ASISTENT)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "11099": {
      "poster": "InCogNiTo124",
      "content": "@tata#11096 a ono, puno vise smeca i prljavog nego na kaggle, pogotovo recimo slike\n\n@micho#11098 nas glavni data scientist je vise manje svo znanje naucio preko kagglea, bio je u Warsawi pozvan medu top 100, sad je otiso u noom u NY, kaggle je stvarno gold mine",
      "votes": {
        "upvoters": [
          "[deleted]",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
          "tata (NISAM ASISTENT)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "11100": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#11099 Ma to je, za znanje je dobar, al datasetovi su divlja stvar, obično nisu prilagođeni za specifičan task ili mrežu\n\ntipa skineš Reuters i jbg treba ga obraditi\n\nIli firma ti da dataset na kojem se trenutno trenira mreža, ako su ti drukčiji embeddinzi ili drukčiji feature extraction opet ih treba prilagoditi na razne načine. Još je dobro ako dobiješ izvor, znači bez gubitka informacije. Al recimo ak dobiješ procesuirani bert embedding dataset a ti trebaš word2vec... to je bruh momenat\n\nS tim da zaboravio sam napomenuti jednu stvar, kvaliteta oznaka jako varira. Najveći problem kod datasetova je zapravo njihova (ne)uravnoteženost i (ne)kvaliteta. Gold standard labelirani datasetovi su iznimno rijetki i obično premali za današnje mreže.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "13482": {
      "poster": "mAcaLukas (mAca Lukas)",
      "content": "Jel itko tu koristio Xilinx Vitis za akceleraciju ML-a na njihovim SoC-ovima?",
      "votes": {
        "upvoters": [
          "[deleted]",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "14043": {
      "poster": "InCogNiTo124",
      "content": "danas sam naisao na fora stranicu\n\nhttps://distill.pub/\n\nkoliko sam skuzio uzmu rad i onda animiraju stvarcice da bude jasnije",
      "votes": {
        "upvoters": [
          "Kushim",
          "NekocBraca",
          "[deleted]",
          "rija"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "20145": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "čak i lit zvuči, šteta što je jako primjetan decay kvalitete\n\nhttps://www.youtube.com/watch?v=iJgNpm8cTE8",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "__builtin_popcount (std::popcount)",
          "rija"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "__builtin_popcount (std::popcount)",
          "rija"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "20833": {
      "poster": "InCogNiTo124",
      "content": "![](assets/2020-05-07/00045.jpeg)",
      "votes": {
        "upvoters": [
          "[deleted]",
          "feel_d_boot (iNut)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Mioch",
          "NekocBraca",
          "ViBorg",
          "moose (sugar BoOo!!)"
        ],
        "wtf": [],
        "tuga": [
          "Lyras",
          "feel_d_boot (iNut)"
        ]
      }
    },
    "20835": {
      "poster": "feel_d_boot (iNut)",
      "content": "@InCogNiTo124#20833 \n\n![](assets/2020-05-07/00046.png)\n\nDa se ponovim",
      "votes": {
        "upvoters": [
          "Fran_- (random_trooper)",
          "InCogNiTo124",
          "[deleted]",
          "ink"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Amali (Amajli)",
          "Amon",
          "Filemon",
          "Fran_- (random_trooper)",
          "Goliathus",
          "InCogNiTo124",
          "Lyras",
          "McKovalski",
          "Mioch",
          "Mrcina (Yoso)",
          "NekocBraca",
          "Svarog (Veles)",
          "__builtin_popcount (std::popcount)",
          "moose (sugar BoOo!!)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "20837": {
      "poster": "Gussy (huba buba)",
      "content": "Hey you, you're finally awake. You were trying to open a **_BILO KOJA RANDOM TEMA NA FORUMU_**, right?\n\nWalked right into RZ ambush.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "Svarog (Veles)",
          "[deleted]",
          "[deleted]",
          "[deleted]",
          "bjunolulz",
          "feel_d_boot (iNut)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Amon",
          "Dome",
          "Filemon",
          "InCogNiTo124",
          "Lyras",
          "McKovalski",
          "Mioch",
          "NekocBraca",
          "Svarog (Veles)",
          "bb8 (zaza)",
          "feel_d_boot (iNut)",
          "ink",
          "mAcaLukas (mAca Lukas)",
          "turunturu (Cement)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "20851": {
      "poster": "feel_d_boot (iNut)",
      "content": "@Gussy#20837 Now gib us all your chromosomes",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "21550": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "Danas su mi stigli prvi rezultati kurikularnog učenja\n\n- neaugmentirani podaci - 91.89% prosječna točnost\n- augmentirani podaci (Gamma +-50%, white noise 0.1 stddev) -> 95.32% prosječna točnost\n- kurikularno učena mreža (1, 4, 16 nasumično odabrane labele na slici, ostatak maskiran s crnom bojom), BEZ augmentacija - 95.89% prosječna točnost\n\nPrimjeri dataseta za kurikularno učenje (sve ista slika):\n\n>! ![](assets/2020-05-09/00026.jpeg)\n\n>![](assets/2020-05-09/00027.jpeg)\n\n>![](assets/2020-05-09/00028.jpeg)\n\n>![](assets/2020-05-09/00029.jpeg)\n\n\nOvo je najlošija forma kurikularnog učenja za koju sam mislio da će bit gora od augmentacija. Srednja točnost je zavaravajuća, mogla bi biti veća ali imam 5.2% točnost na implantatima (prije je to bilo 1.8%).",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "21551": {
      "poster": "InCogNiTo124",
      "content": "@micho#21550 koji dataset koji problem wat",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "21557": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#21551 Detekcija i klasifikacija zubi u ortopantomogramima, in house dataset 😅 \n\nNpr (bez augmentacija i NMS nije fine-tunan):\n\n![](assets/2020-05-09/00032.jpeg)\n\nBrojevi pored klasa su sigurnost mreže",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "24361": {
      "poster": "InCogNiTo124",
      "content": "![](assets/2020-05-13/00085.jpeg)",
      "votes": {
        "upvoters": [
          "NekocBraca",
          "Svarog (Veles)",
          "[deleted]",
          "rija"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Filemon",
          "Fran_- (random_trooper)",
          "NekocBraca",
          "Svarog (Veles)",
          "__builtin_popcount (std::popcount)",
          "rija"
        ],
        "wtf": [
          "Hrvoje (Hrvoje45)"
        ],
        "tuga": []
      }
    },
    "24380": {
      "poster": "mAcaLukas (mAca Lukas)",
      "content": "@InCogNiTo124#24361 Yo model so fat, by the time data propagates to the output, a mechanical turk already finished",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Filemon",
          "Fran_- (random_trooper)",
          "InCogNiTo124",
          "Lyras",
          "Mioch",
          "NekocBraca",
          "Svarog (Veles)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "24387": {
      "poster": "mAcaLukas (mAca Lukas)",
      "content": "> @mAcaLukas#13482 Jel itko tu koristio Xilinx Vitis za akceleraciju ML-a na njihovim SoC-ovima?\n\nNitko se nije javio, ali nije da na račun toga mogu odjebat projekt. Uglavnom, danas dobio pločicu, javljat ću svoja zapažanja redovito ovdje",
      "votes": {
        "upvoters": [
          "Filemon",
          "InCogNiTo124",
          "NekocBraca",
          "[deleted]",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "25130": {
      "poster": "nnn (dinoo)",
      "content": "Baš razmišljam, jel bi bilo moguće napisati decompiler za C kod (ili neki drugi jezik) sa ML? Tražio sam po Google-u ali ništa konkretno nisam našao, tbf nisam nikad imao doticaj s ML..",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "25188": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@nnn#25130 Kak misliš decompiler? ASM -> C? Ne treba ti za to duboko/strojno učenje, tj. ne treba ti algoritam sa svojstvom učenja za inverz kad je već sama funkcija egzaktna",
      "votes": {
        "upvoters": [
          "__builtin_popcount (std::popcount)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "25193": {
      "poster": "nnn (dinoo)",
      "content": "@micho#25188 Da, ASM -> C, Kako ne, pa nije egzaktno, kad koristiš ghidru ili neke druge decompilere uvijek imaš problema tu i tamo tipa ne kuži tip podataka i stavi pointer?\n\nKad smo već kod tog ima negdje neki tutorial za ML da se isplati pogledat samo ovako kao intro?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "25196": {
      "poster": "__builtin_popcount (std::popcount)",
      "content": "@micho#25188\n\nS time se slažem, ali možda bi neki ML mogao poslužiti za pogađanje imena varijabli i funkcija (ako u executableu nema debug simbola) ili npr. za odlučivanje između nekoliko opcija za generirani kod ovisno o okolnim instrukcijama, ako te opcije generiraju isti assembly.",
      "votes": {
        "upvoters": [
          "nnn (dinoo)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "25201": {
      "poster": "InCogNiTo124",
      "content": "Kad smo kod ML za kod, meni vec neko vrijeme struji neka ideja o ML operacijskom sustavu, tipa scheduler koji je naucio bolje koje dretve stavljat na procesor ili nesto tog tipa",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "25207": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@nnn#25193 Samo trebaš više grananja. Fora je u ovome: generacija koda ti je takva da će određeni C kompajler uvijek za iste postavke generirati isti kod. I tu stvari staju, jednom kad imaš dostupne sve algoritme koji generiraju takav ASM, moguće ih je jednoznačno reverse engineerati jer su deterministički.\n\nJoš jedna stvar, DL/ML su izrazito nepododni za generaciju egzaktnih stvari. Oni su dobri za generalizaciju, a tvoj problem nije nešto na čemu se generalizira.\n\n@__builtin_popcount#25196 Može poslužiti, nije najbolje rješenje. Dok god imaš determinizam imat ćeš metode sa 100% točnosti izvan DL/ML.",
      "votes": {
        "upvoters": [
          "Kushim",
          "[deleted]",
          "nnn (dinoo)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "25224": {
      "poster": "__builtin_popcount (std::popcount)",
      "content": "> @micho#25207 Može poslužiti, nije najbolje rješenje. Dok god imaš determinizam imat ćeš metode sa 100% točnosti izvan DL/ML.\n\nMislio sam na to da se više C kodova može kompajlati u isti assembly, pa onda da razviješ neki ML koji po okolnom assemblyju bira koji C kod je programer most likely napisao.\n\nNemam iskustva s ML, ali mi se čini da bi to bilo jako teško za napraviti.",
      "votes": {
        "upvoters": [
          "ink"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "25229": {
      "poster": "[deleted]",
      "content": "@__builtin_popcount#25224 teško da ćeš danas dobiti isti assembly sličnih programa. Kopajliranim komercijalnim programima se radi sve i svašta da bi se spriječilo dekompajliranje i reverse engineering. Nasumična špagetizacija, glupavi bufferi i filleri koji ničem ne služe osim da zbune dekompajler, zapisivanje instrukcija na pola riječi itd....",
      "votes": {
        "upvoters": [
          "Filemon",
          "InCogNiTo124",
          "[deleted]",
          "__builtin_popcount (std::popcount)",
          "kix7 (Fish99)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "25231": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@__builtin_popcount#25224 Ako imaš isti input i očekuješ različiti output, I got news for you son...\n\nUgl. to nije nešto što rješavaju algoritmi uopće - ako imaš isti input a očekuješ rszličiti output onda ćeš gubitak informacije generirati izmišljanjem. Trenutno velika većina generacijskih mreža vuče primjerke iz gaussove razdiobe. Ako ti uzorci ne podliježu takvoj razdiobi (a objektivno ne podliježu), stvar neće dobro raditi.\n\nOpet, ako po nečemu ima naznake koji kompajler je korišten, za to će opet biti prikladniji ekspertni sustav, a ne DL/ML.\n\nAko imaš element nasumičnosti kao što je kolega iznad rekao, onda da, trebao bi neki denoiser.",
      "votes": {
        "upvoters": [
          "__builtin_popcount (std::popcount)",
          "nnn (dinoo)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "25242": {
      "poster": "[deleted]",
      "content": "> @micho#25207 jednoznačno reverse engineerati jer su deterministički\n\nJesu deterministicki, ali nisu injektivni. Znaci imas puno inputa koji se salju u isti output.",
      "votes": {
        "upvoters": [
          "Filemon",
          "Svarog (Veles)",
          "[deleted]",
          "__builtin_popcount (std::popcount)",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
          "nnn (dinoo)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "25246": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@Delpins#25242 Okej, ali DL/ML opet ne može ništa oko toga ako je izgubljena informacija",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "25283": {
      "poster": "__builtin_popcount (std::popcount)",
      "content": "@micho#25246\n\nInformacija je izgubljena, ali ovaj moj prijedlog bi služio za nalaženje od svih mogućih inputa neki relativno visokom vjerojatnošću.\n\nNaravno, problem je što inputa koji daju isti output ima beskonačno, ili barem vrlo mnogo, i ne znaš njihove vjerojatnosti, pa bi tome služio ML.",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "25295": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@__builtin_popcount#25283 Da, to ne bi funkcioniralo baš kak si zamišljaš\n\nZnači prva stvar, broj slučajeva koje bi dobio je toliko velik da bi ih bilo gotovo nemoguće razriješiti (jer bi ogroman broj imao iste vjerojatnosti) kad bi ih uspio i izračunati\n\nDruga stvar, poznavati točan C kod nije relevantan zadatak ako možeš dobiti BILO KOJI C kod koji se preslikava u taj ASM. DL/ML nisu algoritmi koji pokušavaju glumiti proroke. Dakle, ako nemaš diferencijabilnu funkciju gubitka, moraš se oslanjati na generativne modele. Generativni modeli trebaju užasno puno podataka - da bi generirao takve podatke trebaš algoritam koji ih generira. Ako imaš generator takvih podataka, onda ti učenje ne treba jer samo trebaš reverse engineerati taj generator. No čak i da ti radi generativni model, ne znači da imaš algoritam prorok - to samo znači da si uz zaključak o razdiobi i postojeće podatke imao dovoljno informacija za rekonstrukciju.\n\nOno što pokušavam reći je da ovaj zadatak nije primjeren za DL/ML. DL/ML se svodi na pronalazak nepoznate funkcije. Ako imaš na raspolaganju sve funkcije kojima dolaziš do ulaza, onda možeš reverse engineersti iste u mjeri u kojoj one to dopuštaju. DL/ML ti sigurno neće omogućiti da više izvućeš jer to nije čarolija. U većini slučajeva kad izvlačimo više nego je dostupno, kad koristimo generativne modele, to treniramo na užasno velikim skupovima podataka i pretpostavljamo da su uzorci iz gaussove (ili bilo koje druge) razdiobe.",
      "votes": {
        "upvoters": [
          "__builtin_popcount (std::popcount)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "25310": {
      "poster": "__builtin_popcount (std::popcount)",
      "content": "> @micho#25295 Znači prva stvar, broj slučajeva koje bi dobio je toliko velik da bi ih bilo gotovo nemoguće razriješiti (jer bi ogroman broj imao iste vjerojatnosti) kad bi ih uspio i izračunati\n\nNaravno, nisam mislio da generiraš sve slučajeve, nego ML nauči iz podataka koji je od njih najvjerojatniji u kontekstu programa na kojima si ga trenirao.\n\n> @micho#25295 U većini slučajeva kad izvlačimo više nego je dostupno, kad koristimo generativne modele, to treniramo na užasno velikim skupovima podataka i pretpostavljamo da su uzorci iz gaussove (ili bilo koje druge) razdiobe.\n\nToga sam svjestan i baš zato nisam previše siguran da je ovo ostvarivo. Upravo iz tih razloga sam na početku napisao:\n\n> @__builtin_popcount#25224 Nemam iskustva s ML, ali mi se čini da bi to bilo jako teško za napraviti.\n\nSamo nisam znao kako se izraziti, i nisam htio odmah napisati nemoguće, pa sam rekao _jako teško_.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "25330": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "> @__builtin_popcount#25310 Naravno, nisam mislio da generiraš sve slučajeve, nego ML nauči iz podataka koji je od njih najvjerojatniji u kontekstu programa na kojima si ga trenirao.\n\nA da nauči kakvi su to najvjerojatniji podaci on ih treba generirati i uspoređivati s rješenjima... Što je zapravo loše, jer metode učenja kod generativnih modela su za jako egzaktne probleme višestruko gore od napada grubom silom.\n\nKako je kôd jako formalna ljudska tvorevina, onda DL/ML metode i nisu toliko efektivne kao sustav u kojem možeš eksplicitno napisati ta formalna ljudska pravila. Jer ono, da mi znamo konkretne algoritma kako npr. detektirati nešto na slici, ili kako prevesti neki tekst ne bi imali potrebu za DL/ML, jel. Zbog toga što mi ne možemo ručno napisati sva ta pravila koristimo DL/ML kako bi ih \"pogodili\". Nemamo baš puno koristi od pogađanja pravila koje smo prethodno napisali u kompajlerima hahah",
      "votes": {
        "upvoters": [
          "__builtin_popcount (std::popcount)",
          "nnn (dinoo)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "25469": {
      "poster": "InCogNiTo124",
      "content": "kod japančića uspješno pokrenut i primjenjen na custom sliku\n\nhttps://i.imgur.com/hkK7dYA.mp4\n\nidući task do jeseni prepisat py2 -> py3\n\nEdit @micho najozbiljnije kak se embedda imgur",
      "votes": {
        "upvoters": [
          "[deleted]",
          "__builtin_popcount (std::popcount)",
          "miss_anthropocene (neunist.iva)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "25473": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#25469 pa embeddo se?\n\nAha, vidim da je gif, izgleda da se ne može jer je njihov embed neka skripta, a u s9e text formatteru nije implementirano to O.o",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [
          "InCogNiTo124"
        ],
        "tuga": []
      }
    },
    "25474": {
      "poster": "InCogNiTo124",
      "content": "@micho#25473 ma popravio sam kasnije sori\n\nA jel moze sluc bit playable unutar posta? A ne ovak da kad stisnes da ode na imgur",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "25475": {
      "poster": "InCogNiTo124",
      "content": "@micho#25473 jel ima planova za implementacijom haha",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "25478": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#25475 To je odvojeni lib, trebalo bi vidjet jel bi se to moglo updateat ili neš, to definitivno nije nešto što želiš sam raditi",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "29166": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "Svakim danom sve je manje vjerojatnije da će RTX3080 imati više od 12 GB VRAM-a, dok se povećava mogućnost da će Big Navi pokušati naguziti nvidiju s 24 GB HBM2...\n\nIma tu netko tko je nedavno pokušavao išta s ROCm na TF/PyTorchu? Čitam na netu da je najveći problem crashanje a onda brzina, ali isto i da je Radeon VII otrpilike brz kao GTX1080 (ista cijena, samo Radeon VII ima 16 GB HBM2...)\n\nRačunam na AMD-ov fine wine, ali nigdje ne mogu naći koliko su *trenutno* dobre implementacije ROCm-a.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "29311": {
      "poster": "InCogNiTo124",
      "content": "Povezano je sa deep learningom, I swear\n\n![](assets/2020-05-24/00005.jpeg)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "Svarog (Veles)",
          "feel_d_boot (iNut)"
        ]
      }
    },
    "29338": {
      "poster": "NekocBraca",
      "content": "https://www.youtube.com/watch?v=fZSFNUT6iY8 \n\nKaj kazete na ovo? \n\nVjerujam a su ovdje odabrali primjer na kojem jako dobro radi i da ne radi opcenito tako dobro, ali razmisljam u smjeru ako se danas postize ovako nesto (makar kao proof of concept), onda za N godina se moze ocekivati nesto sto bi moglo biti upotrebljivo.\n\nKoliko iteracija ce nam trebati da AI iz specifikacije projekta izgenerira cijeli projekt 😅",
      "votes": {
        "upvoters": [
          "[deleted]",
          "ink"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "ink"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "29345": {
      "poster": "[deleted]",
      "content": "@NekocBraca#29338 \n\nMojih ne-RZ-ovskih 50 lipa:\n\nProsječan industrijski projekt ima jako puno domenskih zahtjeva i specifičnosti koje AI programi koji se treniraju strojnim učenjem ne mogu dokučiti. Za neke generičke projekte vjerujem da će uskoro raditi. Ali za takve stvari već imamo frameworke i toolove koji na temelju pitalica izgeneriraju kostur projekta ili čak čitav projekt.\n\nU svakom slučaju fascinantno i super koliko su te stvari dogurale. Vjerojatno ćemo uskoro dobiti neke nove toolove i moćnije frameworke na temelju ovoga koji će programerima olakšavati repetitivne poslove.\n\nI dalje ne mislim da će biti primjenjivo kao generičko rješenje na imalo složenije probleme u praksi. Jer ako za neki problem postoji dovoljan dataset da neuronka generira kompletno rješenje onda znači da rješavaš već previše puta riješen problem i tvoj posao je suvišan. \n\nSingularitet je daleko, daleko. Ako ga ikad i bude :)\n\nP.S. generiranje iz detaljne specifikacije nije ništa novo. Postoje generatori koji izgeneriraju kompletan projekt iz UML 2.0\n\nOvdje je fascinantno generiranje iz dosta šturih i dvosmislenih specifikacija",
      "votes": {
        "upvoters": [
          "Amon",
          "Filemon",
          "InCogNiTo124",
          "NekocBraca",
          "Svarog (Veles)",
          "[deleted]",
          "__builtin_popcount (std::popcount)",
          "mAcaLukas (mAca Lukas)",
          "member"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "29354": {
      "poster": "NekocBraca",
      "content": "@Psychosis#29345 Slazem se prakticki sa svime. Meni se cini da bi prvo koristenje ovako necega moglo biti u obliku nekog IDE/editor plugina koji bi ti predlagao cijele implementacije metoda/razreda. Mislim da bi ovako nesto moglo dosta promijeniti code monkey nacin kodiranja.\n\nAli sam jako oprezan s mislima poput _ma nece AI tako brzo napredovati_ jer vjerujem da su ljudi na pocetku 2000ih godina kad je SVM bio glavni, a konvolucijski modeli se smatrali prekompleksnima i nemogucim za istrenirat govorili da nema sanse da se napravi autonomno vozilo jer kako bi racunalo u real timeu prepoznavalo objekte i sudjelovalo u prometu. A eto danas se autonomna vozila serijski proizvode i koriste svakodnevno u prometu.",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "29359": {
      "poster": "InCogNiTo124",
      "content": "> @NekocBraca#29354 Slazem se prakticki sa svime. Meni se cini da bi prvo koristenje ovako necega moglo biti u obliku nekog IDE/editor plugina koji bi ti predlagao cijele implementacije metoda/razreda. Mislim da bi ovako nesto moglo dosta promijeniti code monkey nacin kodiranja.\n\nPostoji plugin za vim koji ti semnaticki nudi autocompletion, istreniran na milijunima datoteka u najrazlicitijim jezicima\n\nwww.tabnine.com",
      "votes": {
        "upvoters": [
          "NekocBraca",
          "[deleted]",
          "[deleted]",
          "v-v"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [
          "Hrvoje (Hrvoje45)"
        ],
        "tuga": []
      }
    },
    "29368": {
      "poster": "[deleted]",
      "content": "@NekocBraca#29338 \n\ngeneriraj ovo\n\n![](assets/2020-05-24/00010.jpeg)",
      "votes": {
        "upvoters": [
          "HeHe (Direktor života)",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Filemon",
          "Fran_- (random_trooper)",
          "HeHe (Direktor života)",
          "Lyras",
          "McKovalski",
          "NekocBraca",
          "feel_d_boot (iNut)",
          "turunturu (Cement)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "30128": {
      "poster": "InCogNiTo124",
      "content": "Osobno hejtam indijce ali zasad jedini prave deep learning memese\n\n![](assets/2020-05-26/00002.jpeg)",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Dekan",
          "Fran_- (random_trooper)",
          "McKovalski",
          "__builtin_popcount (std::popcount)",
          "ink",
          "member",
          "rija"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "30130": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#30128 Treniranje modela na macu je tebi real?",
      "votes": {
        "upvoters": [
          "Ovo_je_moj_nick"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Fran_- (random_trooper)",
          "netbitan11 (netbitan)",
          "turunturu (Cement)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "30134": {
      "poster": "InCogNiTo124",
      "content": "@micho#30130 ja se nadam da je spojen na colab ili nesto pa da gleda progress",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "30138": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#30134 Pomislio bi da čovjek koji je voljan spizditi 3k$ za macbook si može priuštiti i Titanicu-dvije 😂",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "30510": {
      "poster": "[deleted]",
      "content": "Što mislite o Jürgene Schmidhuberu?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "30512": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@Delpins#30510 Što nije on tvorac LSTMa?",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "30513": {
      "poster": "InCogNiTo124",
      "content": "@Delpins#30510 nisam dovoljno emocionalno investiran u pricu, ali znanost je prepuna znanstvenika koji nikad nisu dobili dovoljno priznanja za ono sto su radili (rosalind franklin, gregor mandel). ne mislim da izmislja, ali ne mislim ni da su ga namjerno pokralil, jbg ne moze se kontrolirati kako se sire informacije i izvuko je deblji kraj",
      "votes": {
        "upvoters": [
          "Filemon",
          "[deleted]",
          "__builtin_popcount (std::popcount)",
          "feel_d_boot (iNut)",
          "member",
          "rija"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "30517": {
      "poster": "[deleted]",
      "content": "@micho#30512 Jest, ali mislim primarno na kontroverzne stvari. Tipa da je njegov paper  ustvari GAN prije GAN-a, i ovakve stvari: http://people.idsia.ch/~juergen/deep-learning-conspiracy.html\n\nMislim čitaj njegov AMA: https://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%C3%BCrgen_schmidhuber_ama/\n\nZa manje više sve što ga pitaju, navede kako je on to prije u nekom otkrio (npr. VAE, GAN, itd.) i primijenio, čak i za DeepMind to govori. Fora je međutim u tome da dosta tih stvari nisu baš nešto jako opskurno, i mnogi ljudi su se toga kasnije sjetili, pogotovo kada je krenula eksplozija deep learninga i onda su ljudi svakakve ideje isprobavali s njim i objavljivali rezultate.\n\nPogledaj za GAN prvi odgovor ovdje: https://stats.stackexchange.com/questions/251460/were-generative-adversarial-networks-introduced-by-j%C3%BCrgen-schmidhuber\n\nTu drugi lik sam opisuje, kako mu je ideja praktički slična GAN-u sinula prije 2010., a nije bio nikakav ML researcher.",
      "votes": {
        "upvoters": [
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
          "rija"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "30519": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "Šta ima neka drama oko njega trenutno, ili što je bilo?\n\nEDIT: \n\n@Delpins#30517\n\nIako mislim da je u pravu da je ideja njegova, kao što je Predrag rekao, ideje su bezvrijedne ako ostanu ideje. Mislim lik je brutalac, otvoriš njegov LSTM rad i doslovno otvori ti se treće oko koliko je dobar. Ali sad, da je on izumitelj GAN-a jer je prvi imao tu ideju - pa i nije. Jer ako ćemo tako, onda je Aristotel izmislio većinu AI-ja, s obzirom na to da je zaslužan za logiku i da je velikim dijelom s tom logikom u principu radio na formalizaciji znanja, a to je očito too far-fetched.\n\nMeđutim, mislim da je donekle pokraden, trebao je dobiti Turingovu nagradu za LSTM odavno, LSTM-ovi su  uz konvolucijske mreže vjerojatno najzastupljenija arhitektura današnjice.",
      "votes": {
        "upvoters": [
          "[deleted]",
          "member",
          "turunturu (Cement)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "30523": {
      "poster": "InCogNiTo124",
      "content": "@Delpins#30517 well jedina stvar koju sam naucio na MUI je da nitko ne moze imat patent na ideju, tak da ovo \"prvi se dosjetio\" nije nesto convincing.\n\nPaperi su druga stvar, on ima i ideju i implementaciju i rezultate, jedino sto nema izgleda je dobar tajming za samopromociju\n\nMsm da pojasnim, imo je otkrica i papere i nije ga bilo briga vec je gazio naprijed, nema tu nis lose, al sad mu je valjda zao dok je stariji sto se nije reklamiro, idk",
      "votes": {
        "upvoters": [
          "[deleted]",
          "__builtin_popcount (std::popcount)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "30524": {
      "poster": "InCogNiTo124",
      "content": "@micho#30519 tldr poceo je imat javne nastupe da se druge ljude kreditira s njegovim izumima i zeli svima skrenit pozornost da je on izmislio pol stvari (po njegovom)",
      "votes": {
        "upvoters": [
          "NekocBraca",
          "[deleted]",
          "__builtin_popcount (std::popcount)",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "30525": {
      "poster": "[deleted]",
      "content": "@micho#30519 Pa bila ja drama, tj. još uvijek i traje ako ćemo to smatrati dramom, vezano za GAN, i njegov paper od prije dosta vremena.\n\nKad je na NISP2016 Goodfellow držao radionicu o GAN, on je u sred nje potegao to pitanje:\n\nhttps://www.youtube.com/watch?v=HGYYEUSm-0Q&t=3789s\n\nI ovdje imaš neki Goodfellowov odgovor na te tvrdnje:\n\nhttps://www.quora.com/Was-J%C3%BCrgen-Schmidhuber-right-when-he-claimed-credit-for-GANs-at-NIPS-2016",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "30529": {
      "poster": "[deleted]",
      "content": "> @InCogNiTo124#30523 [Delpins](https://forum.studosi.net/d/272/63) well jedina stvar koju sam naucio na MUI je da nitko ne moze imat patent na ideju, tak da ovo “prvi se dosjetio” nije nesto convincing.\n\nTo je točno i moj POV. Za LSTM ga sasvim normalno citiraju i pripisuju zasluge jer to i stvarno dolazi od njega.\n\n Za GAN nisu jer nitko osim njega nije ni znao za taj njegov paper od prije 30 godina, i koji se samo na apstraktnoj idejnoj razini može dovesti u vezu s GAN-om. Taj kontekst od prije 30 godina, nema veze sa kontekstom u 2016. kad je Goodfellow objavio rad o GAN-u. Goodfellow je to palo na pamet negdje u raspravi s kolegom, kao i mnogim drugima jer nije bogzna što revolucionarno, razradio je to matematički, implementirao, dobio rezultate i objavio. I to je njegov doprinos, to sve.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "rija"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "30530": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "Nisam bio upućen u sve ovo, očito ne pratim baš tu dramu dovoljno dobro. Po meni je malo scummy sad prozivati ljude. Postoje sudovi za takve stvari, to se ne rješava napadima.\n\nA uostalom, ovo je moje mišljenje - pa ako je nešto tvoje, i ako netko napravi neku inačicu toga; zašto jednostavno ne poboljšaš svoj koncept i pokušaš izmisliti nešto još bolje? Na taj način ćeš dobiti pozornost znanstvene zajednice i poštovanje koje misliš da zaslužuješ.\n\nSad čitam malo to i baš smrdi na šešire od alufolije. A šteta, svidio mi se njegov rad.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "30538": {
      "poster": "InCogNiTo124",
      "content": "> @Delpins#30529 Za GAN nisu jer nitko osim njega nije ni znao za taj njegov paper od prije 30 godina, i koji se samo na apstraktnoj idejnoj razini može dovesti u vezu s GAN-om. Taj kontekst od prije 30 godina, nema veze sa kontekstom u 2016. kad je Goodfellow objavio rad o GAN-u. Goodfellow je to palo na pamet negdje u raspravi s kolegom, kao i mnogim drugima jer nije bogzna što revolucionarno, razradio je to matematički, implementirao, dobio rezultate i objavio. I to je njegov doprinos, to sve.\n\nmeni on nije nista vise od povijesne crtice. Meni (i tebi, i ostalima) je gan poznat _iskljucivo_ od Goodfellowa jer je on samostalno doso do ideje, razradio i razglasio. Msm, i Schmidhuber je isto samostalno dosao i razvio, po tome su ekvivalentni, ali ne mislim ga citirat samo zato jer se dogodio ranije po nekom svjetskom satu, vec je meni taj koncept usao u glavu zbog Goodfellowa. \\*instert einstein everything is relative*\n\nJedino su tu sad finese, sta ak goodfellow nije nezavisno doso do tog, sto ako su neki rezultati lazirani, ovo, ono, in the end sve to nije prebitno, hvala covjeku na zrtvi, bit ce zapisan u fusnoti obvezno i prepoznat cu mu ime kad ga vidim napisano xD",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "31225": {
      "poster": "InCogNiTo124",
      "content": "@micho#31210 kj nije to curricular learning with extra steps? Jer ono prvo ga razbijes iz nasumicne inicijalizacije u nesto sto je bolje za podatke a onda ga nadogradujes",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "31235": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#31225 Ja i jesam radio curriculum learning.\n\nSamo što je problem što postoji puno načina na koji bi se u CV-u mogla inducirati težina. Jedna od metoda je bila da uz određenu vjerojatnost koruptiram učenje. Na kraju se svelo na to da sam uz vjerojatnost od 50% koruptirao oznake. Dakle, u zadnjoj rundi tog krivog učenja 50% oznaka na slici je bilo krivo. I onda jedna runda učenja na dobrim podacima je napucala model dalje nego bilo što drugo lmao\n\nJa i mentor za sad mislimo da sam samo obavio neku napredniju inicijalizaciju takvim učenjem pa da radi toga imam lude performanse (i generalizacijske su dobre, a i specifične, nijedan zub ne pada ispod 91% točnosti), ali treba to više istražiti (na više problema)",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]",
          "feel_d_boot (iNut)",
          "mAcaLukas (mAca Lukas)",
          "rija"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "31441": {
      "poster": "InCogNiTo124",
      "content": "> @micho#31235 Ja i mentor za sad mislimo da sam samo obavio neku napredniju inicijalizaciju takvim učenjem pa da radi toga imam lude performanse (i generalizacijske su dobre, a i specifične, nijedan zub ne pada ispod 91% točnosti), ali treba to više istražiti (na više problema)\n\nTo zvuci kao jeben paper",
      "votes": {
        "upvoters": [
          "NekocBraca",
          "[deleted]",
          "feel_d_boot (iNut)",
          "mAcaLukas (mAca Lukas)",
          "rija"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "33103": {
      "poster": "InCogNiTo124",
      "content": "![](assets/2020-05-31/00011.jpeg)",
      "votes": {
        "upvoters": [
          "[deleted]",
          "__builtin_popcount (std::popcount)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "AVRFreak",
          "Filemon",
          "Fran_- (random_trooper)",
          "HeHe (Direktor života)",
          "Lyras",
          "McKovalski",
          "Mioch",
          "Svarog (Veles)",
          "__builtin_popcount (std::popcount)",
          "feel_d_boot (iNut)",
          "mAcaLukas (mAca Lukas)",
          "rija",
          "spejs"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "33106": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#33103 zvuči kao da je neko čitao Transformer papere ¯\\\\\\_(ツ)_/¯",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "InCogNiTo124"
        ]
      }
    },
    "33112": {
      "poster": "[deleted]",
      "content": "Ima li para u ML-u? Sto od matematike treba znati?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Ovo_je_moj_nick"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "33119": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@moonlight#33112 Da, parcijalne derivacije i [s]redove[/s] optimizacije uglavnom.\n\n(nemam pojma zašto sam napisao redove)",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]",
          "[deleted]",
          "feel_d_boot (iNut)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "33231": {
      "poster": "[deleted]",
      "content": "@moonlight#33112 Ne mogu odgovoriti na pitanje o novcima (dosta ovisi o tome želiš li u akademiju ili u industriju; upravilu vrijedi ova jednakost - industrija=kapitalizam=novac), ali o matematici bi rekao da imam dost dobar osjećaj. Budem pisao ovako iz prespektive studenta koji bi volio ići kasnije na doktorat iz ML-a pa sam i sam istraživao što mi fali, odnosno kako poboljšati znanje matematike. Uglavnom, strojno učenje dijelimo na tri dijela - supervised, unsupervised i reinforcement learning. Upravilu dijele istu matematiku, a ona je - **linearna algebra** (ove stvari koje radite na preddiplomskom su bare minimum, ak to ne znas mozes se fuckat - pun intended). \"Prava\" linearna su projekcije, eigenvalues/eigen decomp., SVD, QR decomp., LU(P) decomp. . Da bi dobio osjećaj pogledaj si ovaj [playlist](https://www.youtube.com/playlist?list=PLUl4u3cNGP61iQEFiWLE21EJCxwmWvvek). Drugi dio matematike koji ti treba je **matematičk analiza** - drugim riječima, FERovski trivijalizirano - dervacije, parcijalne derivacije ... općenito funkcije viševarijabli i njihova svojstva. I zadnja velika cjelina je **teorija vjerojatnosti**.\n\nNaravno, sada svako to podpodručje traži dodatna znanja. Na primjer, unsupervised i reinforcement learning traže dobro poznavanje teorije vjerojatnosti i stohastičkih procesa.\n\nTo bi bilo više-manje to. Ako si još na preddiplomskom i volio bi se spremit za Strojno učenje na diplomskom (ako ćeš ga upisat), preporučam ti ovu knjigu za preko ljeta - https://mml-book.github.io/ . Čudo od knjige.\n\nIsto tako, stvari koje se koriste u ML-u - prvenstveno optimizacija (konveksna i nekonveksna) - baziraju se na gore navedenim područjima. Tako da ta područja jednostavno čine stvarno fundamentalna znanja za ML i DL.\n\nIsto tako, nemoj si mislit da trebaš znat teoreme i svaku formalizaciju svakog pojma. Kada ulaziš u neko područje matematike bitno ti je znat interpretaciju, (vizualnu) intuiciju. Pogledaj si od 3blue1brown playliste o\n\nEssence of Calculus i Essence of Linear Algebra da dobiješ dojam što to znači imati stvarno razumijevanje koncepata.\n\nUz gore navedenu knjigu preporučio bi ti i http://vmls-book.stanford.edu/ .\n\nP.S. Bilo da ćeš ići u industriju ili akademiju osnovno znanje matematike koja stoji iza ML/DL-a je bitno. Time si osiguravaš da slijepo ne slijediš bezbroj tutorijala koji su dostupni online od ekipe koja je prepisala od drugih taj tutorial. I da znaš kada treba što primjeniti. Strojno učenje nije palo iz vedra neba, nego je posljedica i primjena matematike. Minimalni cilj je da ne završiš kao ova dva lika - https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A i https://www.youtube.com/user/sentdex\n\nŽivio.",
      "votes": {
        "upvoters": [
          "Dekan",
          "Filemon",
          "Hrvoje (Hrvoje45)",
          "InCogNiTo124",
          "Jakic007",
          "McKovalski",
          "NekocBraca",
          "Vasili (Fasili)",
          "WhiteMamba",
          "[deleted]",
          "[deleted]",
          "hermanzdosilovic",
          "in1",
          "indythedog",
          "member",
          "tata (NISAM ASISTENT)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "33345": {
      "poster": "[deleted]",
      "content": "@tolecnal#33231 \n\nPod \"novcima\" sam vise mislio na to koliko ce trazen biti ML u buducnosti. Ne bih htio uloziti vrijeme u proucavanje i studiranje toga da na kraju ne bih mogao naci kvalitetan posao. Vec je netko napisao da u HR i ne postoji neka ponuda poslova sto se tice ML/DL-a. Trenutno ML zvuči kao \"The next big thing\" sto ce sigurno privuci mnoge ljude da se bave tim područjem što bi moglo povećati konkurenciju na tržištu rada. \n\nStvar je da sam FER upisao jer nisam znao sto drugo, nije me interesiralo nikoje podrucje računarstva ali sad slušam kolegij UI i čini mi se zanimljivo područje (ofc znam da je taj kolegij samo minimalan uvod u područje) samo ne znam jesam li preveliki brainlet za to s obzirom na to da je potrebno znanje iz dosta područja matematike koji se tiču ML/DL-a. U svakom slučaju hvala na odgovoru i linkanim knjigama.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "33357": {
      "poster": "tata (NISAM ASISTENT)",
      "content": "@tolecnal#33231 \n\nZnam da je uz Siraja bilo dosta kontroverzi, ali u čemu je stvar sa sentdexom? Po intenretu ga svi hvale i njegove videe preporučuju za ulazak u svijet MLa...",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "33374": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@moonlight#33345 UI nema veze s modernim ML-om. Najažurnija stvar u UI je reinforcement learning, koji se nedavno počeo pojavljivati samo zato što je SGD sranje. Al onda kreneš čitati papere i gledaš te losseve koje jako teško razumiješ i skužiš da nije wurf. Ove ostale stvari: pretrage stanja, logičko zaključivanje i učenje agenata - u tom uglavnom nema toliko para, ako i nađeš posao.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "33512": {
      "poster": "[deleted]",
      "content": "@moonlight#33345 \"sad slušam kolegij UI i čini mi se zanimljivo područje (ofc znam da je taj kolegij samo minimalan uvod u područje)\" - Da. Dobro si primjetio da je UI \"minimalan uvod u područje\". I to je i cilj kolegija (i više manje svakog kolegija na preddiplomskom). Što je dobro, jer onda vidiš što se sve radilo i što se sve radi u U, a onda kada dođeš na diplomski, ako te zanima ML/DL upišeš Strojno, Duboko i Linearnu i na konju si.\n\n\"ne znam jesam li preveliki brainlet za to s obzirom na to da je potrebno znanje iz dosta područja matematike koji se tiču ML/DL-a\" - Nije to ništa neshvatljivo. Nekima treba dulje, nekima treba kraće. Nije bitno. Ako imaš dobre materijale (što ih i ima), a i budući da si već došao do kraja preddiplomskog onda možeš razumijeti sigurno.\n\nEvo ti još nekoliko zanimljivih review članaka da vidiš što je to točno ML/DL:\n1. https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf - ovo je sada već \"stariji\" članak ali je dobar.\n2. https://arxiv.org/pdf/1708.05866.pdf - ovo je za (deep) RL review članak.\n\nSada općenito ML, odnosno \"shallow\" learning nemam neki review članak da ti budem iskren. Ali možda bi ti bio zanmljiv sljedeći članak (jako dobro napisan) koji govori o tome da nije sve tako bajno u MLu (akademska zajednica prvenstveno) kako se čini - https://arxiv.org/pdf/1807.03341.pdf - možda je tebi trenutno malo off-topic, ali svejedno zanimljiv članak.\n\nP.S. Za ML bi trebao znati i malo statistike. Općenito je statistika (barem ja tako vjerujem) dobar uvod u ML. Nekako kada imaš tu progresiju od teorije vjerojatnosti pa statistika i tek onda ML , poslože ti se stvari u glavi.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]",
          "in1",
          "indythedog",
          "member"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "33556": {
      "poster": "[deleted]",
      "content": "Sada malo dulji rant (p.s. molim te nemoj ovo čitati kao da vičem ili da sam nabrijan, nego čitaj schillano, jer sam ja schillan kada pišem ovo, samo želim dobro argumentirati svoj stav oko sentdexa pa je zato dulji odgovor :) ):\n\n@tata#33357 Ja ti to gledam ovako: svaka knjiga/blog/youtube kanal treba imati svoju populaciju, odnosno publiku. Ta publika, naravno kao i svaka grupa, sastoji se od pojedinaca koji dijele zajednička obilježja te time tvore jednu grupu. Sudeći po raznolikosti playlista kod Sentdexa (Neural Networks from Scratch in Python, Quantum Computer Programming, Reinforcement Learning, Machine Learning with Python, Learn Python Programming, Django Web development, Go Lang Programming Basics, Python Programming for Finance, ...) njegova publika su ljudi koji bi htjeli općenito ući u programiranje, odnosno u područja koja traže znanja programiranja. Nekako mi se čini da on prodaje priču kako je programiranje lagano, ne trebaju ti nikakva znanja, ... . No taj čovjek nikada nije rekao da je kvalitetno programiranje jako teško. Ja ti smatram da ako vidim nekakav ružan i nekoherentan kod, onda misli autora tog koda nisu bile posložene. Nekako kao kada gledaš i čitaš tuđi kod jednostavno vidiš stanje uma samog autora. Ja imam ružne kodove, naravno, gdje se vidi da sam samo htio nešto napisat (npr. kada radim labose), ali isto tako imam predivne stvari, gdje se vidi da sam si uzeo vremena za razmislit i posložit misli u glavi.\n\nSada ću ići rastavljati ovu njegovu zadnju playlistu \"Neural Networks from Scratch\". U uvodnom videu kaže \"...the only expectation I have from the viewers is that you understand programming and OOP ...\" . Dakle ovo je apsolutno nerealno. Uzmi si sada razliku između toga, i gore navedene knjige \"Mathematics for Machine Learning\". Gore u knjizi se obrađuje cca. 250 stranice matematike (koju bi već čovjek trebao znat), dakle taj prvi dio služi više-manje za ponavljanje matematike koju bi netko trebao znati da razumije osnovne metode strojnog učenja.\n\nOnda kaže \"... Python is very simple ...\". Pa čovječe kao da me netko lupio čekičem u glavu. Onda u jednom trenutku kaže da će prvo sve kodirati u sirovom pythonu (dakle neće koristiti numpy), i onda da će sve to prebaciti polako u NumPy. Čemu to? Zašto? Isto tako, lik nema pojma što je numpy xd. Dakle nije znao u jednoj rečenici reć da se radi o libraryu koji služi za efikasno baratanje s N-dimenzionalnim tenzorima (što je odmah prva stvar koja piše na naslovnoj strnaici numpya) koji su u širokoj primjeni u STEM zajendinic. Inače, tko želi znati više od Sentdexa o numpyu  neka prouči ovo - https://www.labri.fr/perso/nrougier/from-python-to-numpy/ :) .\n\nZa kraj, njegov playlist o RL-u. Čovjek krene od Q-learninga, a Sutton i Barto u knjizi imaju prvo 100 stranica gradiva i tek onda dolaze do Q-learning. No ne, sentdex može bolje. Sendtex može napraviti RL tutorial na temu DRL-a bez ikakov predznanja. Užas.\n\nUglavnom, da zaključim. Smatram da ako netko želi predavati neku temu da mora imati duboko i intinmno poznavanje teme da može u jednoj rečenici reći suštinu. Postoji razlog zašto su Šnajderova predavanja iz Strojnog kvaliteta. On jednostavno nema dovoljno znanja iz bilo kojeg od područja koje predaje da bi mogao raditi videe na tu temu. Problem je u tome što kada pišeš tutorial, a ti si učio iz tutoriala, onda to ne završi dobro.\n\nNe da mi se više. Živjeli.\n\nIskreno, imam i ja tu fallacies u svojoj logici u tome da baš i ne kužim ekipu koja bi htjela naučiti programirati, ali traže \"Learn programmig in 5 minutes\", ... odnosno traže uvijek shortcutove.",
      "votes": {
        "upvoters": [
          "Bog (Charlie Brown)",
          "Filemon",
          "InCogNiTo124",
          "MEasy",
          "Mariosss77 (Trevor)",
          "feel_d_boot (iNut)",
          "in1",
          "indythedog",
          "member",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
          "tata (NISAM ASISTENT)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "33566": {
      "poster": "[deleted]",
      "content": "@micho#33374 Ovo nije baš točno što si napisao.\n\n\"reinforcement learning, koji se nedavno počeo pojavljivati samo zato što je SGD sranje. \" RL, odnosno DRL koji je nedavno postao u akademskoj zajednici vrlo popularan, nije bukno zato što je \"SGD sranje\". To je apsolutno krivo. SGD i RL su dvije različite stvari. RL, odnoso DRL, je postigao uspjeh jer je 2015. , kada je izašao paper u Nature-u (Human level control through deep reinforcement learning),  ekipa je pokazala da se mogu naučiti kompleksna ponašanja samo iz slika. \n\nJednostavno postavke problema su bile drugačije te su činile problem težim pa se trebalo koristiti DRL za rješavanje tih Atari igara. No to nema veze s time da je SGD loš algoritam.\n\nIsto tako, oni su koristili derivat SGD-a za treniranje (RMSProp) svoje convolucijske mreže. Možda si mislio na dobru stvari, ali si se krivo izrazio.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "Ovo_je_moj_nick",
          "[deleted]",
          "rija"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "33633": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@tolecnal#33566 SGD je najveći razlog zašto stagniramo u dubokom učenju. Jako je restriktivan, baš cuckovska metoda, i za prave stvari je nestabilan.\n\nKao primjer pogledaj npr. DNC. Ne postoji implementacija koja se da trenirati SGDom koliko je nestabilna, a paper nije pisao neki debil nego Alex Graves. Al ljudi su se snašli i rekli da dobro funkcionira s RL-om.\n\nČinjenica je da radi SGDa moramo raditi razne stvari - dobra inicijalizacija, paziti na diferencijabilnost, podrezivati gradijente ak vole eksplodirati, izmišljati nelinearne funkcije, izmišljati schedulere hiperparametara, podešavati odnos gubitaka kod ansambla funkcija gubitaka itd. SGD je jako ograničavajuć i primitivan. On ima nekih lijepih svojstava, ali rekao bih da najveći problem je što ovisimo o njemu i sličnim metodama jer baš i nemam izbora.\n\nZnaš koja je sad procedura za dobiti najjaču mrežu? Moraš\n\n- napraviti brutalno oversizeanu mrežu transformera/konvolucija bez maxpoola\n- sve inicijalizirati kaimingom i xavierom\n- koristiti batchnorm rano u mreži, layernorm kasnije\n- clippati gradijente\n- raditi cyclic cosine annealing with decay scheduling\n- raditi curriculum learning\n- imati ogromni augmentirani dataset\n- prve epohe koristiti Adam, onda se prebaciti na SGD s momentom\n- raditi warmup prije epoha\n- ili imati savršeno uravnotežene losseve ili imati hiperparametre za skaliranje\n- trenirati tu supermrežu prvo na brutalno teškom zadatku, a tek nakon pretraininga finetunati i distilirati mrežu\n- kvantizirati mrežu na FP16/INT8\n- moguće još neke stvari koje sam zaboravio\n\novo je sve zato da popravimo nedostatke, ne zato jer to ima biološki smisla ili jer direktno pomaže učenju. Doslovno svaki potez ovdje je kompenzacija za to što je naše znanje i njegova primjena u području big hot garbage.\n\nNaravno, da ne bi bilo da samo serem, razlog nije samo zato što ništa ne znamo nego jer koristimo preprimitivne metode. Ispada da je učenje nečega jako komplicirana stvar i da puno toga ne vidimo. Bez obzira na to SGD je jako arhaična i primitivna metoda, i onog trenutka kad se napravi novi način treniranja, a polako postoji sve veća i veća potreba za time, mislim da ćemo ga napustiti. Ne zauvijek, mislim, i dan danas se za neke stvari koriste genetski algoritmi - evo, i sam sam imao priliku u završnom radu koristiti genetski algoritam da bih optimizirao veličine anchora za CNN, npr., ali SGD se sigurno neće koristiti kao glavni algoritam.",
      "votes": {
        "upvoters": [
          "ink"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "33652": {
      "poster": "[deleted]",
      "content": "@micho#33633 Da, sve stoji, ali bi se iz tvojeg gornjeg (prijašnjeg) komentara na RL i \"SGD je sranje\" dalo zaključiti da se (D)RL pojavio samo zato što je \"SGD sranje\", no to nije točno. Tvoj komentar je bio \"reinforcement learning, koji se nedavno počeo pojavljivati samo zato što je SGD sranje.\" - nije točno, apsolutno krivo.\n\n(D)RL rješava problem optimizacije dugoročnog ponašanja agenta u stohastičkom svijetu. SGD nema veze s time.\n\nDa, SGD i njegove varijante se koriste prilikom treniranja agenata koji koriste diferencijabline aproksimatore kao procjene za V(s) i Q(s, a), ali nije se (D)RL pojavio jer je SGD zakazao. Krivo si to povezao.",
      "votes": {
        "upvoters": [
          "[deleted]",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "33655": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@tolecnal#33652 Da, nisam mislio da su ljudi izmislili DRL jer je SGD sranje, tj. da su bili primorani. Mislio sam da je dovoljno sranje da DRL daje bolje i stabilnije rezultate za zadatke za koje nije ni osmišljen.",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "36500": {
      "poster": "Lovren97",
      "content": "Možete li mi preporuciti neku knjigu/video/stranicu koja bi me najbolje uvela u deep learning? Prošao sam na feru strojno i znam sve osnove, no volio bih isto tako i za deep learning poceti sa osnovama i imati te neke temelje. Specificno vise bi volio možda deep learning za nlp, no ako znate i neku knjigu koja općenito govori i to bi bilo super.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "janeromero (Serial Number Q5U4EX7YY2E9N)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "36501": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@Lovren97#36500 [Ovo se smatra Biblijom dubokog učenja](https://www.deeplearningbook.org/)\n\nMeđutim ovo nema baš previše veze s modernim dubokim učenjem s obzirom na to da se od 2016. dosta toga izmijenilo, tako da ako želiš biti up to date trebaš pročitati jedno desetak značajnih papera koji su izašli u međuvremenu.\n\nŠto se tiče NLP-a, u NLP-u su najbitniji LSTMovi, attention, i transformeri. Dakle trebao bi pročitati sljedeće štivo:\n\n- [LSTM](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory)\n- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n\nOvo ti ne preporučam dok ne pročitaš Bibliju, jer vjerojatno ćeš jako malo toga razumjeti.",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "36503": {
      "poster": "InCogNiTo124",
      "content": "@Lovren97#36500 od knjiga, ovo je na medu sluzbenom literaturom na ferovom predmetu http://www.deeplearningbook.org/ moram napomenuti da nisam citao, ali cesto sam online vidio reference na to\n\nOstale zanimljive blogove/sadrzaje mozes naci na dnu stranice [ovdje](http://www.zemris.fer.hr/~ssegvic/du/)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "36504": {
      "poster": "InCogNiTo124",
      "content": "@micho#36501 da bas skimmam malo, ne malo tog se promjenilo od tad od stvari koje koristimo\n\nDa se bar povecalo razumjevanje",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "36506": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "> @InCogNiTo124#36504 Da se bar povecalo razumjevanje\n\n175 milijardi parametara na transformer modelu kojeg je nemoguće overfittati goes BRRRRRRRRRR",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "feel_d_boot (iNut)",
          "ink"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "36513": {
      "poster": "Lovren97",
      "content": "@micho @InCogNiTo124 hvala! bacim oko na ove stvari kaj ste mi napisali. Volim mislit da cu imati vremena sada preko ljeta pa bi volio to iskoristiti na nešto korisno 😅",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "member"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "janeromero (Serial Number Q5U4EX7YY2E9N)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "36653": {
      "poster": "[deleted]",
      "content": "@Lovren97#36500 Ja bi ti preporučio da prođeš prvo po članku [Deep Learning](https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf) tako da se upoznaš na intuitivnoj razini s osnovnim terminima. Zatim si potraži specifično review članke na temu kako se DU koristi u NLPu.\n\nNapisao si da imaš cijelo ljeto pa ti možda ne bi bilo loše uložiti 2-3 tjedna na klasične pristupe (čitaj \"ne duboke\" xd) NLPu. Tako ćeš dobiti potrebno znanje terminologije, a i neke ideje, koncepti, pristupi/tehnike i \"trikovi\" (npr. embedding matrica) su fundamentalni te se i dalje koriste .\n\nPogledaj si Karpathyev blog post [Unreasonable effectiveness of RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Isto tako, ako malo googlas, dosta ljudi preporuča Stanfordov course na temu NLPa - https://nlp.stanford.edu/teaching/.\n\nIsto tako, pogledaj si Šnajderov kolegij [Analiza i pretraživanje teksta](https://www.fer.unizg.hr/predmet/apt). Dobro je što sada ima video predavanja.\n\nŽivio.",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "36713": {
      "poster": "Lovren97",
      "content": "@tolecnal#36653 Hvala @tolecnal. Prošao sam Šnajderov kolegij i tamo sam se upravo upoznao sa svim tim finesama i ljepotama Deep Learning u NLPu, no osjecao sam se da sam malo možda uskočio prebrzo u to sve i da nisam imao tu neku osnovu iza Deep Learninga koju bi htio jako. Zato me zanima baš jel postoji tak nešto što bi mi recimo dalo te temelje iz kojih bi sve ovo sto sam do sada prošao uspio zbilja kvalitetno povezati 😃",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "janeromero (Serial Number Q5U4EX7YY2E9N)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "36748": {
      "poster": "InCogNiTo124",
      "content": "Computer vision gang, tek sad vidim da je doso [novi YoloV4](https://arxiv.org/abs/2004.10934), albeit prije mj dana",
      "votes": {
        "upvoters": [
          "Jimothy"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "36836": {
      "poster": "[deleted]",
      "content": "@InCogNiTo124#36748 Navodno (glavni) autor Yolo-a neće više raditi na AIu - https://www.reddit.com/r/MachineLearning/comments/f8wsyg/nd_yolo_creator_joseph_redmon_stopped_cv_research/",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]",
          "feel_d_boot (iNut)",
          "rija"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "36840": {
      "poster": "InCogNiTo124",
      "content": "@tolecnal#36836 dap, bas sam mu sinoc citao tweetove, malo tuzno",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "37268": {
      "poster": "Ovo_je_moj_nick",
      "content": "@micho#31235 \n\nNIsam skuzio iz cijelog texta, nemam previse vremena ulazit u proucavanje psota pa da te direkt pitam, jel to znaci da si ti radio GAN s kojim nisi imao konstantno jendag broj fake vs true inputa za descriptive dio mreze?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "37287": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@Ovo_je_moj_nick#37268 Nah, nije GAN, klasifikator i regresor koji su radili na postepeno jače koruptiranim labelama, a na kraju su finetuneani s pravim zadatkom i nekoruptiranim labelama.",
      "votes": {
        "upvoters": [
          "Ovo_je_moj_nick"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "37291": {
      "poster": "Ovo_je_moj_nick",
      "content": "Aha, vise kao weakly labeled problem, i onda taj isti model finetunnan s dobro obradjenim labelama.\n\nMogu ti rec iz prve ruke, takav fajntjuning je vrlo dobar, na neki nacin izbacis problem \"random incicijalizacije\" mreze.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "NekocBraca",
          "[deleted]",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "41401": {
      "poster": "Ovo_je_moj_nick",
      "content": "Jel ima netko visak iskustva s pruningom mreza?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "47621": {
      "poster": "[deleted]",
      "content": "Što kažete na ovo: https://twitter.com/adjiboussodieng/status/1277599545996779521 ?\n\nMoj stav: Dosta agresivno i u stvari jako ružno. Tako javno prozvat kolege/predavače da su rasisti. Čini se kao poprilično očajan potez samo da bi si povećala broj citata.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "Jimothy",
          "Red_Baron",
          "[deleted]",
          "mAcaLukas (mAca Lukas)",
          "rija"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "47622": {
      "poster": "InCogNiTo124",
      "content": "> @tolecnal#47621 Moj stav\n\nMoj isto, najobicniji marketing",
      "votes": {
        "upvoters": [
          "Jimothy",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "47628": {
      "poster": "Ovo_je_moj_nick",
      "content": "Ocito ne radi u DeepMindu. Nisam gledao lecture, ali mislim, jel treba svaki GAN opisat? Trebalo bi onda sve kineske derivate isto opisat? Neam misljenje dok ne pogledam lecture, a nemam vremena za to, ali uglavnom ovakav jadni live isprdak je po meni jadan, OSIM ako nije imala prvo internal discussion s ekipom i onda su joj nesto rekli sto joj se nije svidjelo.\n\n EDIT\n\n Mislim za papir koji je na arxivu i kemcanje oko toga da nesto nitko ne citira u \"9 mjeseci\" je presmijesno. Peer review 0 bodova",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "47629": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "Zamisli se triggerat jer netko ne spomene tvoju nefascinantnu mrežu koja koristi VAE reparametrizaciju u GANu kek",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "Ovo_je_moj_nick",
          "[deleted]",
          "turunturu (Cement)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "51032": {
      "poster": "InCogNiTo124",
      "content": "Izasla sluzbena knjiga o pytorchu\n\n[Deep Learning with PyTorch](https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf)",
      "votes": {
        "upvoters": [
          "Arfit",
          "Jimothy",
          "M-16",
          "Mike (Klaud Konzultanta)",
          "NekocBraca",
          "Red_Baron",
          "Sans",
          "Svarog (Veles)",
          "[deleted]",
          "indythedog",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "56211": {
      "poster": "InCogNiTo124",
      "content": "@tolecnal#33231 \n\nSto se tice reinforcement learninga, knjiga Sutton-Barto je super, ako vec kuzis o cem se radi. IMHO nije za ucenje o tome\n\nZa ucenje o reinforcement learningu (which could seriously be The Next Big Thing™) preporucujem [ovu videosekvencu](https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)\n\nCovjek iz Deep Minda polako i vrlo jasno obasnjava koncepte kroz 10ak videa tako da imaju glavu, tijelo i rep te strucno odgovara na pitanja. To + knjiga, i mohu reci da imate dovoljno znanja za iskodirat svog bota. Mozda eto downside sto koristi iznadprosjecno puno matematike (ViS i to), other than that nikakvih zamjerki, ukljucujuci i London accent.",
      "votes": {
        "upvoters": [
          "ink"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "56259": {
      "poster": "Ovo_je_moj_nick",
      "content": "@InCogNiTo124#56211 Lik je NLovac s tvrdokornim niskozemskim naglaskom ;)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "56262": {
      "poster": "InCogNiTo124",
      "content": "@Ovo_je_moj_nick#56259 izgleda da sam uzasan u prepoznavanju naglasaka\n\nIn my defense ne znam jesam ikad i cuo NLovca kak prica eng",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "56306": {
      "poster": "[deleted]",
      "content": "@InCogNiTo124#56211 Dosta zanimljiv stav moram priznat.\n\n\"Sutton-Barto je super, ako vec kuzis o cem se radi. IMHO nije za ucenje o tome\"\n\nJel mozes malo vise prokomentirat ako ti nije problem. Što točno misliš da nedostaje u knjizi, odnosno da ju ne čini dobrim za upoznavanje s RLom?\n\nNisam vidio ovu playlistu (vidio sam i gledao onu od Davida Silvera), ali ovo je nešto \"novo\". Izgleda da i njihova prezentacija/lekcije  idu kao i Sutton & Barto, što mi se sviđa. Oni obrade u 7 lekcija (od 10) prva dva dijela knjige (340 stranica). Nisam siguran da stigne sve obraditi u detalje, ali ok. Svima nam je jasno da knjiga + video predavanja = najbolja kombinacija.\n\n\"Mozda eto downside sto koristi iznadprosjecno puno matematike (ViS i to)\": Dosta zanimljiv komentar. To mi je onako nekako kao kada ekipa komentira Šnajderu da na Strojnom ima previše matematike xd.\n\nUnatoč tome što modeli RLa imaju svoje temelje u matematici, svejedno u praksi se treba dosta namučiti da nešto korisno naučiš (pogtovo kod DRLa). Sada zamisli da ima premalo matematike hahahaha pa to ne bi niš radilo.\n\nMoram priznat da mi se kod playliste svida to što nije naglasak na DRLu, unatoč tome što je to DeepMind. Super.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "56311": {
      "poster": "InCogNiTo124",
      "content": "> @tolecnal#56306 Jel mozes malo vise prokomentirat ako ti nije problem. Što točno misliš da nedostaje u knjizi, odnosno da ju ne čini dobrim za upoznavanje s RLom?\n\nNe znam, moguce da je do stila pisanja ali nakon citanja knjige mi puno toga nije bilo jasno, vise kao juha pojmova a ne neka koherentna povezana cjelina. Zato kazem da je dobra ak se vec zna o tome nesto, kad vec imas neki birds eye pogled pa rupe popunis knjigom. Za to je jebena. Ali video je bolji za dobit taj birds eye haha\n\n> @tolecnal#56306 \"Mozda eto downside sto koristi iznadprosjecno puno matematike (ViS i to)”: Dosta zanimljiv komentar. To mi je onako nekako kao kada ekipa komentira Šnajderu da na Strojnom ima previše matematike xd.\n\nOke da se razumijemo, ja _zaista_ nemam problem s matematikom. Samo sam htio rec da ti videi nisu na razini Coursere di je sve hand-wavy i di se upoznaje s konceptima iskljucivo na apstraktnoj razini, vec zahtjevaju predznanje, cisto ono ako netko naleti na te videe/moj review da zna sto ocekivat, jel.\n\n> @tolecnal#56306 Moram priznat da mi se kod playliste svida to što nije naglasak na DRLu, unatoč tome što je to DeepMind. Super.\n\nIstina, i jos na toj razini kvalitete, ugodno iznenadenje",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "56312": {
      "poster": "InCogNiTo124",
      "content": "@InCogNiTo124#56211 oke ispada da sam autistican i _nekako_ stavio krivi link 😐😐😐\n\n@micho prepravi mi pliz url u [ovo](https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "56318": {
      "poster": "[deleted]",
      "content": "@InCogNiTo124#56312 Aha mislio si na predavanja Davida Silvera. Lol. Iako mi se i prva playlista sviđa.\n\nOno što mi je bolje kod Silverovih predavanja nego li u knjizi jest to što on kreće s Markovljevim lancima pa dodaje nagrade i onda na kraju doda akcije pa time dobije MDP. U prethodnoj playlisti i u knjizi odmah se kreće sa MDPovima, kao da su pali iz vedra neba.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "slip"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "57486": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "DL nibbas: Adam je najbolji optimizator!!!!!!!111\n\n[Nakkiran et al.](https://arxiv.org/pdf/1912.02292.pdf): Ara ara Sheguwichu-san\n\n![](assets/2020-07-28/00006.png)\n\n>! Insane rad, must read",
      "votes": {
        "upvoters": [
          "[deleted]",
          "feel_d_boot (iNut)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "57490": {
      "poster": "Ovo_je_moj_nick",
      "content": "@micho#57486 \n\nhttps://youtu.be/S27pHKBEp30?t=1325",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "57491": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@Ovo_je_moj_nick#57490 Kao veliki fan LSTM-a koji puno radi s transformerima ne bih se složio, ali to je samo do momenta kad izumimo Transformer arhitekturu koja actually dobro radi a ne ovo Vaswanijevo sranje",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "57492": {
      "poster": "InCogNiTo124",
      "content": "@micho#57486 joj pa to je segvic pricao na dubokom dok su jos bila predavanja\n\nNedavno sam na redditu naletio da je to poznat fenomen jos iz 90ih, odnosno da ga je neki lik koji se bavi bayesian statistikom predvidio jer je velik broj parametara zapravo pod utjecajem CLT il nes, pokusam nac opet",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "57493": {
      "poster": "InCogNiTo124",
      "content": "@InCogNiTo124#57492 \n\nKomentar je [ovdje](https://www.reddit.com/r/statistics/comments/hc54lc/-/fvdxlx8) mico mozes editat moj previous post i deletat ovaj",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "57495": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#57493 Pričao je da postoje naznake, al očito ovaj rad je nakon te spike (on je to pričao u 10. mj. 2019., rad je izašao u 12. mj. 2019.)\n\nA to o čem pričaš je parameter-wise double descent, svjetla točka ovog rada je epoch-wise gradient descent (tj. broj epoha kao kompleksnost modela)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "57500": {
      "poster": "[deleted]",
      "content": "\"... and there are computer scientists that mostly do not know mathematics.\"\n\nVladimir Vapnik, 2019. , [Lex Fridman's AI Podcast](https://youtu.be/STFcvzoxVw4?t=1479)\n\nMoje mišljenje: The man, the myth, the legend je u pravu.",
      "votes": {
        "upvoters": [
          "Red_Baron",
          "feel_d_boot (iNut)",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "57966": {
      "poster": "InCogNiTo124",
      "content": "https://www.reddit.com/r/MachineLearning/comments/i1aafb/p_i_trained_a_gan_to_generate_photorealistic_fake/",
      "votes": {
        "upvoters": [
          "Filemon",
          "Mioch",
          "Svarog (Veles)",
          "Vonj",
          "[deleted]",
          "__builtin_popcount (std::popcount)",
          "benac (caneb)",
          "feel_d_boot (iNut)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Filemon",
          "McKovalski",
          "Mioch",
          "Mrcina (Yoso)",
          "Ovo_je_moj_nick",
          "Piki",
          "Svarog (Veles)",
          "__builtin_popcount (std::popcount)",
          "feel_d_boot (iNut)",
          "in1",
          "lup",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
          "rija"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "58818": {
      "poster": "[deleted]",
      "content": "https://twitter.com/Tim_Dettmers/status/1143193575011733512?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1143193575011733512%7Ctwgr%5E&ref_url=https%3A%2F%2Fwww.quora.com%2FWhat-is-currently-the-best-GPU-for-deep-learning\n\nDeep learning postaje sve skuplji i skuplji sport, pogotovo ako hoces biti kompetetivan.",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "58822": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "Bilo bi stvarno šteta kad bi dovršio 500 linija training petlje i onda je nepovratno obrisao u VSCodeu :)",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Filemon"
        ],
        "wtf": [],
        "tuga": [
          "NekocBraca",
          "feel_d_boot (iNut)"
        ]
      }
    },
    "58824": {
      "poster": "InCogNiTo124",
      "content": "@micho#58822 just use [pytorch-lightning](https://github.com/PyTorchLightning/pytorch-lightning)",
      "votes": {
        "upvoters": [
          "[deleted]",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "58833": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#58824 Nije prikladno ni za što nekarakteristično - npr. sam modul je već u modulu koji koristimo, ali za naš use case u NLP timu ne radi jer iako nema apstrakcija je opet prelimitirajuć.\n\nOno što je problem kod ovakvih stvari je što pretpostavljaju formu operacije, i onda jbg kad treba nešto distilirati (nije klasični SGD), nedaj bože neko podržano učenje, ili samo imati dataset s različitim procesorima i sve pada u vodu 👽\n\nAli napisat ću neki lagani API koji nema pretpostavki kad dovršim ovo što radim pa ćemo to valjda pushati u naš tools repo u firmi, većinu već imam jer sam pisao koliko-toliko lijepo.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "Mariosss77 (Trevor)",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "58836": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "Ako netko nekim slučajem želi tumor na mozgu: [FFZG dijaspora edition](https://news.sky.com/story/white-artificial-intelligence-risks-exacerbating-racial-inequality-study-suggests-12043491)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "tuga": [
          "feel_d_boot (iNut)"
        ]
      }
    },
    "58837": {
      "poster": "InCogNiTo124",
      "content": "@micho#58836 i shall try to one you up sa neironicnim amerima\n\nhttps://m.facebook.com/story.php?story_fbid=696884627707267&id=390977958297937",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "feel_d_boot (iNut)",
          "maPre",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
          "rija"
        ]
      }
    },
    "59863": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "weeb learning\n\n![](assets/2020-08-13/00019.png)",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "60053": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "Zovu me Jan Šnajder\n\n```\n{\n  \"evaluation_set\": {\n    \"accuracy\": 0.2412583358956394,\n    \"precision\": 1.2398406213545166,\n    \"recall\": 1.0024520199342593,\n    \"f1_score\": 1.1085803096236595\n  }\n}\n```",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "AromaticConfusion (VrloZbunjen)",
          "InCogNiTo124",
          "Lyras",
          "NekocBraca",
          "__builtin_popcount (std::popcount)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "60056": {
      "poster": "InCogNiTo124",
      "content": "@micho#60053 haha imas negativno false negativa haha",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "60058": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#60056 Nah, samo sum vs mean. Pravi rezultati su duplo manji za precision i recall i 4 puta manji za f1. Nisam skužio grešku prije jer kak je prije klasifikator radio samo MAP u ovisnosti o datasetu ne postoji distinkcija sume i prosjeka za binarnu klasifikaciju xD",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "65450": {
      "poster": "feel_d_boot (iNut)",
      "content": "Nezz jel iko objavia ovo, ali...\n\nI'm not like other DL guys\n\nhttps://www.youtube.com/watch?v=fCLI6kxFFTE",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "65722": {
      "poster": "InCogNiTo124",
      "content": "Jel iko cuo za onaj hrvatski startup (?) koji je razvio neuronku koja kakti sazme vijesti? Siguran sam da sam cito o njima prije ajmorec 2 godine i da su iz hrvatske ali ne mogu nis naci o tome",
      "votes": {
        "upvoters": [
          "[deleted]",
          "feel_d_boot (iNut)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "65723": {
      "poster": "feel_d_boot (iNut)",
      "content": "@InCogNiTo124#65722 \n\nČuja sam za to i pokušavam naći bezusjpešno to već duže vrime, ali nisam pojma ima da su Hrvati",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "67625": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "[Malo drame u dubokoučiteljskom svijetu](https://arxiv.org/pdf/2009.01215v1.pdf)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "68763": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "well hello there\n\n![](assets/2020-09-10/00008.png)",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "Ovo_je_moj_nick",
          "[deleted]",
          "grana2"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "69694": {
      "poster": "InCogNiTo124",
      "content": "https://patents.google.com/patent/US20160217368A1/en",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "70595": {
      "poster": "[deleted]",
      "content": "Moram se požalit negdje pa ću to ovdje napravit.\n\nVolim si isprintat seminalne radove i onda \"meditirati\" nad njima, ali ekipa zna stvarno ponekad biti mutava i onda mi ima najmanjim mogući font.\n\nNpr. , [Human level control through DRL](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) , ja nezz je li ekipa iz Nature-a mutava ili šta, ali evo, neka mi netko objasni ovakav font. I još k tome pisan u dva stupca. -.-\n\nZar je teško napraviti [ovakav](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf) template , ili [ovo](https://arxiv.org/pdf/1811.12560.pdf).",
      "votes": {
        "upvoters": [
          "ink"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "70611": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@tolecnal#70595 To je klasičan Nature font i primjeren je za časopise (Nature nije paper publisher lol), a oba ova primjera šta si dao su scuffed - prvi dozvoljava više od 66 charactera u retku tj. preširok je, drugi ima font size 14 valjda (umjesto 12 ili 11, kako je praksa)\n\ntwosided je idealan za članke u časopisu\n\nmislim da se razumijemo, ne mislim da je Nature savršen, al s obzirom na primjenu barem nema mane kao ova dva",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "70639": {
      "poster": "[deleted]",
      "content": "@micho#70611 \n\n> @micho#70611 Nature nije paper publisher lol\n\nZašto moraš sve interpretirati doslovno xd?\n\n> @micho#70611 drugi ima font size 14 valjda (umjesto 12 ili 11, kako je praksa)\n\nPa upravo se na to i žalim, na premale fontove.\n\n> @micho#70611 twosided je idealan za članke u časopisu\n> \nZašto? \n\n> @micho#70611 dozvoljava više od 66 charactera u retku tj. preširok je\n\nKoliko ja znam ova duljina [varira](https://en.wikipedia.org/wiki/Line_length) (između 45 i 75 karaktera).",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "70652": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "> @tolecnal#70639 Zašto moraš sve interpretirati doslovno xd?\n\nČinilo mi se da kritiziraš Nature za loš format papera, jer ne vidim ništa drugo za što bi ih mogao kritizirati, bar ne kod formattinga. Kažem, isprintaj si to na A4 papir pa ćeš vidjeti zašto je dobar format.\n\n> @tolecnal#70639 Pa upravo se na to i žalim, na premale fontove.\n\nNe znam jesi ikad printao papere (kao što bi i trebao ako misliš kritizirati formatting, ti paperi nisu za elektroničke uređaje), ali 12pt je defacto najbolji size za najveći broj ljudi (ni prevelik ni premalen), dok neki ljude vole i 11pt, a manji broj 10pt. Ne znam kako čitaš papere, ali ovaj drugi čitati na papiru... ajmo reći da ćeš izgledati ko klaun, jedino gore od toga je paper u Wordu\n\n> @tolecnal#70639 Zašto?\n\nZato što je efikasan prilikom printanja (štedi skuplji, časopis papis) i kad se figurei raspodijele veća je šansa da će figurei iz teksta završiti na istoj strani. Da ne govorimo da uz 10pt-12pt fontove apsolutno onemogućava više od 66 znakova u retku (po stupcu).\n\n> @tolecnal#70639 Koliko ja znam ova duljina [varira](https://en.wikipedia.org/wiki/Line_length) (između 45 i 75 karaktera).\n\nZaboravio si nastaviti ovo otkud god si pročitao\n\n> though the ideal is 66 cpl (including letters and spaces)\n\nInače izvor je Bringhurst, The Elements of Typographic Style. Za tešku literaturu, kao što su znanstveni radovi preporuča se manji broj znakova po liniji.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "70660": {
      "poster": "[deleted]",
      "content": "> @micho#70652 isprintaj si to na A4 papir\n\n> @micho#70652 Ne znam jesi ikad printao papere\n\nNapisao sam u prvoj objavi da \"Volim si **isprintat** seminalne radove i onda “meditirati” nad njima.\"\n\nSlova su premala. Barem meni.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "70664": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@tolecnal#70660 Aha, previdio sam, isprike, guess it's personal preference",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "72078": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "Bruh\n\n[A Multimodal Memes Classification: A Survey and Open Research Issues](https://arxiv.org/abs/2009.08395v1)",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "72081": {
      "poster": "InCogNiTo124",
      "content": "@micho#72078 taman sam mislio u slobodno vrijeme se zajebavat s time",
      "votes": {
        "upvoters": [
          "[deleted]",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "72128": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#72081 Ah yes kao i ja s programskim jezikom\n\nturns out free time is a scam :^)",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124",
          "NekocBraca"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "73101": {
      "poster": "[deleted]",
      "content": "https://twitter.com/krandiash/status/1302702145808953344",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124",
          "cotfuse",
          "feel_d_boot (iNut)",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "74790": {
      "poster": "Miki",
      "content": "Kompozij linkova na sve\n\nhttps://docs.google.com/document/d/1wvtcwc8LOb3PZI9huQOD7UjqUoY98N5r3aQsWKNAlzk/edit",
      "votes": {
        "upvoters": [
          "BaboTrojka",
          "InCogNiTo124",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "77679": {
      "poster": "[deleted]",
      "content": "Prvih 5 i pol minuta: https://www.youtube.com/watch?v=TrdevFK_am4&ab_channel=YannicKilcher \n\n😆 🤣  😆 🤣",
      "votes": {
        "upvoters": [
          "Ovo_je_moj_nick"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "77681": {
      "poster": "[deleted]",
      "content": "Nisam vidio da je netko share-ao ovu ekipu: [Machine Learning and AI Academy](https://www.youtube.com/channel/UC4lM4hz_v5ixNjK54UwPEVw/videos).",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "77683": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@tolecnal#77679 Netko se fura na Satoshija kek",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "77966": {
      "poster": "[deleted]",
      "content": "![](assets/2020-10-06/00005.jpeg)\n\nIzvor: deeplearning.ai's LinkedIn post",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "Ovo_je_moj_nick"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "78947": {
      "poster": "Ovo_je_moj_nick",
      "content": "https://petapixel.com/2020/10/06/nvidia-uses-ai-to-slash-bandwidth-on-video-calls/",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "79859": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "[Mossad: Defeating Software Plagiarism Detection](https://arxiv.org/abs/2010.01700v1)\n\nidemo",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "79861": {
      "poster": "InCogNiTo124",
      "content": "@micho#79859 kolega na faksu je radio detekciju plagijata preko AST, koliko bi ovo pomoglo u tom slucaju?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "79881": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#79861 Po onome što piše u radu, AST metode nemaju apsolutno nikakve šanse protiv ovoga. Ovo modificira postojeće plagijate koliko sam skužio i super effective je protiv jačih metoda, ne samo tih primitivnih.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "81482": {
      "poster": "benac (caneb)",
      "content": "Jel se netko ovdje bavio s https://gym.openai.com/",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "81483": {
      "poster": "InCogNiTo124",
      "content": "@benac#81482 ja, ali malo. I summon @tolecnal on je dezurni za potkrjepljeno učenje",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "81484": {
      "poster": "[deleted]",
      "content": "@benac#81482 Nešto malo. Trebao bi početi sada za dipl. projekt više, ali za sada ništa gdje je observation/state space slika.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "benac (caneb)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "83602": {
      "poster": "InCogNiTo124",
      "content": "https://ai.facebook.com/blog/introducing-many-to-many-multilingual-machine-translation/",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "87386": {
      "poster": "InCogNiTo124",
      "content": "Za 10ak dana odrzat ce se pytorch developer day online evenat\n\nhttps://pytorchdeveloperday.fbreg.com/",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "94914": {
      "poster": "[deleted]",
      "content": "[Keynote: Sepp Hochreiter, “Modern Hopfield Networks”](http://icdm2020.bigke.org/). Danas u 13:15. [Hopfield Networks Is All You Need](https://arxiv.org/abs/2008.02217) (Sada je pitanje kada će izaći predavanje za širu publiku ...).\n\nP.S. Imam osjećaj da je Subašić oduševljen.",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "105095": {
      "poster": "[deleted]",
      "content": "https://www.reddit.com/r/MachineLearning/comments/k5ryva/d_ethical_ai_researcher_timnit_gebru_claims_to/\n\nhttps://www.reddit.com/r/MachineLearning/comments/k5ryva/d_ethical_ai_researcher_timnit_gebru_claims_to/gegt3o4?utm_source=share&utm_medium=web2x&context=3",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "105096": {
      "poster": "InCogNiTo124",
      "content": "@tolecnal#105095 nikad cuo za zensku",
      "votes": {
        "upvoters": [
          "ink"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "105098": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "womyn go bek to kičin\n\ni ostale stvari kojima ova ženska opravdava svoje bullyjanje",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "105104": {
      "poster": "[deleted]",
      "content": "@InCogNiTo124#105096 Niti ja, ali sam čuo za Jeff Deana. Jedan redditor(ka) je dolje komentirao/la da su Timnit Gebru i Anima Anandkumar toksične na Twitteru. Fun fact: u stvari se ne zna ništa, prazan skup, a žena na twitteru masivno dobiva podršku iako se ništa ne zna. Baš me zanima kako će se stvari raspetljat.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "105106": {
      "poster": "HeHe (Direktor života)",
      "content": "@tolecnal#105095 Šta je šovinisto smrdljiva. To vaše mašinsko izučavanje je samo san svakog uvrnutog muškarca da projektira svoje fantazije. Rasisti i seksisti. Fuj",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "105107": {
      "poster": "InCogNiTo124",
      "content": "@tolecnal#105104 ne znam koliko su te zene toksice, ali znam koliko je twitter i ne cudi me, valjda zenska ima svoj fanclub\n\nnadam se da joj google nece vratit posao samo zbog fancluba",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "105109": {
      "poster": "iLabaviUmeDaZabavi (prvi do boga)",
      "content": "Google je odavno postao leftard kompanija",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "105113": {
      "poster": "[deleted]",
      "content": "@InCogNiTo124#105107 Upravo to je i rekao ovaj gambs na redditu xd.",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "105114": {
      "poster": "InCogNiTo124",
      "content": "@tolecnal#105113 mora da taj s reddita neki pametni xD",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Filemon"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "106147": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "# Warning Guardian članak\n\nhttps://www.theguardian.com/technology/2020/dec/04/timnit-gebru-google-ai-fired-diversity-ethics\n\n![](assets/2020-12-06/00019.jpeg)\n\nGuardian je za jednakost osim ako si bijeli muškarac, ko jebe izjave bijelih muškaraca",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]",
          "feel_d_boot (iNut)",
          "maPre",
          "rija"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Broono (Burućuh)",
          "Dekan",
          "HeHe (Direktor života)",
          "Lyras",
          "feel_d_boot (iNut)",
          "in1",
          "mAcaLukas (mAca Lukas)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "110752": {
      "poster": "miss_anthropocene (neunist.iva)",
      "content": "jel postoji kakva GPU virtualka dostupna studentima? znam za azureove, al ne znam jel ih mogu uzet u sklopu onog studentskog budgeta od 100$?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "110766": {
      "poster": "InCogNiTo124",
      "content": "@miss_anthropocene#110752 google colab je najbolji ak ti ne treba nist fensi, jeftin i easily available\n\nOvdje je vise comprehensive lista:\n\nhttps://github.com/zszazi/Deep-learning-in-cloud",
      "votes": {
        "upvoters": [
          "Svarog (Veles)",
          "[deleted]",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
          "miss_anthropocene (neunist.iva)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "110959": {
      "poster": "Miki",
      "content": "@miss_anthropocene#110752 \n\nNisam imao priliku koristit ovo ali naleto sam na [lambdalabs](https://lambdalabs.com/service/gpu-cloud).\n\nČini mi se da pružaju jako kompetitivnu cijenu i mislio sam ih možda koristit kad budem trenirao modele za diplomski rad.",
      "votes": {
        "upvoters": [
          "miss_anthropocene (neunist.iva)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "110975": {
      "poster": "miss_anthropocene (neunist.iva)",
      "content": "@InCogNiTo124#110766 \n\n@Miki#110959 \n\nwhat a time to be alive 😍",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Filemon",
          "InCogNiTo124",
          "STAGGER_LEE"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "114457": {
      "poster": "InCogNiTo124",
      "content": "MuZero algoritam jednako dobro performira na Atari videoigrama kao i na sahu i Go iako mu nikad nisu bila recena pravila.\n\nhttps://arxiv.org/abs/1911.08265\n\n> When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.\n\nHypehypehypehype",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "115606": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "https://www.reddit.com/r/MachineLearning/comments/km0rcz/d_i_refuse_to_use_pytorch_because_its_a_facebook/",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "115609": {
      "poster": "InCogNiTo124",
      "content": "@micho#115606 \n> perfectly good alternative (tensorflow)\n\nTo ce biti jaix od mene\n\nEdit: uostalom tf je googleov u istoj mjeri koliko je pt fejsov, tako da mu je s moralne strane isti faking kurac",
      "votes": {
        "upvoters": [
          "[deleted]",
          "rija"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "115610": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#115609 Softverski vegani, ništa više nije potrebno za reći\n\nDoslovno bi jeli govno da ga je Linus Torvalds posrao",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Miki",
          "NekocBraca",
          "Svarog (Veles)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "116009": {
      "poster": "[deleted]",
      "content": "Izašao je draft knjige _Probabilistic Machine Learning_ od Murphya: https://probml.github.io/pml-book/book1.html",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]",
          "kix7 (Fish99)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "121685": {
      "poster": "InCogNiTo124",
      "content": "Je li umjetna inteligencija otisla predaleko?\n\nhttps://www.popularmechanics.com/technology/robots/a35165370/microsoft-resurrects-the-dead-chatbots/",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "121863": {
      "poster": "[deleted]",
      "content": "@InCogNiTo124#121685 Prije bi rekao da je Microsoft otišao predaleko. Zabrijali su si malo.",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "121864": {
      "poster": "[deleted]",
      "content": "Bertsekas ima novo izdanje kolegija \"Reinforcement Learning and Optimal Control\" baziran na svojoj knjizi istog imena. Link: http://www.mit.edu/~dimitrib/RLbook.html",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "139121": {
      "poster": "Asura (Asura)",
      "content": "Zamolio me frend  s veterine da malo istrazim misljenja s nase strane o onome sto on radi.\n\nUkratko, jos tamo pocetkom 2017. uvidio je korisnost DL-a u veterini. Krenuo je uciti Python te DL koncepte. Nakon negdje godinu dana napisao je [ovaj post](https://kvinicki.medium.com/why-veterinary-medicine-desperately-needs-deep-learning-daf1785f2146) gdje je uz najopskurniji prototip uspio semi-automatizirano prikupiti dataset i istrenirati model i dobiti dovoljno dobre rezultate s kojima je i objavio jedan paper na Arxivu. Nakon toga je krenu u izgradnju enabling tehnologije, odnosno skenera koji bi skeniranje odradio kako treba. To je krenulo negdje krajem 2018. i trenutna verzija skenera je poprilicno solidna kao sto se moze vidjeti u [drugom postu](https://kvinicki.medium.com/lets-redefine-veterinary-pathology-dbc6f6538b03).  U medjuvremenu kako se skener razvijao, probavao je rijesiti razne probleme sa slikama koje bi prikupljao. Neki problemi su bili prelagani i nisu imali niti primjene, dok neki su zahtijevali kvalitetnije slike i/ili modernije modele. Skener je jos uvijek u fazi poboljsavanja i tu ima jako puno posla jer slike su na razini mikrona i svaka mala vibracija moze pokvariti rezultate (vremenski / kvalitativno).  Sto se tice hardwarea, za sada imaju jednu 2080 i nekoliko slabijih 2060. U pocetku je radio sam, ali vec oko godinu dana prikuplja studente veterine koji uce programirati i do neke mjere shvatiti modele koje koriste. Takodjer, izradjen je jedan GUI za olaksano oznacavanje dataseta. Jos neke zanimljive informacije se mogu vidjeti u videu na kraju drugog posta.\n\nOno sto je frendu bitno je okvirno misljenje o tome sto se uvodi DL na veterini, sto studenti uce programirati te kreiraju zajednicu koja ce koristiti high level library za rjesavanje problema u veterini.\n\nU jednom trenutku je cak i pronasao jednog studenta koji je iz RZ podrucja. To je jako lose zavrsilo jer je stav lika bio da ne mora nista znati o datasetu i da su veterinari samo cheap labor za oznacavanje dataseta, a da je njegov jedini posao koristiti  .fit i .predict.",
      "votes": {
        "upvoters": [
          "Hrvoje (Hrvoje45)",
          "InCogNiTo124",
          "Kushim",
          "[deleted]",
          "feel_d_boot (iNut)",
          "mAcaLukas (mAca Lukas)",
          "member"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "144259": {
      "poster": "netbitan11 (netbitan)",
      "content": "Ima li netko iskustva s generiranjem glazbe pomoću AI? Tražim temu za završni rad i jedna od ideja mi je bila sakupiti dataset klapskih pjesama i dobiti kopjuterski generiranu klapsku pjesmu, ali praćenje nekog postupka kao na ovom [linku](https://towardsdatascience.com/generating-music-with-artificial-intelligence-9ce3c9eef806) mi se čini prevelik zalogaj za završni rad. Bilo kakva informacija, kontraprijedlog za kombinaciju AI i glazbe ili općenito neki prijedlog za temu za završni rad je dobrodošao.",
      "votes": {
        "upvoters": [
          "Stoja_9 (Bije_san_u_autobusu)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "144260": {
      "poster": "slip",
      "content": "@netbitan11#144259 Ne znam ništa o tom, no vidi npr. ove završne radove (navodno ih možeš posudit u knjižnici FER-a):\n\nhttps://repozitorij.fer.unizg.hr/islandora/object/fer%3A3344\n\nhttps://repozitorij.fer.unizg.hr/islandora/object/fer%3A3216",
      "votes": {
        "upvoters": [
          "netbitan11 (netbitan)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "144265": {
      "poster": "InCogNiTo124",
      "content": "@netbitan11#144259 ovisi sta predvidas. Generiranje audiosamplesa je dosta tezak problem, tek su ga relativno nedavno poceli rjesavat, i to ne generalno. evo bas je ovaj mjesec doso novi model za generiranje ljudskih glasova. Nadam se da imas dosta intuicije da vidis nakon ovog da feedanje klapskih pjesama kao mp3 i generiranje nije pristup kojeg zelis\n\nNo, ono sto je rjesivo je recimo, generiranje glazbe u nekom stilu. Recimo jos prije par godina je google razvio style transfer model koji uzme sekvencu nota i pretvori ih u stil recimo chopina/bacha itd. To je mislim pristup kojem trebas teziti (ako zelis okej rezultate, to jest) samo kad bi imao dataset nota svih pjesama haha to mozda i mozes dobit strojno s nekim music-to-sheet-notes alatom",
      "votes": {
        "upvoters": [
          "[deleted]",
          "netbitan11 (netbitan)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "146522": {
      "poster": "mAcaLukas (mAca Lukas)",
      "content": "@InCogNiTo124#144265 a kad bi mu sastavio veliki dataset pjesama i tekstova u nekom žanru, bili mogao generirati pjesme u tom žanru?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "146523": {
      "poster": "mAcaLukas (mAca Lukas)",
      "content": "> @mAcaLukas#24387 Nitko se nije javio, ali nije da na račun toga mogu odjebat projekt. Uglavnom, danas dobio pločicu, javljat ću svoja zapažanja redovito ovdje\n\nZaključak projekta: Vitis AI zasad pravu podršku ima samo na skupljim pločicama (Ultrascale), za Pynq Z1 postoji FINN koji ok radi s njihovim primjerima, ali je muka natjerat bilo šta drugo da radi na njemu.\n\nDon't bother",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "146525": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@mAcaLukas#146522 bi, al bi to sve bile interpolacije viđenog, i šanse su da bi bilo slično jednoj ili dvije pjesme u tolikoj mjeri da bi bilo kao remix.\n\nUčenje generativnih modela ti je kao učenje reverzne hash funkcije - na ulazu je hash, a on pokušava izbaciti originalni podatak. I što ga više hasheva naučiš, to će on vjernije naučiti neki žanr, ali isto tako tranzicije ti postaju glađe čime imaš manje generalizacije. Dakle daš mu diskografiju Sinana Sakića, on neće postati Sinan, nego neki eksperimentalni cover band Sinana.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]",
          "mAcaLukas (mAca Lukas)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "146526": {
      "poster": "mAcaLukas (mAca Lukas)",
      "content": "@micho#146525 that's ok, krajiška muzika to i jest u suštini",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "146529": {
      "poster": "InCogNiTo124",
      "content": "@mAcaLukas#146522 u teoriji, mozda. generiranje teksta je najlaksi problem, i vec sad ga mozes u jednom danu rjesit, puno laksi od, recimo, generiranja glazbe i pjevanja teksta. S jednom neuronkom gotovo nemoguce, s vise (recimo jedna za generiranje teksta, druga za glazbu, treca za pjevanje teksta, cetvrta za style transfer glasa -> glazba) bi se jos i dalo. no sad ulazimo u sferu \"definitivno nemas dovoljno resurasa za ovo, ako hoces da bude kvalitetno\".",
      "votes": {
        "upvoters": [
          "[deleted]",
          "mAcaLukas (mAca Lukas)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "146530": {
      "poster": "InCogNiTo124",
      "content": "@micho#146525 bas sam o tom razmisljao neki dan, generativni modeli rade dosl obrnuto od onog sta radi npr shazam ili spotify za recomendere haha",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "146531": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@mAcaLukas#146526 A onda okej ali sljedeći problem ti je kvaliteta. Najbolju kvalitetu dobiješ midijem, a to ti je možda i okej\n\nAli onda imaš problem teksta, a tu si totalno u kurcu jer nemamo ni jedan JAKI model koji uopće [u]razumije[/u] govor. Daš mu cajke i stalno će blebetat o ženama, pijanstvu itd., što i nije toliko različito od cajki generalno, ali to će biti interpolacije postojećih tekstova. Dakle\n\n[center]\nSamo pijano drugovi,\n\nda prebolim\n[/center]\n\nkind of shit.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Hrvoje (Hrvoje45)",
          "InCogNiTo124",
          "Lyras",
          "miss_anthropocene (neunist.iva)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "146532": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "Međutim ako želiš vocaloid cajke tj. Hacune Žika, to može, sam onda sam treba napisat tekst",
      "votes": {
        "upvoters": [
          "[deleted]",
          "mAcaLukas (mAca Lukas)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "146533": {
      "poster": "mAcaLukas (mAca Lukas)",
      "content": "Razumijem, moram rentat cluster i dodat primjese amino muzike kako bi bilo zanimljivo.\n\nKickstage money go brrrrrrrr",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "147496": {
      "poster": "[deleted]",
      "content": "> @mAcaLukas#146522 [InCogNiTo124](https://fer.studosi.net/d/272/191) a kad bi mu sastavio veliki dataset pjesama i tekstova u nekom žanru, bili mogao generirati pjesme u tom žanru?\n\nUz dobar model sigurno bi jako dobro radilo, ali problem je što znači veliki dataset pjesama. Ono što ti možeš trenutno naći i sastaviti je vjerojatno jako daleko od dovoljno velikog i raznolikog dataseta koji bi iz toga mogao nešto kvalitetno raditi. \n\nA i kod dobiješ takav dataset, trebao bi dosta istraživanja da dođeš do nekog generativnog modela koji bi dobro radio, barem 4-5 iteracija raznih papera.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "147548": {
      "poster": "mAcaLukas (mAca Lukas)",
      "content": "@Delpins#147496 jeli 65gb krajiške muzike dovoljno za daljnje treniranje?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Hrvoje (Hrvoje45)",
          "McKovalski",
          "Mioch",
          "NekocBraca",
          "Svarog (Veles)",
          "jadzia-dax (Jadzia Dax)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "147550": {
      "poster": "[deleted]",
      "content": "@mAcaLukas#147548 vjv nope",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "147561": {
      "poster": "mAcaLukas (mAca Lukas)",
      "content": "@Delpins#147550 why even live?",
      "votes": {
        "upvoters": [
          "[deleted]",
          "miss_anthropocene (neunist.iva)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "148633": {
      "poster": "Asura (Asura)",
      "content": "@netbitan11 \n\nNajbolje ti je krenuti s basics. Pitaj se koliko ce ti trebati da sakupis dataset, koliko vremena ce ti trebati da izvrtis neki baseline. Ako je ovo vise od 40 dana, trazi drugu temu.\n\nCak i da ti ne radi model kako treba, barem mozes vidjeti zasto nije radio dobro. To je po meni isto dobar zavrsetak.",
      "votes": {
        "upvoters": [
          "netbitan11 (netbitan)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "151620": {
      "poster": "InCogNiTo124",
      "content": "[Identifying signals associated with psychiatric illness utilizing language and images posted to Facebook](https://www.nature.com/articles/s41537-020-00125-0)\n\n> Integrating Facebook data with clinical information could one day serve to inform clinical decision-making.\n\nRIP",
      "votes": {
        "upvoters": [
          "[deleted]",
          "mAcaLukas (mAca Lukas)",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [
          "M-16",
          "Mioch",
          "NekocBraca",
          "Red_Baron",
          "Svarog (Veles)",
          "koode7",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
          "slip"
        ],
        "tuga": []
      }
    },
    "151686": {
      "poster": "mAcaLukas (mAca Lukas)",
      "content": "@InCogNiTo124#151620 Doslovno dijagnosticirat ljude koji komentiraju na indexu",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124",
          "M-16",
          "Red_Baron"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "152052": {
      "poster": "vjeva",
      "content": "je li netko pokusavao migrirati model checkpoint iz tf1 u tf2?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "152064": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@vjeva#152052 da, sounds great, generally doesn't work\n\nnajbolje ti je probati učitati sve što možeš u numpy, spremiti to, pa učitati u TF2 model, ne znam jel ijedan drukčiji konverter radi potpuno",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "vjeva"
        ]
      }
    },
    "157007": {
      "poster": "InCogNiTo124",
      "content": "https://efemarai.com/",
      "votes": {
        "upvoters": [
          "Koalalica (zaba)",
          "[deleted]",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "177702": {
      "poster": "InCogNiTo124",
      "content": "[In Japan, an A.I. system designed to distinguish croissants from bear claws has turned out to be capable of identifying cancer cells](https://www.newyorker.com/tech/annals-of-technology/the-pastry-ai-that-learned-to-fight-cancer)",
      "votes": {
        "upvoters": [
          "Hrvoje (Hrvoje45)",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "181182": {
      "poster": "PeroPerica",
      "content": "Evo ideja za deep learning fanove, napravite python software koji pomocu webcama prepoznaje dugorocne promjene sjedenja koje predstavljaju los polozaj tijela (bad posture). Vjerujem da bi bilo ultra korisno, ovo što postoji ne radi baš najbolje.",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "Ovo_je_moj_nick"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "181185": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@PeroPerica#181182 ill-conceived problem",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "181187": {
      "poster": "InCogNiTo124",
      "content": "@PeroPerica#181182 Sta fali dosadasnjih posture analysis toolovima",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "181199": {
      "poster": "PeroPerica",
      "content": "@InCogNiTo124#181187 Prije negdje godinu dana sam bio našao jedan koji više ne nađem, nije mogao imati više od jednog pravilnog držanja, na svaki pokret bi se triggerao, itd.\n\nNema baš open source rješenja.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "182359": {
      "poster": "InCogNiTo124",
      "content": "![](assets/2021-04-30/00003.jpeg)",
      "votes": {
        "upvoters": [
          "Ovo_je_moj_nick",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Amon",
          "Asura (Asura)",
          "Dekan",
          "Joji",
          "Katulich",
          "Lyras",
          "NekocBraca",
          "Svarog (Veles)",
          "indythedog",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "195013": {
      "poster": "[deleted]",
      "content": "@tolecnal Danas sam prvi put vidio malo vise matematicke teorije da se koristi u nekom deep learning paperu (a nije obskuran: MobileNetV2)\n\nhttps://arxiv.org/pdf/1801.04381.pdf \n\n![](assets/2021-05-23/00013.png)",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "195016": {
      "poster": "InCogNiTo124",
      "content": "@Delpins#195013 nes ti teorije haha\n\nSto je zapravo samo tuzno",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "195028": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@Delpins#195013 Ma ima tog stalno\n\nhttps://arxiv.org/pdf/2007.00811.pdf\n\novaj paper je čak i bullshit malo pa su se raspisali o teoremima",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "195057": {
      "poster": "[deleted]",
      "content": "@Delpins#195013 Skimmao sam samo kroz paper i ovako na prvu mi se čini da su stavili to tek toliko da imaju dio Theorem 1 xd. Možda sam u krivu, u stvari se nadam da sam u krivu.\n\nBilo bi lijepo kada bi ih taj teorijski dio usmjerio, npr. prema boljoj arhitekturi , odnosno općenito dao usmjerenje kako pristupiti eksperimentalnom dijelu cijelog problema, ali to su već, naravno, idealizirane situacije namijenjene laboratorijskim vježbama xd.\n\nDobro su to opisali u https://arxiv.org/abs/1807.03341 - tldr: revieweri često znaju srat da nema dovoljno teorije pa moraš nešto nasrat koliko god beskorisno bilo xd (to je samo jedna od četiri problematične točke koje opisuju, ako se ne varam).\n\nIskreno, kod čistog eksperimentalnog ML-a jako mi se sviđa pristup koji je opisao prof. Weinberger u https://slideslive.com/38938218/the-importance-of-deconstruction . Izvrstan talk (mislim da sam ga već bio linkao). tldr - potrebno je napraviti ablation study i tek onda znaš na čemu si i što je u stvari donijelo najveće gain-ove u performancama.",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "195537": {
      "poster": "InCogNiTo124",
      "content": "> @tolecnal#195057 potrebno je napraviti ablation study\n\nIstinabog ne citam puno bas papera, ali samo jedan, kojeg sam imao za seminar, je imao ablation study lol\n\nTo je bas poklon za bozic kad naletis na paper sa ablation study haha",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "195811": {
      "poster": "[deleted]",
      "content": "> @InCogNiTo124#195537 To je bas poklon za bozic kad naletis na paper sa ablation study\n\nSlažem se, pogotovo ako se uzme u obzir veličina današnjih modela.",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "196091": {
      "poster": "InCogNiTo124",
      "content": "Jednom tjedno dobim newsletter sa rezimeom tjedna kao, i dobim ove papere\n\n>! ![](assets/2021-05-25/00022.jpeg)\n\nSkoro svi su kinezi smh\n\nPitam se jel to do kvalitete ili kvantitete\n\nOntopic: ovo je fkt fora\n\nhttps://ai.facebook.com/blog/wav2vec-unsupervised-speech-recognition-without-supervision/",
      "votes": {
        "upvoters": [
          "Ovo_je_moj_nick",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "196100": {
      "poster": "[deleted]",
      "content": "> @InCogNiTo124#196091 Jednom tjedno dobim newsletter sa rezimeom tjedna kao, i dobim ove papere\n\nKoji newsletter?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "196102": {
      "poster": "InCogNiTo124",
      "content": "@Delpins#196100 https://alphasignal.ai/",
      "votes": {
        "upvoters": [
          "Erinon",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "197585": {
      "poster": "nnn (dinoo)",
      "content": "e nije deeplearning al me zanima nešto, ne razumijem se previše pa neka me netko ispravi:\n\nznači ako imam dataset sa servera, ima atribute tipa port (npr. 23223), proces koji ga je pokrenuo (npr. netcat), i label ako je taj port maliciozan, mogu li koristiti naivni bayesov klasifikator ili neki drugi klasifikator za detekciju ako je nešto maliciozno npr. backdoor ili slično?\n\nNaravno u podatcima za treniranje bi bile i namjerno pokrenuti backdoori tako se ima šta label-at.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "197605": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@nnn#197585 Možeš",
      "votes": {
        "upvoters": [
          "[deleted]",
          "nnn (dinoo)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "204359": {
      "poster": "tonkec",
      "content": "Jel možda netko kupio Google Colab Pro pa imao nekih problema jer Hrvatska nije u podržanim zemljama?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "204365": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@tonkec#204359 Kaže kolega koji se ne želi ulogirati da odgovori da sam treba unijeti američki zip",
      "votes": {
        "upvoters": [
          "ink",
          "tonkec"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "204370": {
      "poster": "njofra",
      "content": "@tonkec#204359 Meni zadnjih \\~3 mjeseca normalno funkcionira, samo sam stavio americku adresu kad sam placao. Koristio sam Revolut karticu, ali ni hrvatske banke ne bi trebale imati problema.",
      "votes": {
        "upvoters": [
          "tonkec"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "204392": {
      "poster": "tonkec",
      "content": "@micho#204365 Da, vidio sam to, ali me interesira jesu li nekome ukinuli pretplatu jer se ne nalazi u tom geografskom područuju što bi realno mogli.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "204438": {
      "poster": "mijauexe",
      "content": "Bok svima!\n\nPred kraj druge godine sam i mislim da sam ispodprosjecan student i po znanju i po inteligenciji, volja za radom mi je iznadprosjecna tho.\n\nPodrucje mi je zanimljivo ali i vrlo zastrasujuce(na prvi pogled)...no ipak bi htio upoznati to kroz zavrsni rad i kroz daljnje studiranje..\n\nMoze li mi netko preporuciti moje prve korake (vidio sam neke knjige koje ste gore spominjali), te ako ima vremena tu i tamo da mu postavim glupo pitanje, nek mi se javi u private pls.\n\nthanks!😊",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "204461": {
      "poster": "InCogNiTo124",
      "content": "@mijauexe#204438 svasta\n\nNadi neki beginner friendly yt playlist ili course. Ja sam npr gledo Machine Learning A-Z na udemyju al anything would do\n\nOd predznanja, mat1, mat2, vis, pogotovo statistika. Ne moras znat rjesit zadatke koliko moras kuzit koncepte i formule (tipa ak ti ja napisem P(C_i) • P(x|C_i) da odma znas kaje to). Puno vise moras znat baratat slovima neg rjesit zad haha\n\nOd materijala preporucam [Walpola](https://g.co/kgs/xZmxQP), [PRML](https://g.co/kgs/6N5pKK) (odi na libgen za pdfove) i [r/learnmachinelearning](https://www.reddit.com/r/learnmachinelearning). Walpole ima super uvod u vjerojatnost i statisticke testove, prml ima super objasnjeno all things strojno ucenje (al nemoj se prepast velicine knjiga ne moras ih cijele procitat haha)\n\nPraksa je puno bitnija od teorije. Odi na kaggle (stranica za data sci natjecanja) i uzmi neka jednostavna natjecanja i citaj kaj su ljudi radili u svojim rjesenjima.\n\nOnda kad skuzis otprilike, probaj sam rjesit neki problem analizom podataka. To moze bit [odavde](https://github.com/NirantK/awesome-project-ideas) il izmisli nest svoje ili rjesi na kaggleu, whatever. Prvi put ce ti bit zakurac, al bitan je da si u glavu, onak duboko u lizard brain, usadis taj iterativni pristup s koracima 1) analiziraj podatke 2) napravi model 3) analiziraj model 4) donesi zakljucak 5) GOTO 1 ili 2, ovisi kako kad.\n\nZa kraj, ovo su osobne preporuke i ne znaci da su bas najbolje. Stovise, nisam siguran da se isplati trazit apsolutno najbolje materijale. Toliko tog ima na internetu a sve je vrlo slicno tako da su sanse da ces uzet nesto bss lose relativno niske haha\n\nA ovo za private, nema potrebe, bolje pitaj javno pomoci ces jos nekom",
      "votes": {
        "upvoters": [
          "BreadCat",
          "[deleted]",
          "ink",
          "mijauexe",
          "steker"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "205049": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@tonkec#204392 Nisam nigdje našao iskustvo da jesu, ali po termsima imaju puno pravo",
      "votes": {
        "upvoters": [
          "tonkec"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "218121": {
      "poster": "[deleted]",
      "content": "https://distill.pub/2021/distill-hiatus/",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "InCogNiTo124"
        ]
      }
    },
    "223400": {
      "poster": "InCogNiTo124",
      "content": "Prvo je bio keras\n\nPa je doso tensorflow\n\nPa su napravili tf backend za keras\n\nPa su zivili skupa dost dugo\n\nPa je tensorflow 2 u sebe absorbiro keras\n\nA sad ga yeetaju opet nazad van?? https://github.com/tensorflow/tensorflow/releases/tag/v2.6.0-rc0",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "ink"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "223428": {
      "poster": "Ovo_je_moj_nick",
      "content": "Nemoj zaboravit jax",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "223429": {
      "poster": "InCogNiTo124",
      "content": "@Ovo_je_moj_nick#223428 Sto žnjime tocno?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "223433": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#223429 To što google polako deprecatea TF za JAX",
      "votes": {
        "upvoters": [
          "Ovo_je_moj_nick",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "223444": {
      "poster": "InCogNiTo124",
      "content": "@micho#223433 Pa okej\n\nNe vidim taj jax ko neki widespread product",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "223459": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#223444 Kad je nova stvar, ali ako se interno shifta na JAX za očekivati je da je TF mrtav kroz 5 godina, TF bez googleove podrške nema šanse opstati hahaha",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "223798": {
      "poster": "Ovo_je_moj_nick",
      "content": "i dalje mi je najveci kek cinjenica da ni jax ni tf ni keras ni nista guglovo nema podrsku instaliranja cuda i ostalih prek python environmenta nego moras systemski instalirat odgovarajuci CUDA za odgovarujuci paket za 3 pm. A toliko toga je napisano da se sakrije od korisnika da nemres ni skuzit da nekaj ne stima under the hood, jednostavno moras prihvatit da je tako kako je",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "NekocBraca",
          "Svarog (Veles)",
          "[deleted]",
          "nnn (dinoo)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "223926": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@Ovo_je_moj_nick#223798 Zašto bi imali, to je dependency koji je dosta odvojen od samih libova",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "224298": {
      "poster": "Ovo_je_moj_nick",
      "content": "@micho#223926 Kad moras na istoj mashini hendlat par projekata odjednom postaje vrlo komplicirano",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "224308": {
      "poster": "InCogNiTo124",
      "content": "@Ovo_je_moj_nick#224298 Svako nek developa u svom dockeru",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "NekocBraca",
          "ink"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "224310": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@Ovo_je_moj_nick#224298 I zato postoji docker",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "NekocBraca"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "224312": {
      "poster": "[deleted]",
      "content": "@Ovo_je_moj_nick#224298 Zar nije CUDA na razini sustava pa ne mozes imati vise razlicitih verzija odjednom?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "224315": {
      "poster": "Ovo_je_moj_nick",
      "content": "@Delpins#224312 Mozes imat pytorch s cuda v 10.1 i vrtit na sistemu koji ima CUDA 11.2 instaliran sistemski (ako me to pitas).\n\nA kaj se tice dokera, to mi je jos jedan extra layer na koji vecinu colaboratora nemrem ni nagovorit. Vise manje, doker je ok kad si gotov s devom (po meni) i zelis spremit sve u fixed verziju koju mozes onda pokrenut jednostavno kasnije bez dodatnog mucenja s verzijama i instalacijom pipa i istalog pogotovo s verzijama koje su notsuported/removec/renamed.\n\nDrugo, corp proxy settings i koristenje dockera je muka isusova, 3 od 4 rade kak spada, sve postavljeno kako spada ali odjednom jedan od njih jednostavno ne radi, nece instalirat nista jer proxy setting \n\nMozda sam u krivu, nemam previse (dobrog) iskustva",
      "votes": {
        "upvoters": [
          "NekocBraca",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "224323": {
      "poster": "InCogNiTo124",
      "content": "@Ovo_je_moj_nick#224315 Ne znam nama je to okej radilo, na hostu se drzala cuda lastest ili skoro latest, docker je imo svako svoju userland komponentu koja mu je trebala i nikakvih problema nije bilo\n\nEdit: uostalom predobro je ak si sjebes env nisi cijelu masinu pa mos sam ispocetka hehe a kod si stavis ko volume",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "224325": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@Delpins#224312 Možeš imati koliko god verzija hoćeš, sam se treba pozabavit s pathovima hehe",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "224328": {
      "poster": "Ovo_je_moj_nick",
      "content": "@InCogNiTo124#224323 Mislim, mi recimo imamo par projekata izmedju razlicitih timova, sve je na gitu, ne vidim preveliku prednost da sad slazem docker ako moram izvrtit nekaj kaj su oni devali ili nekaj kaj bi trebalo promijenit. Virt env na mashini radi sasvim ok, svaki ima svoje instalacije, ne moram selit simo tamo docker image od par GBa jer ima sav dependency instaliran. Nekako mi je nezgrapno raditi dev na razlicitim mjestima izmedju razlicitih timova preko dockera",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "225648": {
      "poster": "Jokerrr",
      "content": "Ekipa, kolko je razvijeno ovo podrucje u hrvatskoj industriji? Skoro svi oglasi za posao su webdev i mobile dev. Ima li koja firma sa pravom deep learning engineer pozicijom?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "225650": {
      "poster": "InCogNiTo124",
      "content": "@Jokerrr#225648 Cisti research dl tesko van faksa, ali u zg ima nekoliko applied dl firmi. Realnetworks, gideon brothers, visage technologies su neke od CV firmi, za nlp znam samo real al ima ih jos\n\nNaravno to sve jer datasci != DL",
      "votes": {
        "upvoters": [
          "Jokerrr",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "225682": {
      "poster": "Lyras",
      "content": "@InCogNiTo124#225650 DoXray je isto za NLP, meni su ponudili 40kn/h, ali nisam otišao kod njih tako da ne znam kakvi su.",
      "votes": {
        "upvoters": [
          "Jokerrr",
          "[deleted]",
          "ink"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "226031": {
      "poster": "[deleted]",
      "content": "Za nekoga tko se nikad nije bavio NLP-om što preporučujute za početak.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "226036": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@Delpins#226031 LSTM sentiment analysis -> LSTM CNN sentiment analysis -> Transformer sentiment analysis -> BERT sentiment analysis -> BERT quantization -> TinyBERT distillation -> GPT2 generation -> T5 generation -> T5 sentiment analysis -> T5 sentiment distilacija na BERT\n\nI to je to više manje\n\nUzmeš što manje modele, Huggingface, i sve se da zavrtiti i na 1060",
      "votes": {
        "upvoters": [
          "[deleted]",
          "[deleted]",
          "ink"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "226052": {
      "poster": "InCogNiTo124",
      "content": "@micho#226036 Previse distilacije za moj ukuus",
      "votes": {
        "upvoters": [
          "AVRFreak"
        ],
        "downvoters": [
          "[deleted]"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "230497": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "lmao\n\nhttps://www.reddit.com/r/MachineLearning/comments/p59pzp/d_imitation_is_the_sincerest_form_of_flattery/",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "[deleted]"
        ]
      },
      "reactions": {
        "haha": [
          "InCogNiTo124"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "231768": {
      "poster": "[deleted]",
      "content": "https://nn.labml.ai/",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "Ovo_je_moj_nick",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
          "mikimoj",
          "miss_anthropocene (neunist.iva)",
          "tonkec"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "232919": {
      "poster": "InCogNiTo124",
      "content": "Deep learning nad grafovima uspio smanjit ETA gresku i do 40% u produkciju na google mapsama\n\nBrought to ypu by deep mind ekipa i gospodin doktor znanosti petar velickovic among others\n\nhttps://arxiv.org/abs/2108.11482\n\nTo me pocelo jako zanimat lately (graf neuronke) pa proucavam malo, ovo je freshest of the fresh",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "232940": {
      "poster": "[deleted]",
      "content": "> @InCogNiTo124#232919 Brought to ypu by deep mind ekipa i gospodin doktor znanosti petar velickovic among others\n\nMi smo obični smrtnici prema toj ekipi. :(",
      "votes": {
        "upvoters": [
          "ink"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "233091": {
      "poster": "[deleted]",
      "content": "https://www.reddit.com/r/MachineLearning/comments/pdwxxz/d_colab_pro_no_longer_gives_you_a_v100_not_even_a/",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "233092": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@Delpins#233091 dost fake news IMO, ali ajde, lako je skužiti iz komentara",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": [
          "[deleted]",
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "233141": {
      "poster": "[deleted]",
      "content": "sto vi znate o ucenju u dubini?",
      "votes": {
        "upvoters": [
          "AVRFreak",
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)",
          "kix7 (Fish99)",
          "spampers (majmunska boginja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124",
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)",
          "ink",
          "kix7 (Fish99)",
          "spampers (majmunska boginja)",
          "steker"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "235180": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "Novi shitpost od herr Schmidhubera\n\nhttps://people.idsia.ch/~juergen/most-cited-neural-nets.html",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": [
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "235236": {
      "poster": "InCogNiTo124",
      "content": "@micho#235180 inb4 schmidthuber je srbin",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "235237": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#235236 da\n\n![](assets/2021-09-08/00025.png)\n\n![](assets/2021-09-08/00026.png)",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": [
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "235578": {
      "poster": "[deleted]",
      "content": "https://twitter.com/egrefen/status/1435700593138487299",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Amon",
          "Dekan",
          "InCogNiTo124",
          "Kushim",
          "Lyras",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "236205": {
      "poster": "InCogNiTo124",
      "content": "nemrem nac odg na ovo pitanje googleanjem\n\nkad se trenira model za binarnu klasifikaciju, zasto se cesto koristi softmax umjesto sigmoide? nema mi to smisla uopce, a to sam vidio na masu mjesta.\n\nmeni je puno logicnije puknut sigmoidu na jedan logit, nego racunat dva logita, pa softmax, jer ovaj drugi ionak ispadne 1-prvi, a ustedili smo malo komputacije",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "236209": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#236205 Nema ti razlike, softmax je višedimenzionalna sigmoida. Ali ako imaš klasifikaciju s 2 logita i softmaxom, umjesto 1 + sigmoida, to je veća moć zaključivanja, jer možeš lakše modelirati nesigurnost (kod sigmoide je nesigurnost oko 0.5, kod 2 class softmaxa je to u blizini svih uređenih parova [imath](x, x), x \\in \\left[0, 1\\right][/imath]).\n\nNesigurnost kod sigmoide je 0D, a kod softmaxa 1D, ili 1D vs 2D, ako gledaš kao neki raspon, bitno je da softmaxom vidiš barem dimenziju više na taj način.",
      "votes": {
        "upvoters": [
          "Ovo_je_moj_nick",
          "[deleted]"
        ],
        "downvoters": [
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "236211": {
      "poster": "InCogNiTo124",
      "content": "@micho#236209 Bzvz lol\n\nJa msm da cu ja nastavit svojim modelima gurat torch.sigm",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "236214": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#236211 Well da sam na tvojem mjestu provjerio bih stvari ali moguće da ti je softmax i brži od sigmoide hahah",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "236218": {
      "poster": "InCogNiTo124",
      "content": "@micho#236214 Jesi ti to upravo \"citation needed\" mene bahaha",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "236223": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#236218 Ma ne, just talking bzvz",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": [
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "236451": {
      "poster": "Erinon",
      "content": "Neda mi se sad računat i provjeravat ovo što ću reći, ali mislim da je softmax stabilniji od sigmoide. I ja sam prije par mjeseci naišao na to da ljudi koriste softmax (uvijek sam koristio sigmoidu do tad) pa mi se čini da je stabilnost bio razlog. Uz to, naravno vrijedi i ovo već rečeno da imaš veću moć zaključivanja. Također sam i isprobao to na dosta teškom i neizbalansiranom datasetu i meni je bilo dosta stabilnije sa softmaxom, ali eto, možda slučajno.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "236453": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@Erinon#236451 Isto je stabilna sigmoida i softmax, oboje imaju e^x u nazivniku i brojniku, s tim da sigmoida može imati i samo u nazivniku (ali tu je nejasno jel to pomaže ili ne 🤔)\n\nJedino može biti drukčije ako su drukčije implementirane al npr. u PyTorchu su te eksponencijale isto implementirane, pa bi moralo biti i isto stabilno",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "[deleted]",
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "236537": {
      "poster": "InCogNiTo124",
      "content": "@Erinon#236451 Mislim da sam skuzio\n\nS obzirom da je softmax invarijatan na pomak, odnosno bitan je samo realtivni odnos logita, onda model moze ili predvidat oba logita relativno visoko ili relativno nisko, sto daje stupanj slobode. Zato mu je to lakse nauciti\n\nSa sigmoidom je fucced jer mora tocno pogodit blizu jednog broja koji ce se pretvorit u posteriori distribuciju\n\nMnogo zajebato, a glupo",
      "votes": {
        "upvoters": [
          "Dekan",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "237261": {
      "poster": "InCogNiTo124",
      "content": "@micho https://medium.com/@hamishogilvy/vectors-are-over-hashes-are-the-future-of-ai-98c4dc33d8ee",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "237263": {
      "poster": "[deleted]",
      "content": "@InCogNiTo124#237261 Na mediumu barem dvaput tjedno vidim članak tipa _X is the new future_, _Y is over, X is the future_, itd, i to po svim mogućim područjima.",
      "votes": {
        "upvoters": [
          "Ovo_je_moj_nick"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "237265": {
      "poster": "InCogNiTo124",
      "content": "@InCogNiTo124#237261 U jbt otkad studosi imaju medium integraciju",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "237266": {
      "poster": "InCogNiTo124",
      "content": "@Delpins#237263 Al ovaj je ledžitan",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "237281": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#237265 oduvek",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": [
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "237284": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#237261 autor teksta je preglup da shvati da su embeddinzi zapravo LSH tehnika, ali dobro, šta je tu je, da sam htio pročitati nešto pametno ne bih otvarao medium link",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]",
          "iLabaviUmeDaZabavi (prvi do boga)"
        ],
        "downvoters": [
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "237286": {
      "poster": "InCogNiTo124",
      "content": "@micho#237284 E taj komentar sam cekao xd",
      "votes": {
        "upvoters": [
          "ink"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "237495": {
      "poster": "[deleted]",
      "content": "@micho#237284 \n\nNa dubokom kolega je spomenuo da je pročitao neku \"činjenicu\" , na TowardsDataScience blogu, vezanu za optimizacijske algoritme koji se koriste u dubokom ucenju te je trazio Segvica da malo prokomentira. Segvic mu je odgovorio: \"Nemojte citati te blogove. Da ti ljudi nešto znaju pisali bi radove.\"",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "Kushim",
          "Lyras",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "BreadCat",
          "Dekan",
          "Erinon",
          "Haki",
          "InCogNiTo124",
          "Kushim",
          "Lyras",
          "Svarog (Veles)",
          "iLabaviUmeDaZabavi (prvi do boga)",
          "ink",
          "login",
          "matt (Matt)",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
          "spampers (majmunska boginja)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "237554": {
      "poster": "InCogNiTo124",
      "content": "[Photomath's Top Training Resources for AI and Machine Learning Skills] \n\nA common question we get from students interested in learning AI: “What resources would you recommend for getting started in AI and ML?”\n\nIt’s a great question - and one that’s worth a thoughtful answer! It’s easy to get overwhelmed by all the resources at your disposal, so for those curious beginners just looking for a place to start, we’re happy to share some of the resources that were instrumental in our own journey as ML Engineers at Photomath.\n\nFind the learning method that works for you and jump-start your Machine Learning Adventures: https://photomath.com/presskit/ML_Engineer_Learning_resources.pdf\n\nFeel free to share this free training resources with your network!",
      "votes": {
        "upvoters": [
          "AromaticConfusion (VrloZbunjen)",
          "Asura (Asura)",
          "JBear",
          "Kushim",
          "kix7 (Fish99)",
          "mikimoj"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "238068": {
      "poster": "InCogNiTo124",
      "content": "Ideja: skrejpat linkedin i natrenirat model da pise inspirativne postove i vidit koliko ce ljudima trebat da skuze",
      "votes": {
        "upvoters": [
          "[deleted]",
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)",
          "ink"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "238069": {
      "poster": "Asura (Asura)",
      "content": "@InCogNiTo124#238068 \n\nPrvo napravit model koji predicta dal je inspirativan post kak' bi olaksali scrapeanje?",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "238071": {
      "poster": "InCogNiTo124",
      "content": "@Asura#238069 moze i to, al ne treba ti neki fensi, puknes linearnu regresiju na vektor reakcija + mozda neki embedding komentara ili zemlje komentatiora (indijci jako vole inspirativne postove)",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Asura (Asura)",
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)",
          "ink"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "238072": {
      "poster": "Asura (Asura)",
      "content": "@InCogNiTo124#238071 \n\nJa bih ti preporucio ovako nesta. Odes na twitter, scrapeas ljude koji imaju tipa life coach (to je ona druga domena NLP-a, neuro lingustical programming) u imenu i imas maltene sve inspirativne postove xD",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "238073": {
      "poster": "InCogNiTo124",
      "content": "@Asura#238072 e al linkedin ima onak citave fabule, twitter bi mi bio samo neki unsupervised djelic",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "238074": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#238073 Tvoja mreža ionak ne može uzeti kao input te čitave fabule; 12 GB VRAMa ti je dosta za oko  batch size 2 od 230 char max",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "[deleted]"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "InCogNiTo124",
          "ink"
        ]
      }
    },
    "238076": {
      "poster": "InCogNiTo124",
      "content": "@micho#238074 onda, u redu, moze kombajnani dataset",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "238078": {
      "poster": "InCogNiTo124",
      "content": "> @micho#238074 batch size 2 od 230 char max\n\ncek zasto char level embedding a ne samo word level",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "238082": {
      "poster": "Asura (Asura)",
      "content": "@InCogNiTo124#238078 \n\nMa nije mislio na char lvl, subword level je, ali i s njim ti nece bas puno teksta stati",
      "votes": {
        "upvoters": [
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "238240": {
      "poster": "InCogNiTo124",
      "content": "[Torch.manual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision](https://arxiv.org/abs/2109.08203)\n\n> In this paper I investigate the effect of random seed selection on the accuracy when using popular deep learning architectures for computer vision. I scan a large amount of seeds (up to 104) on CIFAR 10 and I also scan fewer seeds on Imagenet using pre-trained models to investigate large scale datasets. The conclusions are that even if the variance is not very large, it is surprisingly easy to find an outlier that performs much better or much worse than the average.",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "238552": {
      "poster": "InCogNiTo124",
      "content": "razmisljam si auxillary lossu za neki problem\n\nrecimo model pljuje van N vektora logita, te uz neku mjeru slicnosti tipa cross entropy prema targetima, zelite da jos uz to recimo susjedni logiti budu slicni (ono x[i-1], x te x[i+1], njihova udaljenost bi kao trebalo smanjit). koje sve nacine mozete smislit?\n\npalo mi je napamet kao KL izmedu dva susjedna vektora nakon softmaxa al ta mjera nije simetricna, s druge strane L2 loss pretpostavlja istu skalu.\n\nmozda cosine? ili nekaj novog?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "238568": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#238552 Why\n\nSamo napiši custom konvolucijski kernel, fiksiraj mu težine, i minimiziraj iznos toga. CUDA sat vremena posla",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "[deleted]",
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "238569": {
      "poster": "InCogNiTo124",
      "content": "> @micho#238568 Why\n\npalo mi na pamet\n\ni nisi provajdao rjesenje",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "238575": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#238569 Je, neću ti ja napisati rješenje XD\n\nugl. napravio bi kernel takav da npr. uzmeš ulaz i umjesto da vratiš sumu umnožaka njega i težina, kao težine postaviš ulaz shiftan za jedno mjesto u desno ili lijevo te vratiš sumu apsolutne razlike. Minimiziraš to i GG\n\n\nAjmo reći da bi kod za to bio\n\n```python\ndef conv(a, kernel_length):\n    return sum(\n        sum(\n            abs(a[j] - a[j + 1]) for j in range(i, i + kernel_length)\n        ) for i in range(len(a) - kernel_length + 1)\n    )\n```",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "[deleted]",
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "238577": {
      "poster": "InCogNiTo124",
      "content": "@micho#238575 nisi mi shvatio haha ti pricas o implementaciji u cudi, a ja pitam ovak poblem-solving konceptualno, na koje sve nacine mogu mjerit razlicitost dva vektora a da to ima smisla ako znamo da nisu random vektori vec vjerojatnostni logiti\n\nas I said:\n\n> @InCogNiTo124#238552 palo mi je napamet kao KL izmedu dva susjedna vektora nakon softmaxa al ta mjera nije simetricna, s druge strane L2 loss pretpostavlja istu skalu.\n\ntrazim javni brain storming session\n\ntipa ovo kaj si ti napiso je L1 razlika, takve odgovore trazim",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "238578": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@InCogNiTo124#238577 E pa, stavijo sam ti rešenje za ovo što si spominjao, a različitost se svakako može mjeriti, ovisi što bi ti bio sličan vektor. Kao, iz iste distribucije?\n\nJer ak se radi o 2 vektora najbolje samo cosine i gotovo, možeš pokušati izmišljati toplu vodu za neke specifične probleme, al ovako da nemaš pojma što se događa cosine radi ko bog hehe\n\nOno što bi mi možda palo na pamet je da gledaš sličnost diferencijala - npr.\n\nImaš `1 2 3 4 5` i `1 2 1 4 5`.\n\nTada vektori diferencijala glase `1 1 1 1` i `1 -1 3 1`, pa lupiš cosine na to. Al tak možeš izmišljat finte unedogled.",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": [
          "[deleted]",
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "239928": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "Pozdrav wizardi,\n\nkoliko su relevantna starija izdanja ML knjiga? Konkretno, jel Introduction to Machine Learning, Ethem Alpaydin, 2nd edition, 2010 ok il zastarjela? Pretpostavljam da je ok jer su tamo relativno stari koncept, al eto, da cujem vasa misljenja",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "239930": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@bodilyfluids#239928 Ak misliš za klasični ML, onda je okej, ak misliš za DL više ni Deep Learning Book iz 2015 nije toliko relevantan",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": [
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "239935": {
      "poster": "InCogNiTo124",
      "content": "@bodilyfluids#239928 imas moj review te knjige ovdje @InCogNiTo124#200916  ugl relevantna je mada nije bas za ucit i ima puno netrivijalne matematike. al super je ko referenca\n\nradije si uzmi ISLR 2nd edition ili prodi https://strojnoucenje.takelab.fer.hr/",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "239940": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@micho#239930 pretpostavljam da se svejedno isplati upoznati sa starijim metodama DL-a?\n\n@InCogNiTo124#239935  ty, jel takelab playlista dio kolegija struce 1?",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "239941": {
      "poster": "InCogNiTo124",
      "content": "@bodilyfluids#239940 Trebalo bi bit, bar prvih 70%",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "239942": {
      "poster": "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)",
      "content": "djeco ostavite se ovih izmišljenihsranja. dođite na tražene poslove raditi, fali nam frontendaša",
      "votes": {
        "upvoters": [
          "Emma63194",
          "[deleted]",
          "ink",
          "kix7 (Fish99)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "239945": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "> @bodilyfluids#239940 starijim metodama DL-a?\n\nNope, jedino gradivnim blokovima. Starije metode u DL-u su gotovo neupotrebljive. ML vječno radi, ali to je zato što za ML imaš puno veća ograničenja kakve podatke možeš dat i što očekuješ.\n\nDL je više manje samo \"Imam ovakve podatke, želim da se ovako shvate/označe/prevedu, smisli neku funkciju za to\". I onda je to brljanje i izmišljanje, ne neka čvrsta teorija i to traje dok netko ne izmisli nešto (trunkicu) bolje, a nakon toga je useless.",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": [
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "239947": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@micho#239945\n\n> Starije metode u DL-u su gotovo neupotrebljive.\n\nOnda se DL uci preko novih papera?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "239948": {
      "poster": "InCogNiTo124",
      "content": "@bodilyfluids#239947 U zadnje vrijeme gotovo iskljucivo tako, iako svako malo imas neki veliki sazetak podrucja\n\nJa npr jako volim https://d2l.ai",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "239949": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@bodilyfluids#239947 Ako te zanima SOTA, više manje da, visiš na Twitteru, gledaš konferencije, arxiv sanity itd.\n\nAko te zanima primjena postojećeg, onda malo gledaš Git repoe, šta je fora. Za NLP sam pratiš commitove na huggingface transformers više manje. Za CV nemam pojma jel ima neka tako sveopsežna knjižnica di bi mogao dobiti friške standardizirane implementacije. Za reinforcement learning imaš OpenAI i navodno neki lik sad oživljava Gym na githubu, ali nisam upućen u taj dio struke",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": [
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "239950": {
      "poster": "InCogNiTo124",
      "content": "> @micho#239949 Za CV nemam pojma jel ima neka tako sveopsežni library di bi mogao dobiti friške standardizirane implementacije.\n\nMsm da takvog ni nema honestly, torch ima neki svoj torchvision al to je vise utils nego nesto sveobuhvatno, opencv je jedno vrijeme imo inicijativu bit to pa nitko nije htio to dvoje mjesat, a i ajmo rec da se podrucje blago zasitilo, conve-bn-relu i reziduali go a long way nemas tu kaj vise",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "239953": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "> @InCogNiTo124#239950 conve-bn-relu i reziduali go a long way nemas tu kaj vise\n\nšta pa transformeri su sad aktualni, konve i reziduali su totalno u kurcu",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "239955": {
      "poster": "InCogNiTo124",
      "content": "@micho#239953 Well yeah but actually no, performance/compute ratio je ipak na strani dovrih starih cbrovki haha",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "243494": {
      "poster": "login",
      "content": "Racunalni vidasi imam jedno pitanje, koliko se C++ koristi u racunalnom vidu? Citam po netu i ima ljudi koji ga dosta preporucuju pa me zanima jel ima neko iskustva s tim. Znam da se C++ koristi ispod haube u pythonu al me zanima bas neka konkretna implementacija, sta i kolko se radi s njim",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "243495": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"login\"#p243494 To ti se koristi za finalne korake, kad prebacuješ na neki uređaj, npr.\n\nJer ovi PyTorch i Tensorflow ti ili ne rade, ili loše rade OOTB na npr. mobitelima ili nativnim aplikacijama. Dakle ako ne radiš callove na neki server, onda ćeš imati komplikaciju i onda to prebacuješ u visokooptimizirani C++.\n\n@\"Emilia\"#4 i @\"InCogNiTo124\"#999 će ti moći puno više reći o svemu tome.",
      "votes": {
        "upvoters": [
          "[deleted]",
          "login"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "245763": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"InCogNiTo124\"#p245750 Nemoj ovo reći Schmidhuberu",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "246148": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "Jel imate preporuke matematičke literature relevantne za područje? \n\nLinearna algebra, ViS i to",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "246150": {
      "poster": "InCogNiTo124",
      "content": "@\"Precious Bodily Fluids\"#p246148 ako odes na http://www.zemris.fer.hr/~ssegvic/du/ vidit ces dva fora linka dole\n\nlinearna: https://www.math.ucla.edu/~tao/resource/general/115a.3.02f/\n\nmatricni kalkulus: https://atmos.uw.edu/~dennis/MatrixCalculus.pdf\n\noboje skupa https://explained.ai/matrix-calculus/index.html",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "246195": {
      "poster": "[deleted]",
      "content": "@\"Precious Bodily Fluids\"#p246148 \n\nZa Linearnu ti ovisi radi li se o tome da se po prvi put susrećeš s linearnom ili ne. Ako se radi o prvom susretu s linearnom, onda je cilj gledati objekte kroz geometriju i steći tu geomterijsku interpretaciju. Ako se radi o drugom susretu s linearnom, onda se cilja na uvođenje novih matematičkih objekata i transfromacija nad njima - vektori kao članovi vektorskih prostora (apstrakcija za promatranje objekata koji se mogu zbrajati i množiti skalarom), linearne operatore kao transformacije između vektorskih prostora i njihova svojstva.\n\nZa prvi susret s linearnom:\n1. Linearna algebra, FER3 program preddiplomski studij\n2. Linear algebra, Gilbert Strang (nije knjiga, ali su predavanja fantastična)\n3. [Introduction to Applied Linear Algebra](http://vmls-book.stanford.edu/) - Fantastična knjiga, super zadaci za vježbu + postoje još i predavanja i knjiga je dostupna besplatno. Možda najbolji izbor za početak.\n4. [Interactive Linear Algebra](https://textbooks.math.gatech.edu/ila/) - Iskreno nisam čitao (malo sam skimmao samo), ali mi se jako sviđa sadržaj! Vrlo vjerojatno prođem kroz knjigu sljedeće godine.\n\nZa drugi susret s linearnom:\n1. Linearna algebra, FER2 program diplomski studij - postoji i knjiga, dobra je, slobodno preskočiš 5. poglavlje (iako ima zanimljivih stvari, npr. iterativni postupci za rj. sustava lin. jedn. ), ali je poglavlje dosta natrpano i isprepleteno s različitim drugim područjima matematike gdje se koristi LinAlg\n2. Linear Algebra Done Right - nisam još čitao, ali je u planu. Ljudi na internetu su poprilično napaljeni na knjigu, a i postoje službena predavanja od profesora koji je napisao knjigu i [predavanja od jednog drugog profesora](https://www.youtube.com/playlist?list=PLoxJTbDttvt7ny0WEJHWw6-0Sjx7EImIQ) . Bio sam malo prošao kroz obje playliste i više mi se sviđa ova druga. Zadaci su više-manje _proof-based_, ali je dobro to što se dosta lagano krene pa nekako i vježbaš dokaze (postoje neslužbena rješenja online).\n\nŠto se tiče vjerojatnosti, isto tako bi rekao da postoje dva pogleda. Jedan pogled, čisti matematički pogled, jest kroz na vjerojatnost kao mjeru (engl. _measure theory_). Odnosno da je vjerojatnost P mjera nad [imath](\\Omega , F)[/imath], gdje je [imath]\\Omega[/imath] prostor elementarnih događaja, a [imath]F[/imath] je [imath]\\sigma[/imath]-algebra nad [imath]\\Omega[/imath]. Uglavnom, da nešto krivo ne napišem jer i sam nisam još prošao kroz taj dio, ako nećeš ići na doktorat iz matematike ili teorijskog (nenadziranog) strojnog učenja onda ti ovakav pogled neće trebat. Da skratim, preporučam ti knjigu [Introduction to Probability, 2nd Edition](https://www.amazon.com/Introduction-Probability-2nd-Dimitri-Bertsekas/dp/188652923X). Super zadaci, postoje službena rješenja i službena video predavanja.\n\nKolegij Teorija vjerojatnosti (ViS nekoć), uvodio je vjerojatnost kao mjeru na vjerojatnostnim prostorom i sigma-algebrom, ali je to beskorisno i može se \"izbjeći.\"",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "indythedog",
          "ink"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "246207": {
      "poster": "InCogNiTo124",
      "content": "> @\"tolecnal\"#p246195 Linear Algebra Done Right\n\nJa sam ju probo citat, al honestly, _meh_. Odvec teoretska, a zadaci su Šnajederovski: odjednom moras znat povezat 7 razlicitih stvari iz poglavlja ucoravo naslijepo. Takoder puno se zadaci baziraju na dokazima, sto meni opet nije nesto. Al to su sve razlike iskljucivo u stilu ucenja i shvacanja svijeta oko sebe\n\nJa sam se pak odusevio s knjigom FCLA http://linear.pugetsound.edu/fcla (pise beta al takva je od 2016 tako da je najs) jer krece jos od sustava jednadzbi i polako uvodi sve koncepte, a dokazi u knjizi su znaci tako super lik nikad nije niti jedan korak preskocio. A i smjesna je nekad.\n\nPostoji i nastavak http://linear.pugetsound.edu/scla/html\n\nAgain, ovisi koji si tip ucenja, probaj jednu ak nejde probaj drugu i tak",
      "votes": {
        "upvoters": [
          "[deleted]",
          "ink"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "246212": {
      "poster": "[deleted]",
      "content": "@\"tolecnal\"#p246195 ViS na FER2 je uvodio vj. kao mjeru ... ali je to bilo toliko beskorisno i samo je zbunjivalo studente (barem mene).",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": [
          "[deleted]"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "247717": {
      "poster": "Jokerrr",
      "content": "Postoje negdje na internetu/youtubeu vizualizacije kako se mijenja input nakon prolaska kroz svaki sloj CNN mreze (naravno one s malim brojem slojeva)? Osobno mi treba ResNet al posto je to nastalo prije 5 godina sumnjam da postoje takvi materijali na webu.",
      "votes": {
        "upvoters": [
          "Kushim"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "247719": {
      "poster": "InCogNiTo124",
      "content": "@\"Jokerrr\"#p247717 dap\n\nhttps://cs.ryerson.ca/~aharley/vis/conv\n\nNazalost ne da da slozis svoju arhitekturu niti prikazuje reznete al super je za vizualizirat",
      "votes": {
        "upvoters": [
          "Jokerrr",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "250091": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Precious Bodily Fluids\"#p246148 Evo pitao sam i Šnajdija za preporuku mat literature, ovo je njegov recommend\n\nhttps://mml-book.github.io/",
      "votes": {
        "upvoters": [
          "[deleted]",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
          "rolotex (brr)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "252757": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "Pozdrav , imam projekt di se treba raditi s velikim datasetom, 5.1GB, al mi je problem što ne mogu s tim na leptop zbog nedostatka rama. \n\nE sad, ono što sam našao je bio Google Colab, stavio sam dataset na google drive i onda ga je colab od tamo učitavao. Ovo nije neka sreća jer mi se data učitava cca 30-ak minuta. \n\nImate li kakve preporuke?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "252763": {
      "poster": "InCogNiTo124",
      "content": "@\"Precious Bodily Fluids\"#p252757 nemoj cijeli dataset ucitavat",
      "votes": {
        "upvoters": [
          "Jokerrr",
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "252768": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"InCogNiTo124\"#p252763 \"drops mic\"",
      "votes": {
        "upvoters": [
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Jokerrr",
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "252772": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Precious Bodily Fluids\"#p252757 Razdvoji ga na manje dijelove, osobno ih razdvajam na max 100 MB.",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "Ovo_je_moj_nick",
          "[deleted]"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "252775": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"MićoTechTips\"#p252772 Oke, a onda kad racunas neke operacije koje zahtjevaju da prodes po svemu, kako oblikujes kod? oprosti, novi sam ovdje 🤓",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "252780": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Precious Bodily Fluids\"#p252775 Jednom kad prođeš jedan komadić dataseta, unloadaš ga iz memorije i učitaš drugi. Ako bi iz nekog razloga trebao cijeli dataset odjednom, onda ćeš to morati nekako kompresirati ili naći računalo s više RAMa.",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "Ovo_je_moj_nick"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "252782": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"MićoTechTips\"#p252780 Ty, a kada bi zapravo trbao cijeli dataset odjednom u memoriji? ne pada mi na pamet use case",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "252787": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Precious Bodily Fluids\"#p252782 Pa zapravo ne znam ni ja kad ga ne bi mogao na drugi način učitati. Najbliže što ti se može dogoditi je da radiš neko klasteriranje, onda je to teško raditi na lokalnoj razini. Ali u tom slučaju ćeš definitivno trebati ili više RAMa ili pametniju metodu, a i LSH postoji i prilično je dobar u takvim scenarijima, a on se skalira samo podlinearno, s tim se da živjeti.",
      "votes": {
        "upvoters": [
          "[deleted]",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "252792": {
      "poster": "[deleted]",
      "content": "@\"Precious Bodily Fluids\"#p252768 Povećaj swap, tj. prepusti OS-u da se bavi prebacivanjem dataseta s diska u RAM i nazad, i nadaj se najboljem.",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "Ovo_je_moj_nick"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "252894": {
      "poster": "Ovo_je_moj_nick",
      "content": "@\"Precious Bodily Fluids\"#p252782 Guglaj train CNN with batches",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "253212": {
      "poster": "Mariosss77 (Trevor)",
      "content": "@\"Precious Bodily Fluids\"#p252757 Pogledaj [HuggingFace Datasets](https://github.com/huggingface/datasets).\n\nNisi spomenuo u kojem formatu imaš podatke. Trenutno podržani formati su `csv`, `json`, `parquet`, `pandas` i `text`. Ako tvoji podaci nisu ni u jednom od tih formata, možeš napisati svoju loading skriptu (ima primjer u dokumentaciji). Učitani datasetovi nikad nisu cijelu u memoriji osim ako to ne zatražiš (operacije nad podacima direktno u RAM-u su naravno brže), već se dohvaćaju potrebni chunkovi iz datoteka (projekt se oslanja na PyArrow koji omogućava tzv. memory mapping). Isto, jedan nice feature je da se rezultati svake transformacije automatski cachiraju, ali o tome ima više u dokumentaciji.",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "254906": {
      "poster": "InCogNiTo124",
      "content": "@\"Precious Bodily Fluids\"#p252757 \n\n![](assets/2021-11-17/00009.jpg)",
      "votes": {
        "upvoters": [
          "Dekan",
          "[deleted]",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "ink"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "AVRFreak",
          "Amon",
          "Dekan",
          "Jokerrr",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "ink",
          "spampers (majmunska boginja)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "256901": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"MrPeanutButter\"#p256893 RZ regresija nibbas be like\n\n![](assets/2021-11-23/00021.jpg)",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "262663": {
      "poster": "Rene",
      "content": "Može mala pomoć od nekog iskusnijeg, pokušavam napravit neki jednostavni handwritten character detection CNN, i na testiranju ima super točnost (dataset s kagglea, preko 99%). Kad krenem slikat na papiru svoj rukopis (i to bas jako jako citko i razdvojeni characteri), onda dosta lose radi. Pretpostavljam da je problem u resizeanju slika na 28x28, ali gledao sam kako izgledaju u usporedbi s ovima iz dataseta i ne vidim neku veliku razliku",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "262665": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Rene\"#p262663 nisam iskusan, ali možda treba prvo obraditi tvoje fotke, npr pretvoriti ih u crno bijelu boju, bez nijansi sive, ako je takav i dataset",
      "votes": {
        "upvoters": [
          "Rene"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "262669": {
      "poster": "InCogNiTo124",
      "content": "@\"Rene\"#p262663 \n- 99% tocnosti, koja je to metrika? accuracy? Na kojem datasetu? Ako je MNIST, to je sandbox dataset, uzmi veci jaci bolji\n- jel model treniran samo sa train setom? jesu li train i test (i val) disjunktni\n- resize moze biti problem, da, ali najvj nije s obzirom da je neka metrika visoka cak 99%\n- @\"Precious Bodily Fluids\"#p262665 ima dobar insight. mozda je dataset pun binarnih slika a ti modelu feedas 256 razina sive ili boju. teorija kaosa nalaze da output mora otic u picku materinu. pozadina svakog znaka zapravo puno utjece na konacnu odluku (ne bi trebalo, but hey)\n- imas li dobar preprocessing? dakle normalizacija, oduzimanje srednjeg pixela, varijanca, imagenet mean i std\n- imas li mozda slucajno upaljene neke augmentacije in inference time poput flipanja, zasumljivanja gausom, afinih transformacija, 5crop, 10crop\n- mozda jednostavno nisi dovoljno dugo trenirao model xd",
      "votes": {
        "upvoters": [
          "BreadCat",
          "Emma63194",
          "Rene",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "262670": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Rene\"#p262663 Prenaučio si model (u odnosu na skup na kojem testiraš)\n\nIsto tako vjv trebaš napraviti neku normalizaciju uzoraka jer su male šanse da su distribucije skupa za treniranja i tvojih slika iste. Za početak najbolje je samo napucati kontrast slike na maksimum, s obzirom na to da ćeš time dobiti isti spektar boja i potencijalno eliminirati većinu šuma u slici.",
      "votes": {
        "upvoters": [
          "BreadCat",
          "Rene"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "262674": {
      "poster": "Rene",
      "content": "@\"Precious Bodily Fluids\"#p262665 \n\n@\"InCogNiTo124\"#p262669 \n\n@\"Mićo Maco\"#p262670 \n\nHvala svima na odgovorima, al evo:\n\nJe MNIST + dodatnih 30000 slika operatora,al s obzirom na upotrebu mislim da je sasvim dovoljan.\n\nNa svemu sam koristio threshold za nabit kontrast i normalizirao.\n\nModel je treniran samo s train, lol.\n\nOvo s oduzimanjem srednjeg pixela, varijance i ostalo sto si nabrojao nisam radio jer prvi put dabbleam s OpenCVom, nemam pojma koja je dobra praksa\n\nNemam augmentacije, valjda",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "262676": {
      "poster": "InCogNiTo124",
      "content": "> @\"Rene\"#p262674 Na svemu sam koristio threshold za nabit kontrast i normalizirao.\n\nthreshold u smislu binarizirao si sliku? a normalizacija znaci samo 0 ili 1?\n\ni koji model koristis? ako si skinuo s interneta, provjeri koje normalizacije on koristi pa to skopiraj\n\n> @\"Rene\"#p262674 prvi put dabbleam s OpenCVom, nemam pojma koja je dobra praksa\n\ncek jel koristis duboko ucenje ili ne?",
      "votes": {
        "upvoters": [
          "Rene"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "262683": {
      "poster": "InCogNiTo124",
      "content": "@\"Rene\"#p262663 ako binariziras sliku in inference time mozda dobijes artefakte koji bune model. probaj vizualizirat ono sta dobijes nakon normalizacije\n\nworkflow bi ti trebao biti detekcija znaka -> crop -> grayscale -> (thresholding sa K € [0, 255] pa scaling) ili (scaling pa thresholding sa K € [0, 1]) -> resize  -> VIZUALIZACIJA HERE -> feed into model",
      "votes": {
        "upvoters": [
          "Rene",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "262684": {
      "poster": "Rene",
      "content": "@\"InCogNiTo124\"#p262683 E vidis ja sam prvo resizeao pa onda ovo ostalo, jel to problem?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "262685": {
      "poster": "InCogNiTo124",
      "content": "@\"Rene\"#p262684 ako si resizeao na 28x28 pa onda traziio znakove, da to je jako krivo\n\nako si nasao znak pa resizeao, mislim da bi cak moglo biti i okej\n\nobicno se resize radi medu zadnjih operacijama jer zelis ostalo radit nad ajmorec \"sirovim\" podacima dok resize nuzno unosi neke artefakte jer mora ili halucinirat piksele kojih nema, ili disregardat visak piksela",
      "votes": {
        "upvoters": [
          "Rene"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "262693": {
      "poster": "InCogNiTo124",
      "content": "@\"Rene\"#p262684 svejedno probaj bez thresholdanja vec direkt sa grayscale slikama :D",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "262716": {
      "poster": "[deleted]",
      "content": "pseudoznanost odite kucat neki webdev, ovo su sve samo pogadnja od kojih niko koristi nema. Ekipa zna pokrenut dvije python skripte i drze se ko da su svu pamet svijeta popili",
      "votes": {
        "upvoters": [
          "ElonDusk (💀  )",
          "Lyras"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "ElonDusk (💀  )",
          "HeHe (Direktor života)",
          "Lyras",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "262718": {
      "poster": "ElonDusk (💀  )",
      "content": "@\" ℵ₀-male\"#p262716 it just werks 🤡",
      "votes": {
        "upvoters": [
          "AVRFreak"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "262724": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "> @\" ℵ₀-male\"#p262716 pseudoznanost\n\nPotpuno se slažem, ali većina stvari u IT-u nije znanost ili je pseudoznanost, pa se opet vraćamo na ono što nosi više para",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "ElonDusk (💀  )",
          "[deleted]"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "262831": {
      "poster": "Ovo_je_moj_nick",
      "content": "> @\"Rene\"#p262674 Nemam augmentacije, valjda\n\nNadam se da vas na faxu ne uce \"valjda\" kako raditi, nego kak je ispravo. Nemres mi doc s pitanjem, \"e ne znam kaj je krivo\" a imas u svom workflowu \"valjda nesto koristim il ne koristim\"\n\nZbog takvih i ispadne da je ML i slicno \"nitko ne zna sto se dogadja\" kad ekipa ni sama ne zna sto radi",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "[deleted]",
          "kix7 (Fish99)"
        ]
      },
      "reactions": {
        "haha": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "262835": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Ovo_je_moj_nick\"#p262831 ono kad ucis model transponiranom matricom znacajki.\n\nWhy no workee",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "262836": {
      "poster": "Jokerrr",
      "content": "@\"Rene\"#p262663 sad cu ti ja transcendirati um, ukloni si sum s 2D fftom, salim se n e m o j",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "262837": {
      "poster": "InCogNiTo124",
      "content": "@\"Jokerrr\"#p262836\n\n# DOAS GANG DOAS GANG",
      "votes": {
        "upvoters": [
          "Bog (Charlie Brown)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "262838": {
      "poster": "InCogNiTo124",
      "content": "samo si poboljsaj kvalitetu slike sa vlastitom implementacijom unsharp maske kolko tesko moze bit",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Jokerrr",
          "Lyras",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "262847": {
      "poster": "Jokerrr",
      "content": "@\"InCogNiTo124\"#p262837 mislio sam upisat ovo, no kad sam vidio pragove….\n\nEditat cu i dodat DEEP LEARNING u post, da me admin nemoze dirat",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "InCogNiTo124"
        ]
      }
    },
    "262857": {
      "poster": "InCogNiTo124",
      "content": "@\"Jokerrr\"#p262847 ma dobro ja sam samo sretan sto bar netko zna za odsumljivanje fftom",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Jokerrr"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "263449": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "Vezano za ML, opcenitije AI, kako pratite novosti u području? AI je valjda na vrhu sto se tice inovacija pa me zanimalo kako u toj struji novosti ostajete informirani? Pratite li relevantne ljude na twitteru, konferencije, yt kanale, blogove, citate papere? Rado bih cuo vase metode. Sve dolazi u obzir, znanstveni radovi, akademija, industrija, biblioteke whatever.\n\nZapocet cu yt kanalom koji bi mogao biti\n\nposebno koristan novacima.\n\nhttps://youtube.com/c/YannicKilcher\n\nGospon na engaging i zabavan nacin komentira novosti u podrucju. Takoder ima videe gdje objasnjava novije i/ili povijesno vazne papere, odlicno je cuti iz druge ruke kondenziranu verziju znanstvenog rada. Dotice se i drugih aspekata podrucja, npr. njegovo iskustvo na doktoratu, svakako vrijedi pogledati...",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "263458": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Precious Bodily Fluids\"#p263449 arxiv sanity preserver top recent i top hype.\n\nTop hype strože odabirem teme nego top recent, znači praktički skimmam naslove pa koji mi zapadne za oko. Top recent čitam naslov po naslov i gledam autore, znači tu mi ima više potencijala nešto zapeti za oko kad detaljnije gledam metapodatke papera.\n\nE al da autore znam iskustveno tak da to vjv nije dobra metrika za početnike koji ni ne znaju mamojebače u struci, a i ponekad ti mamojebači sudjeluju na nečem izvan specijalizacije pa nije banger rad, npr. Kaiming He ima par trash koautorstva iako je tata mata inicijalizacije.",
      "votes": {
        "upvoters": [
          "[deleted]",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "nnn (dinoo)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "263463": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Mićo Maco\"#p263458 Eh i da, još jedna stvar, Google Brain obično ima dobre radove ali su im implementacija i ablacijske studije apsolutno smeće, to je presumably tako jer Google ima neke interne frameworke za prave implementacije, a na Github izbacuju TF/PT/JAX smeće koje ima jako malo veze s radom. Zato i dan danas implementaciju transformera nudi 3rd party Huggingface, jer Google ne da svoje visokooptimizirane interne inačice, nego nefunkcionalno smeće u TF-u.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "263489": {
      "poster": "InCogNiTo124",
      "content": "@\"Mićo Maco\"#p263458 kaiming he je tata mata dubokog racunalnog vida tocka, lik je izmislio valjda sve najkoristenije blokove, od resneta, pyramid blokova, mask rcnn, izmislio panoptic segmentaciju, izmislio group normalizaciju, izmislio focal loss...",
      "votes": {
        "upvoters": [
          "Ducky",
          "[deleted]",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "263490": {
      "poster": "InCogNiTo124",
      "content": "> @\"Mićo Maco\"#p263463 Google ima neke interne frameworke za prave implementacije\n\nUuu trac najs",
      "votes": {
        "upvoters": [
          "Dekan",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "265693": {
      "poster": "nnn (dinoo)",
      "content": "Koje sve firme se bave s DL u hr?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "265697": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"dino\"#p265693 \n\nnpr. @\"InCogNiTo124\"#p225650 i ispod toga",
      "votes": {
        "upvoters": [
          "nnn (dinoo)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "266777": {
      "poster": "Jokerrr",
      "content": "Zas tolko ljudi na internetu hvali taj kaggle? Slozio sam cijeli gameplan da se osposobim za neki junior posao do ljeta, ubacio Kaggle tecajeve posto su na dobrom glasu. Ne zelim biti snob al mi se ti tecajevi cine pre jednostavni?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "266782": {
      "poster": "InCogNiTo124",
      "content": "@\"Jokerrr\"#p266777 kaggle tecajevi? Kanye fuj\n\nKaggle natjecanja di u praksi vjezbas sva teoretska znanja koja imas te nova koja naucis od tudih notebooka? Kanye nice",
      "votes": {
        "upvoters": [
          "Jokerrr",
          "kix7 (Fish99)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "266791": {
      "poster": "Jokerrr",
      "content": "@\"InCogNiTo124\"#p266782 ok brisem kaggle tecajeve (oni udemy tecajevi ML i DL A-Z su bili zanimljiviji)\n\nOstaje mi d2l knjiga, maths for ML (naprednija poglavlja), takelab predavanja\n\nImas neki prijedlog da dodam nes prakticnog (nemam bas vremena za ta kaggle natjecanja)?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "266792": {
      "poster": "InCogNiTo124",
      "content": "> @\"Jokerrr\"#p266791 Imas neki prijedlog da dodam nes prakticnog (nemam bas vremena za ta kaggle natjecanja)?\n\nOno sta je meni najbolje pomoglo, iskreno, su vlastiti projekti. Pala mi je neka DL ideja, and i was like \"ajd idem iskorirat lmao\" i tak naucis najvise\n\nProbaj smislit neato svoje il dobij inspiraciju iz recimo https://github.com/NirantK/awesome-project-ideas",
      "votes": {
        "upvoters": [
          "Jokerrr",
          "[deleted]"
        ],
        "downvoters": [
          "[deleted]"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "267243": {
      "poster": "[deleted]",
      "content": "[The AI Research Paper Was Real. The ‘Coauthor’ Wasn't](https://www.wired.com/story/ai-research-paper-real-coauthor-not/)\n\n> David Cox, the co-director of a prestigious artificial intelligence lab in Cambridge, Massachusetts, was scanning an online computer science bibliography in December when he noticed something odd—his name listed as an author alongside three researchers in China whom he didn’t know on two papers he didn’t recognize.\n\n> At first, he didn’t think much of it. The name Cox isn’t uncommon, so he figured there must be another David Cox doing AI research. “Then I opened up the PDF and saw my own picture looking back at me,” Cox says. “It was unbelievable.”\n\n> It isn’t clear how prevalent this kind of academic fraud may be, or why someone would list as a coauthor someone not involved in the research. By checking other papers written by the same Chinese authors, WIRED found a third example, where the photo and biography of an MIT researcher were listed under a fictitious name.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "Ovo_je_moj_nick"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Ovo_je_moj_nick"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "267655": {
      "poster": "InCogNiTo124",
      "content": "Savage\n\nhttps://mobile.twitter.com/pmddomingos/status/1477435370706903043",
      "votes": {
        "upvoters": [
          "AVRFreak",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "AVRFreak",
          "Log3 (Jahač Kali Yuge)",
          "Lyras",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "kix7 (Fish99)",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "267662": {
      "poster": "Lyras",
      "content": "@\"InCogNiTo124\"#p267655 c0pium twitteratija u odgovorima. Based.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "Log3 (Jahač Kali Yuge)",
          "MarokMarsci (Collapse Enjoyer)",
          "[deleted]",
          "feel_d_boot (iNut)",
          "kix7 (Fish99)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Log3 (Jahač Kali Yuge)",
          "MarokMarsci (Collapse Enjoyer)",
          "feel_d_boot (iNut)",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "267834": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "Nešto se retard događa s torch 1.10; znači ako se pokuša instalirati preko conde, a u pozadini je ne-condin Python, dakle kompajliran, kloniran iz venva itd. pokušat će instalirati svoj python, i to 3.9.7. Ono što se dogodi je da sad imate 2 verzije pythona u jednom env folderu, i sve instalacije nadalje idu na taj 3.9.7. Nije bitno jel original verzija 3.8 ili 3.10, on umetne tu svoju koja potencijalno potrga sve ostale pakete. Originalan paket ne vidi ništa naknadno instalirano (jer je to u 3.9 site-packages), a taj 3.9 vidi stvari iz originalnog paketa na neku foru.\n\nInstalacija s pip-om nema takvih problema, dakle on poštuje env i instalira torch za tu verziju koju imate",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [
          "InCogNiTo124"
        ],
        "tuga": []
      }
    },
    "267835": {
      "poster": "InCogNiTo124",
      "content": "@\"Mićo Ketchum\"#p267834 koristenje conde u prvom mjestu je dosta sramotno dbi al ok\n\nZanimljiv bug jel ima reporata po internetima o tom ili ces ti slozit prvi\n\nMsm bug ocito je zeljeno ponasanje al pitanje zasto",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "267839": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"InCogNiTo124\"#p267835 koristim minicondu, ne vjerujem da ima bolji produkcijski package manager za DL i ono što ja trebam, a i pytorch default install je preko conde\n\nNisam našao da je netko već prijavio bug, možda složim kasnije, al htio bih probat jel i sa python 3.9.7 kompajliranim pokušava instalirati svoj python, jer to bi bio clown show tek",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "267847": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"InCogNiTo124\"#p267835 kaj fali condi? Koje su alternative?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "267860": {
      "poster": "InCogNiTo124",
      "content": "@\"Precious Bodily Fluids\"#p267847 malo trolam, u pricnipu nema nikaj lose u njoj, meni je samo bila cudna kad sam pocinjao sa datasci podrucjem, a i usko je nekako vezana uz ds, ja radim dosta i swe pa sam ja vise venv guy",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "267871": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Precious Bodily Fluids\"#p267847 Probaj instalirati clean environment s condom\n\nHINT: ne možeš, navodno kreatori conde nisu zamislili da bi ju mogao koristiti za clean install pythona nego da ti uvijek povlači pakete koje imaš instalirane globalno, alternative su ti:\n\n- instaliraš conda env pythona koju želiš, napraviš venv toga, i onda kloniraš taj venv za novi env (on će ti biti čist)\n    - cons: ne smiješ brisati/micati venv folder inače ti se skrši sve</LI>\n- moj način: skineš python direktno sa stranice, iskompajliraš i instaliraš na neko mjesto pa povlačiš s `--clone`\n    - npr. ja želim instalirati čisti 3.10 repo, pokrenem `conda create -n ime --clone /home/suflaj/programs/python/3.10`</LI>\n\nOvo sam saznao kad sam sav bijelo-privilegiran pokušao napraviti novi conda env bez 200 predinstaliranih paketa iz globala",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "267880": {
      "poster": "InCogNiTo124",
      "content": "@\"Mićo Ketchum\"#p267871 again, iz prvih principa si izmislio pyenv\n\nPogledaj si poetry uostalom",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "267896": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"InCogNiTo124\"#p267880 Nije, jer pyenv se koristi za switchanje između python **verzija**\n\nJa koristim kompajlirane pythone kao **bazu** za envove, ali imam nekoliko envova po verziji.\n\nPoetryevo forsiranje tomlova je dealbreaker, a sve druge probleme ionako rješava `pip-chill`",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "267900": {
      "poster": "InCogNiTo124",
      "content": "> @\"Mićo Ketchum\"#p267896 Poetryevo forsiranje tomlova je dealbreaker\n\nBoomercino, toml je buducnost",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "267901": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "Pyenv, pip-chill, poetry? Imenujte model i implementirat cu ga u C-u 😎",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "kix7 (Fish99)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "267912": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"InCogNiTo124\"#p267900 Preferiram kontekstno neovisnu gramatiku za datafileove",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "268002": {
      "poster": "[deleted]",
      "content": "fyi nikog nije briga za vas neradnike sta kradete graficke gejmerima",
      "votes": {
        "upvoters": [
          "ElonDusk (💀  )",
          "[deleted]",
          "kix7 (Fish99)",
          "mijauexe",
          "rija"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "268119": {
      "poster": "InCogNiTo124",
      "content": "[Cracking the Machine Learning interview](https://arxiv.org/abs/2201.00650)",
      "votes": {
        "upvoters": [
          "Emma63194",
          "[deleted]",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "mikimoj"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "268207": {
      "poster": "Asura (Asura)",
      "content": "Ako netko ne zna gdje potrositi 3 sata, ja sam stao na 1/4 i vjv cu sutra pogledat do kraja. Pretpostavljam da bi moralo biti korisno zbog ljudi koji pricaju.\n\nhttps://www.youtube.com/watch?v=86ib0sfdFtw&ab_channel=MachineLearningStreetTalk",
      "votes": {
        "upvoters": [
          "Ovo_je_moj_nick",
          "[deleted]",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "nnn (dinoo)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "268217": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"InCogNiTo124\"#p268119 koristio si?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "268222": {
      "poster": "InCogNiTo124",
      "content": "@\"Precious Bodily Fluids\"#p268217 ne to je onak jucer izaso paper",
      "votes": {
        "upvoters": [
          "[deleted]",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "268488": {
      "poster": "InCogNiTo124",
      "content": "> @\"Mićo Ketchum\"#p267896 `pip-chill`\n\nA prekul, to sam ja mislio napravit nekad, obicni toposort\n\nOh well",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "270584": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "jel radi itko na faksu RL?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "270680": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Precious Bodily Fluids\"#p270584 Da samome sebi odgovorim,\n\nZvonko Kostanjčar drži predmet o podržano učenje",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "270696": {
      "poster": "InCogNiTo124",
      "content": "> @\"Precious Bodily Fluids\"#p270680 podržano učenje\n\nHrindz",
      "votes": {
        "upvoters": [
          "Emma63194",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "271572": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "Znalci, koje predmete preporučate u ljetnom semestru? Za sada je ideja uzeti analizu i pretrazivanje teksta i duboko ucenje 1. Kaj još valja za AI i okolna podrucja? Nekakva matematika, obrada podataka (signali, slike, tekst), pretpostavljam dobro dođe?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "271580": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "> @\"Precious Bodily Fluids\"#p271572 Nekakva matematika, obrada podataka (signali, slike, tekst), pretpostavljam dobro dođe?\n\nDa al nema baš dobrih predmeta za to, rađe upiši nešto lagano i rokaj projekte u slobodno vrijeme, ništa što na FERu u sklopu nastave budeš radio neće moći konkurirati tome, osim ak je DUBUCE2 nekakav bogovski predmet na kojem se radi state of the art prethodne godine il neš",
      "votes": {
        "upvoters": [
          "[deleted]",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "271583": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Mićo poslednji klovn\"#p271580 a apt i dubuce1?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "271584": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Precious Bodily Fluids\"#p271583 Ma ti su okej, referiram se na beskorisnost drugih predmeta",
      "votes": {
        "upvoters": [
          "[deleted]",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "271647": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Mićo poslednji klovn\"#p271584 a dubinska analiza podataka?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "271654": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Precious Bodily Fluids\"#p271647 Idk to je neš novo al Jović kao nositelj nije reassuring...\n\nTreba gledat što će UUZOP ekipa reći o predmetu s obzirom na to da je to isto njegov predmet, ja nemam pozitivno mišljenje o njegovim predmetima niti vidim kako bi ti 3.5 mjeseca tog predmeta pomogla više od samostalnog rada, bar po programu",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "[deleted]",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": [
          "sheriffHorsey"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "271656": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Mićo poslednji klovn\"#p271654 na uuzopu sam, a nezz, predmet je po mom mišljenju za 3. godinu jer se govori o svemu po malo, površno, kao pregled svega. Ali projekt nije bio loš, treba iz nekog postojećeg papera reproducirati pa pokušat poboljšat njegove rezultate... Okej praktičan rad za prvu ruku",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "271659": {
      "poster": "Lyras",
      "content": "@\"Mićo poslednji klovn\"#p271654 uuzop je doslovno pregled cijelog područja i nije baš koristan izvan samog projekta. Dosta lagan, ali uz strojno učenje, SAP, APR i duboko učenje je useless, osim ako nekoga samo zanima da dobije grupu procjenu tematika koja se obrađuje dalje. Ne znamo kakvi će još biti ispiti.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "271681": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Precious Bodily Fluids\"#p271656 \n\n@\"Lyras\"#p271659 \n\nNe mislim na gradivo predmeta, nego na vodstvo. Dealbreaker kod Jovinja je što su mu predmeti do sad bili katastrofalno organizirani, gradivo je ionak nebitno kad nećeš ništa relevantnog za moderni DL naučiti na FERu, jedino je zgodno naći predmete koji su manje zlo radi nekvalitete RZ/ZOP diplomskog.",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "271682": {
      "poster": "InCogNiTo124",
      "content": "> @\"Mićo poslednji klovn\"#p271681 nećeš ništa relevantnog za moderni DL naučiti na FERu\n\nIstina\n\nSpeaking of, ima li nekih sazetih materijala za moderni dl? Ili barem neki guided DL popis papera",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "271686": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"InCogNiTo124\"#p271682 Nope, popis možda i nađeš negdje, ali sve što sam ja do sad vidio je bilo baš jako opinionated\n\nnpr. ovo https://medium.com/@diegobonila/top-deep-learning-papers-of-2021-529a6f2e17cb\n\nlolčina, vision transformeri, dall-e, knowledge retrieval niđe na vidiku",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272415": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "single author paperi; referenciranje na autora kao `we` ili `I`?\n\nJa koristim `we` ali znam da su (neki?) mentori na FERu triggerani na to i često zahtijevaju da se referenciraš u 1. licu jednine jer je gramatički ispravno, neovisno o postojećoj praksi u znanosti.\n\nDakle, u pogledu deep learning papera, jeste li za gramatiku ili povijesnu konzistentnost znanstvenih radova? Jeste li zumi ili bumi?\n\nEDIT:\n\n- `I`: haha reacc\n- `we`: sad reacc",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "Amon",
          "AromaticConfusion (VrloZbunjen)",
          "Dekan",
          "ErnestHemingway (Alfetta)",
          "General_Kenobi (Aborteur)"
        ]
      }
    },
    "272416": {
      "poster": "Amon",
      "content": "@\"Mićo poslednji klovn\"#p272415 Kako da glasamo? Ćeš napraviti thread s anketom ili samo neke reakcije or what?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272419": {
      "poster": "InCogNiTo124",
      "content": "@\"Mićo poslednji klovn\"#p272415 zumi all the way. Tak je cudno kad pises o sebi u mnozini ko da vas ima 15 u toj maloj glavici. Pisi u jednini",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272420": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Mićo poslednji klovn\"#p272415 koji reacc je za they/them?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Amon",
          "Lyras",
          "feel_d_boot (iNut)",
          "kix7 (Fish99)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "272423": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Amon\"#p272416 stavijo sam edit mb\n\n@\"InCogNiTo124\"#p272419 pišem `we` u smislu ja i čitatelj hehe",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "HeHe (Direktor života)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "272426": {
      "poster": "InCogNiTo124",
      "content": "@\"Mićo poslednji klovn\"#p272423 jes citatelj je sigurno sa mnom dobio +0.18% na imagenetu, je kurac, da je nebi cito paper",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "kix7 (Fish99)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "272428": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"InCogNiTo124\"#p272426 Ma više vodiš ga za rukicu dok mu pokazuješ što si radio",
      "votes": {
        "upvoters": [
          "General_Kenobi (Aborteur)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272429": {
      "poster": "InCogNiTo124",
      "content": "@\"Mićo poslednji klovn\"#p272428 al to onda nije paper, to je medium blog haha",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ]
      }
    },
    "272431": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"InCogNiTo124\"#p272429 Al mislim paper nije ni mjesto gdje ima ikakve individualnosti, za to postoje Twitter i konferencije",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272433": {
      "poster": "InCogNiTo124",
      "content": "@\"Mićo poslednji klovn\"#p272431 predlazem slijedece: uvodenje treceg termina \"tim istrazivaca\" koji moze imati [imath]n \\ge 1[/imath] clanova",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272449": {
      "poster": "Ovo_je_moj_nick",
      "content": "@\"Mićo poslednji klovn\"#p272415 \n\ncek what?\n\noces publishat neki clanak kojeg si sam napisao? ako je tako, nema smisla da pises WE. Ako si posao obavio s nekim, prvenstveno koautorima, onda pises WE.\n\nsad je pitanje di mislis poslat clanak, ako je arxiv tek tolko da imas nesto ko i ostali, nije bitno, za sve ostalo imas journal guidelines",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272451": {
      "poster": "InCogNiTo124",
      "content": "> @\"Ovo_je_moj_nick\"#p272449 ako je tako, nema smisla da pises WE\n\nTo ni meni nema smisla al tog ima, sto vise vecina solo autora pise we, tipa najnedavniji kojeg sam citao je https://arxiv.org/abs/1806.09997",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272455": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Ovo_je_moj_nick\"#p272449 Kužim ja da postoje razlozi za to, ali ne mislim se preobratit, nego zanima me što drugi misle haha\n\nUvijek prioritet imaju zahtjevi institucije di pišeš, a onda tek ti, no ovog puta zanimaju me mišljenja, ne činjenice",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272472": {
      "poster": "tata (NISAM ASISTENT)",
      "content": "koji su razlozi za we?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272478": {
      "poster": "Ovo_je_moj_nick",
      "content": "@\"InCogNiTo124\"#p272451 arxiv nije previse kredibilan tak da moze pisat do preksutra We. Stvarno je retardirano da jedan autora prica kako smo mi nesto napravili ako je ocito to on napravio\n\nrecimo frend: https://arxiv.org/abs/2108.09478\n\nkifla krivi link, i on je pisao u ovom WE",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272598": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"NISAM ASISTENT\"#p272472 \n\nhttps://academia.stackexchange.com/questions/2945/choice-of-personal-pronoun-in-single-author-papers/2948#2948\n\ntldr konvencija i konzistencija",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "272648": {
      "poster": "Jokerrr",
      "content": "@\"InCogNiTo124\"#p272426 +0.18 p.b. molim lijepo\n\nBas nam je prekjucer sensei segvic poslao wiki link na postotne bodove i listu od 20 jezicnih primjedbi u izvjescu",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "BreadCat",
          "InCogNiTo124",
          "Lyras"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "272649": {
      "poster": "InCogNiTo124",
      "content": "@\"Jokerrr\"#p272648 to zvuci ko sinisa, da",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "BreadCat",
          "Jokerrr",
          "Lyras"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "277106": {
      "poster": "InCogNiTo124",
      "content": "Junior Bog isus Dubokog Ucenja, odmah nakon Kaiming fckn He\n\n\nhttps://github.com/lucidrains?tab=repositories",
      "votes": {
        "upvoters": [
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277109": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"InCogNiTo124\"#p277106 Ček dis nigga ima više   implementacija mreža nego ja faks repoa 😶",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124",
          "Lyras",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "277285": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "minhash + dft = sex\n\ndisregard dimensionality, acquire perfect feature extractor",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277302": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "Kakav je ovaj konstanjcarev lab sto spaja ML i financije?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277321": {
      "poster": "InCogNiTo124",
      "content": "@\"Mićo Simpćo\"#p277285 ti si se bas bacio u te hasheve",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277351": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"InCogNiTo124\"#p277321 Postoji jako velika ljepota u tome da možeš proizvoljno veliku sekvencu prebaciti u fiksiranu veličinu jeftino i s dobrim featurima.\n\nUšteda parametara je ogromna, a i može se asinkrono vrtiti na CPU dok GPU radi nešto drugo.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "277353": {
      "poster": "InCogNiTo124",
      "content": "@\"Mićo Simpćo\"#p277351 morat cu se bacit u to jednom",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "280628": {
      "poster": "Jokerrr",
      "content": "Potencijalno krivi thread al mislim da su tu ljudi koji bi najvise mogli pomoc\n\nZnate li za kakve internshipe/strucne prakse u podrucju dubokog ucenja? (ak imate info i o klasik strojnom slobodno napisite)",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "280636": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Jokerrr\"#p280628 Photomath planira organizirati ljetnu ML akademiju, to bi ti vjerojatno bilo zanimljivo",
      "votes": {
        "upvoters": [
          "Jokerrr"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "280641": {
      "poster": "login",
      "content": "@\"Jokerrr\"#p280628 Opcenito ima vise firmi u Zg koje se bave DL podrucjem: visage technologies, gideon brothers, photomath, real networks, megatrend. Ovo su neke od poznatijih ali ima i jos manjih koje isto ulaze u to podrucje. Ne znam koje od ovih ce ti bit ove godine na ljetnim praksama, prosle godine su bile sve ove navedene osim photomatha, ali ove godine budu i oni. Najbolje ti je pratit obavijesti pa ce vec objavit, al ove ti budu sigurno",
      "votes": {
        "upvoters": [
          "Jokerrr",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "280642": {
      "poster": "InCogNiTo124",
      "content": "@\"Jokerrr\"#p280628 fer ljetne prakse svake godine imaju preko nekoliko ai/ml/ds firmi u kojima mozes bit intern\n\n@\"Mićo pita za kuje\"#1 jel imas ti screpjano od lani?",
      "votes": {
        "upvoters": [
          "Jokerrr"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "280643": {
      "poster": "InCogNiTo124",
      "content": "> @\"login\"#p280641 prosle godine su bile sve ove navedene osim photomatha, ali ove godine budu i oni\n\notkud ti ovo 🤔",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "280680": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "> @\"InCogNiTo124\"#p280642 jel imas ti screpjano od lani?\n\nima na githabu",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "280851": {
      "poster": "login",
      "content": "@\"InCogNiTo124\"#p280643 Pa kolko se sjecam nije ih bilo na ljetnim praksama. Mozda sam fulao al skoro pa sam siguran da ih nije bilo na popisu haha",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "280930": {
      "poster": "feel_d_boot (iNut)",
      "content": "Napravite par svojih projekata i posa sida ubrzo, jebesh ljetne prakse. Pržite se na suncu i ispijajte cocktele",
      "votes": {
        "upvoters": [
          "Dorinacoko (Podvlaka_)",
          "[deleted]",
          "atp0lar (‮ 🏳️‍⚧️‍⃠ 🏳️‍🌈⃠ 🇮🇱⃠at⁭p⁩⁫0⁮lar)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "282938": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "XD onaj trenutak kad ti particioniranje tenzora ubije cijeli gradijent i godinu dana rada XDDDDDDD",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "Lyras"
        ]
      }
    },
    "282983": {
      "poster": "[deleted]",
      "content": "@\"Tko je Mićo Simpćo?\"#p282938 ko ti kriv sta se ogranicen",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "287240": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Mićo konjoser Čehinja\"#p287230\n\n1 piksel za input, wat?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "287284": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Precious Bodily Fluids\"#p287240 MNIST je toliko lagan zadatak da kad bi gradio MLP klasifikator mogao bi odabrati 1 piksel inputa kao sve ulazne značajke i radilo bi. Kad bi nasumično permutirao piksele mreža bi ti naučila razdiobu piksela. Aritmetička sredina piksela je 1. član uzorka u frekvencijskoj domeni, stoga povezujem da je uvijek nasumična permutacija značajki analogna učenju jedne značajke: aritmetičke sredine vrijednosti piksela.",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "287294": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Mićo konjoser Čehinja\"#p287284 kad kažeš 1 piksel misliš da se rezolucija smanji na jedan koristenjem avga jel?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "287295": {
      "poster": "[deleted]",
      "content": "reddpilujte me o ovom podrucju vs ri",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "287307": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Precious Bodily Fluids\"#p287294 Ne nužno, mislim da možeš uzeti bilo koji piksel slike tj. bilo koju fiksnu koordinatu i na tome klasificirati. Tj. vjv bi bilo dobro uzeti neku koordinatu koja je bliže centru slike, al you get the point",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "287308": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Mićo konjoser Čehinja\"#p287307 ummm, dosta iznenadujuce, moram reci,\n\nkoliki bi accuracy ocekivo?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "287312": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Precious Bodily Fluids\"#p287308 80-90%, barem po pričama asistenta od kojeg sam ovo prvi put čuo",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "289245": {
      "poster": "InCogNiTo124",
      "content": "jebote\n\nhttps://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html",
      "votes": {
        "upvoters": [
          "[deleted]",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "indythedog"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "289257": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"InCogNiTo124\"#p289245 Procjene da bi se ovo istreniralo su IIRC 300 mil dolara",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [
          "InCogNiTo124"
        ],
        "tuga": []
      }
    },
    "289260": {
      "poster": "InCogNiTo124",
      "content": "@\"Mićo konjoser Čehinja\"#p289257 a i ovaj pathways je banger od arhitekture \n\nhttps://arxiv.org/abs/2203.12533",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "289549": {
      "poster": "BreadCat",
      "content": "https://openai.com/dall-e-2/",
      "votes": {
        "upvoters": [
          "[deleted]",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "289570": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "Ja, spajajući slojeve u def forward()  \n\n>! https://www.youtube.com/watch?v=V0t3V-jaKNM",
      "votes": {
        "upvoters": [
          "Lyras",
          "indythedog"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Amon",
          "Lyras",
          "feel_d_boot (iNut)",
          "indythedog",
          "rolotex (brr)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "291895": {
      "poster": "[deleted]",
      "content": "https://arxiv.org/abs/2106.10165\n>      This book develops an effective theory approach to understanding deep neural networks of practical relevance. \n\n471 stranica nekakve matematike vezane za razumijevanje neuronskih mreža. Ne vjerujem da će biti  korisno, ali već sam navučen. 😍\n\nthx @\"InCogNiTo124\"#999",
      "votes": {
        "upvoters": [
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "291897": {
      "poster": "InCogNiTo124",
      "content": "@\"Emilia\"#p291895 \n\n![](assets/2022-04-20/00020.jpg)",
      "votes": {
        "upvoters": [
          "BreadCat",
          "[deleted]",
          "feel_d_boot (iNut)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Amon",
          "BreadCat",
          "Ducky",
          "Jale (čakijale)",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "feel_d_boot (iNut)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "291919": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "umetna intelidžencija je budućnost, areyoutheasshole.com\n\n![](assets/2022-04-20/00023.png)\n\nizgleda da je učeno na super primitivnom modelu, na užasnom datasetu, i model koji je treniran samo da ti govori da si šupak još uvijek poziva na genocid, top kek",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Ducky",
          "General_Kenobi (Aborteur)",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "ink"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "291955": {
      "poster": "InCogNiTo124",
      "content": "Numpy limerick\n\n![](assets/2022-04-20/00030.jpg)",
      "votes": {
        "upvoters": [
          "[deleted]",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "292471": {
      "poster": "Haki",
      "content": "Jel postoji neka stranica gdje mogu naci najbolje modele za neki zadatak? Trazim klasifikacijski model koji je po broju parametara/racunalnoj slozenosti u razini ResNeta50 il EfficientNeta B4.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "292483": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Haki\"#p292471 Imaš PapersWithCode, ResNet50 ti je 23 milijuna parametara i 4 GFLOPSa, što znači da tražis neki conv model s ResNet34 ili ResNet18 kao backboneom\n\nMeđutim to je daleko od najboljeg, top efikasni modeli su ti transformerski sa 300-1000 GFLOPS i nekoliko stotina milijuna parametara. Ja sam u sklopu diplomskog gledao malo i čini se da je trenutno najbolji pretrainani model koji se može vrtiti na consumer hardweru Swin, a njega imaš i u HuggingFace `transformers` libu.",
      "votes": {
        "upvoters": [
          "Haki"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "292488": {
      "poster": "InCogNiTo124",
      "content": "@\"Mićin mali Mićo\"#p292483 fucking deep learning koji je to meme\n\nKolki je inference time nekog vita na consumer grafi, 20sec?\n\nEdit: oke ovaj swin je dost brz al on ima 50G flosp i nakon kvantizacije\n\nOvi sirovi me zanimaju",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "292489": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"InCogNiTo124\"#p292488 t. cnn enjoyer, ngmi",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "292490": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Mićin mali Mićo\"#p292483 I inače nek te ne zabrinjavaju previše redovi veličine veće složenosti, predtrenirani transformeri dost brzo konvergiraju naspram čistim konvolucijskih modela.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "292497": {
      "poster": "Haki",
      "content": "@\"Mićo?\"#p292490 Hvala, isprobat ću Swin, valjda će 6gb vrama bit dosta🤞",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "292506": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Haki\"#p292497 Je, meni na 1060 6GB radi",
      "votes": {
        "upvoters": [
          "Haki"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "293419": {
      "poster": "--- ( 🤡 )",
      "content": "ne znam je li ovo prava tema za pitati, noob sam za CNN, ali jel bi ovo bila korektna implementacija opisane arhitekture u PyTorchu? ty unaprijed!\n\n> 1. Conv with 16 channels and 3x3 filters. Relu activations.\n> 2. Batch normalization.\n> 3. Max pooling with 2x2 filters and 2x2 stride.\n> 4. Conv with 32 channels and 3x3 filters. Relu activations.\n> 5. Conv with 32 channels and 3x3 filters. Relu activations.\n> 6. Batch normalization.\n> 7. Max pooling with 2x2 filters and 2x2 stride.\n> 8. Fully connected with output dimension 1024. Dropout.\n> 9. Fully connected layer with output dimension 3.\n\n```\nclass SmallNet(nn.Module):\n    def __init__(self):\n        super(SmallNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, (3, 3), (1, 1))\n        self.conv2 = nn.Conv2d(16, 32, (3, 3), (1, 1))\n        self.conv3 = nn.Conv2d(32, 32, (3, 3), (1, 1))\n        self.batch_norm1 = nn.BatchNorm2d(16)\n        self.batch_norm2 = nn.BatchNorm2d(32)\n        self.dropout = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(70688, 1024)\n        self.fc2 = nn.Linear(1024, 3)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.batch_norm1(x)\n        x = F.max_pool2d(x, (2, 2), (2, 2))\n\n        x = self.conv2(x)\n        x = F.relu(x)\n\n        x = self.conv3(x)\n        x = F.relu(x)\n\n        x = self.batch_norm2(x)\n        x = F.max_pool2d(x, (2, 2), (2, 2))\n\n        x = x.view(x.shape[0], -1)\n        x = self.fc1(x)\n        x = self.dropout(x)\n\n        x = self.fc2(x)\n\n        output = F.log_softmax(x, dim=1)\n        return output\n```",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "293423": {
      "poster": "InCogNiTo124",
      "content": "@\"😎🚬😎🚬😎🚬\"#p293419 je tocno je koliko vidim, ali malkoc je preverbozno nego sto mi je ugodno 😅\n\nS obzirom da je arhitektura poprilicno linearna (nemas grane kao tipa resnet), probaj prepisat isti ovaj model koristeci `nn.Compose()` klasu za vjezbu pa pejstaj ovdje\n\n>! razmisli kaj ces sa aktivacijskim funkcijama, odnosno kak u `nn.Compose()` stavit relu i reshape",
      "votes": {
        "upvoters": [
          "--- ( 🤡 )"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "293424": {
      "poster": "InCogNiTo124",
      "content": "@\"😎🚬😎🚬😎🚬\"#p293419 \n\n> `super(SmallNet, self).__init__()`\n\nNemoj ovo radit nismo u python2 vec desetljece, obican `super()` funkcionira",
      "votes": {
        "upvoters": [
          "--- ( 🤡 )",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Ovo_je_moj_nick"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "293435": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"😎🚬😎🚬😎🚬\"#p293419 Par zamjerki\n\n- Batch norm bi mogao ići prije ReLUa ako je dozvoljeno, probaj, možda dobiješ bolje rezultate\n- Sloj nakon kojeg ide batch norm ne treba bias\n- Flatten je bolja opcija od view jer radi na više vrsta podataka\n- Dropout treba biti manji ili ga stavi kao argument",
      "votes": {
        "upvoters": [
          "--- ( 🤡 )"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "293550": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "Kakva je situacija trenutno sa ponudom/potražnjom poslova u AI/ML, i kakvo će bit stanje u budućnosti?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "293614": {
      "poster": "--- ( 🤡 )",
      "content": "@\"InCogNiTo124\"#p293423 ne mogu naći Compose u nn kontekstu, jesi možda mislio na Sequential?\n```\nclass SmallNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(3, 16, (3, 3), (1, 1), bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.MaxPool2d((2, 2), (2, 2)),\n            nn.Conv2d(16, 32, (3, 3), (1, 1)),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, (3, 3), (1, 1), bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.MaxPool2d((2, 2), (2, 2)),\n            nn.Flatten(1),\n            nn.Linear(14112, 1024),\n            nn.Dropout2d(0.25),\n            nn.Linear(1024, 3)\n        )\n\n    def forward(self, x):\n        x = self.network(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n```\n\ntakoder, znate li jel ovaj error dobivam zbog mozda krivog korištenja flattena/view-a? radim s rgb slikama i batchevima tako da bi tenzori trebali biti 4D ako se ne varam:\n> dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "293617": {
      "poster": "InCogNiTo124",
      "content": "> @\"😎🚬😎🚬😎🚬\"#p293614 jesi možda mislio na Sequential\n\nJesam sori\n\nI vidis kak je ovo puno urednije 😃 \n\n> @\"😎🚬😎🚬😎🚬\"#p293614 takoder, znate li jel ovaj error dobivam zbog mozda krivog korištenja flattena/view-a?\n\nTehnickiii ovaj error ti je zbog krivog koristenja dropouta. nakon flattena vise nemas batch slika vec batch vektora, dakle niz 1d podataka. droput 2d pretpostavlja da ces mu feedat slike, al kak mu feedas vektore on se zbrejka\n\najmo rec da radis sa slikama sve do drugog MaxPoola,a nakon tog radis sa feature vektorima\n\n`nn.Dropout` obicni bi trebo radit tho",
      "votes": {
        "upvoters": [
          "--- ( 🤡 )"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "293650": {
      "poster": "--- ( 🤡 )",
      "content": "malo mi je sus da mi točnost klasifikacije nad validacijskim setom već nakon prve epohe bude >= 97%, i onda sam pustio da ide još nekih 40 epoha i uvijek je oko 99% :S",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "293653": {
      "poster": "InCogNiTo124",
      "content": "@\"😎🚬😎🚬😎🚬\"#p293650 to je malo sus al ovisi o problemu, velicini dataseta, jesu li train val i test disjunktni, augmentacije...",
      "votes": {
        "upvoters": [
          "--- ( 🤡 )"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "293654": {
      "poster": "--- ( 🤡 )",
      "content": "@\"InCogNiTo124\"#p293653 radi se u biti o klasifikaciji kutija gdje je glavni faktor njihova boja i guess, više manje je sve ostalo slično, dataset ima oko 7000 slika po klasi (podijeljeno onda 70/20/10 na train/test/val)\n\novaj eksperiment je bio bez augmentacije, ali mislio sam provesti i jedan koristeći RandAugment()\n\ntakođer slike nisu istih dimenzija pa sam ih ja u loaderu resizeao da sve budu iste, to bi trebalo biti ok zar ne?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "293665": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"😎🚬😎🚬😎🚬\"#p293650 Provjeri da nemaš leak",
      "votes": {
        "upvoters": [
          "--- ( 🤡 )"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "293670": {
      "poster": "--- ( 🤡 )",
      "content": "@\"Mićo?\"#p293665 u smislu korištenja test/val podataka tijekom treniranja? ne bi trebalo biti... što se tiče podataka sve je ručno podijeljeno u 3 poddirektorija i koristim 3 DataLoadera (train, test i validation) + u train u pojedinim eksperimentima dodam RandAugment()\n\nslike su definitivno odvojene ispravno, ali jesu jako međusobno slične. Može li biti do toga?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "293673": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"😎🚬😎🚬😎🚬\"#p293670 Ne test, val, tebi je problem train i nešto, ručno dijeljenje ti ne znači puno ako su podatci duplicirani, dakle red bi bio provjeriti sadržaj nekom hash funkcijom.\n\nZa egzaktne je dovoljan md5, ali možeš primjerice raditi forward passove i uspoređivati distribucije izlaza između setova, oni bi međusobno trebali biti unutar distribucije, ali ne bi smjeli biti slični setovi, dakle ako ti 2 različite slike daju gotovo iste logite, onda ih treba provjeriti.\n\nAko su međusobno slične onda to možda može biti leak (?), ali prije je da samo imaš užasno jednostavan zadatak, pa je lakše naučiti raspodjele. Ja pričam o slučajevima kad baš imaš iste uzorke ili gotovo identične (npr. png i jpg iste slike neće imati isti md5, al bi mogle imati isti simhash)\n\nIsto tako točnost nije relevantna metrika ako imaš disbalans klasa, tako da rađe koristi recall i precision",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "293786": {
      "poster": "Jokerrr",
      "content": "Kaj tocno znaci poduzorkovana reprezentacija podataka i koji bi to termin bio na engleskom?\n\nKonkretna recenica glasi: konvolucijski modeli zakljucivanje provode na poduzorkovanoj reprezentaciji.\n\nRazumijem laicki al trebam nesto detaljnije posto moram pojasnit u zavrsnom",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "293788": {
      "poster": "InCogNiTo124",
      "content": "@\"Jokerrr\"#p293786 poduzorkovanje = eng _subsampling_ a u CVju je to fensi način za reć skalirat sliku (resize)\n\nkad feedas sliku velicine (recimo) 256x256 u duboki model, na izlazu dobis feature representation od (recimo) 16x16 nad kojim onda radis predikciju klase ili cega vec. upravo to smanjenje rezolucije, ulaz vs izlaz, na to se misli kad se kaze da se zakljucivanje provodi nad poduzorkovanom reprezentacijom",
      "votes": {
        "upvoters": [
          "--- ( 🤡 )",
          "Emma63194",
          "Jokerrr",
          "[deleted]",
          "mikimoj"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "293789": {
      "poster": "Jokerrr",
      "content": "@\"InCogNiTo124\"#p293788 aaa sad mi je sve sjelo na mjesto. Hvala ti",
      "votes": {
        "upvoters": [
          "InCogNiTo124"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "294657": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "TIL ONNX ne podržava LERP :(",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "boogie_woogie (nika_1999)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "294663": {
      "poster": "[deleted]",
      "content": "@\"Mi Ćinping\"#p294657 Što je LERP? Ako je linearna interpolacija što su ulazi i izlazi?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "294667": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Emilia\"#p294663 minimum, maksimum i težina (0 do 1)",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": [
          "boogie_woogie (nika_1999)"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "295268": {
      "poster": "Rene",
      "content": "Kako se prevode R-CNN (region based cnn) na hrvatski? Regionalne konvolucijske mreže mi zvuči užasno",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "295270": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Rene\"#p295268 Ostavi se kao R-CNN najvjerojatnije\n\nTočniji prijevod bi bio *konvolucijske (neuronske) mreže na temelju područja* ili *lokalitetne konvolucijske (neuronske) mreže*, ali nisam ništa takvog baš vidio u našim radovima\n\nAko te baš zanima prijevod možeš pitati mentora ili nekog stručnog za to na faksu, oni će najprije znati jel se to u praksi prevodi i u što",
      "votes": {
        "upvoters": [
          "Rene"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "295271": {
      "poster": "InCogNiTo124",
      "content": "@\"Rene\"#p295268 napisi \"konvolucijski duboki modeli temeljeni na podrucjima (u daljnjem tekstu R-CNN)\" 😉",
      "votes": {
        "upvoters": [
          "Emma63194",
          "Rene",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Emma63194",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "295360": {
      "poster": "matt (Matt)",
      "content": "Koristi li se [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/starter/introduction.html) u nekoj manje ozbiljnoj produkciji? Znam da je više služi za research i slične stvari ali moram reći da je meni bio pravi užitak koristiti ovaj framework. Fakat sam impresioniran koliko je bolierplatea makne bez da te zakine za neku funkcionalost jer ti i dalje omogućava da se spustiš na jako nisku razinu. Dapače, neke stvari čak i lakše dodam preko gotovih/novih callbackova. Hard recommend ako radite neki solo projekt.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "295366": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Matt\"#p295360 U produkciji obično imaš servane modele, tak da ne baš, više ONNX pa onda može bit onnxruntime, TensorRT, Triton, Tensorflow Serving, TorchServe i tako",
      "votes": {
        "upvoters": [
          "matt (Matt)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "295368": {
      "poster": "[deleted]",
      "content": "@\"Matt\"#p295360 Ne znam sad o kakvoj točno produkciji misliš. Dok god možeš na kraju model ili modele eksportati u nešto normalno (tipa onnx, a trebao bi to moći iz pytorch lightninga), kod mene je irelevantno u čemu to treniraš ili praviš.\n\nA dalje taj onnx ide za produkciju (TensorRT,  kvantizacije za razne platforme, itd.).",
      "votes": {
        "upvoters": [
          "matt (Matt)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "295371": {
      "poster": "InCogNiTo124",
      "content": "@\"Matt\"#p295360 not to break NDA al mi ga u mojoj osobnoj firmi koristimo intenzivno\n\nŠtoviše, imam par majica od njih jer sam im napravio par PR\n\nUgl top shit framework",
      "votes": {
        "upvoters": [
          "Emma63194",
          "Lyras",
          "[deleted]",
          "kix7 (Fish99)",
          "matt (Matt)",
          "mikimoj"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "295489": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "![](assets/2022-05-10/00010.jpg)",
      "votes": {
        "upvoters": [
          "Amon",
          "InCogNiTo124",
          "Lyras",
          "StefBaustelic",
          "[deleted]",
          "feel_d_boot (iNut)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Dekan"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "295550": {
      "poster": "Jokerrr",
      "content": "Kmice moje drage jel itko tu ima iskustva sa swiftnetom?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "295573": {
      "poster": "InCogNiTo124",
      "content": "@\"Jokerrr\"#p295550 postovanje segvic gang na tom sam diplomiro",
      "votes": {
        "upvoters": [
          "Jokerrr",
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "295583": {
      "poster": "Jokerrr",
      "content": "@\"InCogNiTo124\"#p295573 jel postoje ikakve informacije/dokumentacija o toj mrezi osim github repo-a?\n\nTrebam umjesto softmaksa stavit arcface al mreza nije ni slicna mrezama s kojima sam radio pa se malo gubim u kodu",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "295649": {
      "poster": "InCogNiTo124",
      "content": "@\"Jokerrr\"#p295583 well ima OG paper ali nema nista drugo\n\nDoduse ak pitas Orsica i Segvica za detalje bit ce sretni i odgovorit ce ti sve sta treba",
      "votes": {
        "upvoters": [
          "Jokerrr",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "295675": {
      "poster": "Jokerrr",
      "content": "@\"InCogNiTo124\"#p295649 og paper je zakljucan. Bas sam ga zelio iznenadit i napravit sve sam al eto, nis od toga",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "295677": {
      "poster": "InCogNiTo124",
      "content": "@\"Jokerrr\"#p295675 ma sta zakljucan hehe zar nisi cuo za pricu aleksandre elbaykan i https://sci-hub.se",
      "votes": {
        "upvoters": [
          "Jokerrr",
          "[deleted]",
          "[deleted]",
          "feel_d_boot (iNut)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "295680": {
      "poster": "Jokerrr",
      "content": "@\"InCogNiTo124\"#p295677 brte ni aleksandra nema  taj rad u databaseu hahahaha. Pitat cu ih da mi posalju direkt",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "InCogNiTo124"
        ]
      }
    },
    "295816": {
      "poster": "Eugene",
      "content": "Poyy,\n\nRadim klasifikaciju teksta u tri klase (recimo pozitivan, neutralan, negativan).\n\nCilj je maksimizirat aritmetičku sredinu f1 scoreova pozitivne i negativne klase.\n\nKao loss funkciju koristim CrossEntropyLoss. \n\nZanima me bi li imalo smisla dodati dodatnu komponentu lossa koja će više kažnjavati suprotnu klasu (jer ako model zamijeni suprotne klase to više utječe na metriku koju gledam).\n\nZnači ako je primjer npr. pozitivan, uz CSE komponentu bi dodao nešto što bi trebalo minimizirati vjerojatnost negativne klase. \n\nOno što sam probao dodati je \n- -log(1 - probability(negativan))  - ako je pozitivne klase i\n- -log(1 - probability(pozitivan))  - ako je negativne klase.\n\nAli to je pogoršalo rezultat. \n\nJel ima smisla uopće to pokušavati? Ima netko bolju ideju?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "295818": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "> @\"Eugene\"#p295816 Zanima me bi li imalo smisla dodati dodatnu komponentu lossa koja će više kažnjavati suprotnu klasu (jer ako model zamijeni suprotne klase to više utječe na metriku koju gledam).\n\nOvisi šta želiš postići\n\nOvako bez razloga, ne",
      "votes": {
        "upvoters": [],
        "downvoters": [
          "[deleted]"
        ]
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "295821": {
      "poster": "Eugene",
      "content": "@\"Mićo vladar ženskog tijela\"#p295818 \n\n> Ovisi šta želiš postići\n\nPa želim maksimizirat metriku koju sam opisao. \n\nIdeja je da s ovim smanjim fulavanje između suprotnih klasa.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "295823": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Eugene\"#p295821 Ako želiš maksimizirat metriku bolje bi bilo proučiti dataset, podesiti model i nekako uključiti neutralnu klasifikaciju u cijelu priču.\n\nOvime što si htio si samo suštinski razdvojene klasifikacije (bar u zadnjem sloju) povezao lossem. To je eventualno metoda regularizacije, a regularizacija načelno ne povećava metrike, nego može smanjiti prenaučenost modela.\n\nKljučno je za primijetiti da ono što si htio napraviti nije povećalo količinu informacije u sustavu, pa očekivati da ćeš dobiti bolje metrike možeš eventualno na temelju sreće (što je definitivno moguće), ili zato što ti se model nije inicijalno dobro istrenirao, ili je nešto krivo definirano.\n\n---- \n\nE sad, ako ti je model znatno ispod ciljanih metrika, onda valjda jedino ima smisla mijenjati model, ili metodu učenja, ili u slučaju zeznutog dataseta popraviti isti. Ako nije onda bi trebao puno dublje teorijski proučiti **zašto** mu tako malo fali do cilja, a to onda nije pogađanje lossa.",
      "votes": {
        "upvoters": [
          "Eugene"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "296361": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "Ne prihvaćam nijedan optimizator osim Adama,\n\nHvaljen Isus i Marija",
      "votes": {
        "upvoters": [
          "ErnestHemingway (Alfetta)",
          "Jaster111",
          "[deleted]",
          "feel_d_boot (iNut)",
          "kix7 (Fish99)",
          "matt (Matt)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "ErnestHemingway (Alfetta)",
          "Jaster111",
          "feel_d_boot (iNut)",
          "matt (Matt)",
          "steker"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "298053": {
      "poster": "Jokerrr",
      "content": "@\"InCogNiTo124\"#p295573 oprosti na uznemiravanju al jos me nesto interesira o swiftu\n1. Jesi radio na colabu?\n2. Ako da, kak si izveo to posto moras pokrenut build.sh koja napravi .so datoteku koja se importa kao modul…? 😅 \n\nIma pun k posla s refactoranjem koda da se moze izvrtit na colabu",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "298054": {
      "poster": "InCogNiTo124",
      "content": "@\"Jokerrr\"#p298053 nisam skoro nikad koristio colab jebiga\n\nPitaj sinisu za train masinu, ja sam dinodas koristio",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "298057": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"InCogNiTo124\"#p298054 whats that?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "298061": {
      "poster": "Jokerrr",
      "content": "@\"InCogNiTo124\"#p298054 dobili smo odjeb, prezauzeti su s istrazivanjima. Colab it is 😆",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "301248": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "lol\n\nhttps://gpt-4chan.com/home",
      "votes": {
        "upvoters": [
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)",
          "matt (Matt)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "matt (Matt)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "301371": {
      "poster": "[deleted]",
      "content": "Imam .bin file s predtreniranom mrežom, na koji način da to naloadam na transformers BERT? Pokušavala sam na razne načine, ali jedan ne radi jer mu treba .json (BertModel.from_pretrained('...bin')), jedan učita nekaj ali ne radi mreža  (transformers.PreTrainedModel(BertConfig('....bin')) itd.\n\nSori ak je glupo pitanje, nisam ekspert a nemam više snage trošiti vrijeme na naizgled sitnicu, a možda ni nije moguće.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "301375": {
      "poster": "InCogNiTo124",
      "content": "@\"Aimée\"#p301371 odakle ti taj .bin? Mozda tamo imaju upute za ucitavanje modela\n\nRazlika je jel spremljeno sa torchem, kerasom, caffe2, mxnet, chainer, onyx...",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "301376": {
      "poster": "[deleted]",
      "content": "@\"InCogNiTo124\"#p301375 tenks, budem malo istražila",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "301380": {
      "poster": "[deleted]",
      "content": "@\"InCogNiTo124\"#p301375 torch je, model odovud (ali ti ne pomaže baš https://drive.google.com/drive/folders/1JcpY638OBM6zQFuRXZU0IzqtXaueAHcZ), ne vidim upute na gitu https://github.com/dykang/xslue\n\nak slučajno neko zna da ne moramo trenirati onaj Googleov  🙏",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "301381": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Aimée\"#p301380 Pogledaj si u kodu kako se učitava, vidio sam da u BERT verziji barataju s configovima",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "301384": {
      "poster": "InCogNiTo124",
      "content": "@\"Aimée\"#p301380 u tom slucaju neki torch.load bi trebao proc, bar prema kodu sa repoa",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "301664": {
      "poster": "Mariosss77 (Trevor)",
      "content": "@\"Aimée\"#p301371 \n\nOsim `pytorch_model.bin` datoteke, za inicijalizaciju modela trebas i `config.json` datoteku. Obicno su obje datoteke u istom direktoriju pa se jednostavno zove `BertForSequenceClassification.from_pretrained(\"path/to/dir\")` da bi se ucitao model. `BertModel.from_pretrained(\"path/to/dir\")` bi u tvojem slucaju ignorirao `SequenceClassification` glavu modela iz linkanog direktorija, a nisam siguran da to zelis (to se moze zakljuciti i iz warninga pri ucitavanju). Jednostavnije je shvatiti na temelju koda pa je u nastavku kod koji mozes isprobati u Colabu, a koji dohvaca model iz [StanfordPoliteness](https://drive.google.com/drive/folders/1i_MvWicbxUPheJ_sAxZXQDRu8oen-D1z) direktorija:\n ```python\n!pip install transformers\n!pip install gdown\n\nimport gdown\nfrom transformers import BertForSequenceClassification, BertTokenizer, Trainer\n\nURL = \"https://drive.google.com/drive/folders/1i_MvWicbxUPheJ_sAxZXQDRu8oen-D1z\"\nMODEL_DIR = \"/content/model\"\ngdown.download_folder(URL, output=MODEL_DIR, use_cookies=False)\n\nmodel = BertForSequenceClassification.from_pretrained(MODEL_DIR)\ntokenizer = BertTokenizer.from_pretrained(MODEL_DIR, use_fast=False)\n\n# Inference\npipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\npipe(\"Nice car!\")\n\n# Training\ntrainer = Trainer(...) # see: https://huggingface.co/docs/transformers/tasks/sequence_classification#train\n```",
      "votes": {
        "upvoters": [
          "[deleted]"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "301697": {
      "poster": "Emma63194",
      "content": "Tesla P100 ili RTX 2080 Ti?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "301709": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Emma63194\"#p301697 P100 ako je za DL bez puno filozofije, bolje na svaki način ako cijena nije problem",
      "votes": {
        "upvoters": [
          "Emma63194"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "301713": {
      "poster": "Emma63194",
      "content": "@\"Silly little Mićo\"#p301709 Gledam rabljene i cijene su više manje iste.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "301714": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Emma63194\"#p301713 Trebalo bi pogledati u kojem su stanju, i isto tako pitanje je jel se radi u 12 GB ili 16 GB modelu.\n\nZbog toga što se teže iživljavati na P100, ali i zbog toga što su Tesle generalno kvalitetnije napravljene, one generalno traju dulje. No to ne znači da najjeftinija P100 mora biti u boljem stanju od najjeftinije 2080Ti.\n\nOno što bi trebalo napraviti je zavrtiti neke benchmarke s neta pa usporediti s postojećim rezultatima. Tu će se vidjeti jel kartica podnosi load, jel silicij degradiran, i jel memorija još uvijek štima. Ako sve štima, znači da je unutar 5-10% rezultata, P100 je i za 12 GB model za DL bolja kartica.",
      "votes": {
        "upvoters": [
          "Emma63194"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "301957": {
      "poster": "Jokerrr",
      "content": "Upomoć. Imam CamVid dataset čije labele trebam pretvorit u uint8 grayscale da bih mogao pokrenut na singlescale modelu. Također imam color_info CamVida. Trebao bi mapirat svaku boju (klasu) iz tog color_info u neki integer. Kak izvest to pravilno? Znam da ovisi o implementaciji modela na kojem radim al neke generalne smjernice bi puno pomogle.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "301969": {
      "poster": "Jokerrr",
      "content": "@\"Jokerrr\"#p301957 uspio si bravo",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "302026": {
      "poster": "Schm0sby",
      "content": "![](assets/2022-06-07/00137.png)\n\nideja za zavrsni",
      "votes": {
        "upvoters": [
          "Piki",
          "feel_d_boot (iNut)",
          "kix7 (Fish99)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Haki"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "302028": {
      "poster": "InCogNiTo124",
      "content": "@\"Schm0sby\"#p302026 ak uspijes na feru nac dataset za to, to je podvig odma za phd",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Emma63194",
          "ErnestHemingway (Alfetta)",
          "Haki",
          "Omnitron ((-_-) )",
          "Schm0sby",
          "feel_d_boot (iNut)",
          "kix7 (Fish99)",
          "steker",
          "tata (NISAM ASISTENT)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "302231": {
      "poster": "Jokerrr",
      "content": "Nek mi neko vrati volju za racunalnim vidom 😫. Zavrsni mi se sveo na ucitavanje dataseta i popravljanje errora iz dijela koda koji nije moji. Tek sam danas napokon poceo trenirat model.\n\nPosljednji metak u glavu mi je bio to sto je rok 10.6. a ucenje traje 10 jebenih sati.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": [
          "sane_insane",
          "steker"
        ]
      }
    },
    "302235": {
      "poster": "InCogNiTo124",
      "content": "> @\"Jokerrr\"#p302231 ucitavanje dataseta i popravljanje errora iz dijela koda koji nije moj\n\nAlways has been kolega\n\nAlways has been\n\nDodi da te cika inco zagrli pa te vodim na sladoled pa idemo u devopse",
      "votes": {
        "upvoters": [
          "AVRFreak"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Amon",
          "BreadCat",
          "Emma63194",
          "ErnestHemingway (Alfetta)",
          "Jokerrr",
          "Lyras",
          "Omnitron ((-_-) )",
          "Piki",
          "Schm0sby",
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "feel_d_boot (iNut)",
          "kix7 (Fish99)",
          "matt (Matt)",
          "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
          "sane_insane"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "303427": {
      "poster": "InCogNiTo124",
      "content": "https://twitter.com/kareem_carr/status/1536160991549001731",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Amon",
          "Emma63194",
          "Haki",
          "Jokerrr",
          "Lyras",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "indythedog",
          "sane_insane",
          "steker"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "303794": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "Imate li kakve smjernice/materijale/primjere dobre organizacije koda za ML/DL projekte",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "303823": {
      "poster": "matt (Matt)",
      "content": "@\"Precious Bodily Fluids\"#p303794\n\nsve u jedan jedini main.py, reportove copy paste iz terminala u a.txt (sticky notes ako si win retard), model uvijek spremas kao ./model (overrideas stari model jer je irrelevant), dokumentaciju pišeš u readme.txt (ne u readme.md, nismo github cuckovi), a slike ne koristis jer onaj tko ne zna izvući šta se događa iz txt dokumentacije i ne komentiranog koda vjerojatno nije imao 5 iz DU",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "Jale (čakijale)",
          "Jokerrr",
          "kix7 (Fish99)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Jokerrr",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "303828": {
      "poster": "InCogNiTo124",
      "content": "@\"Matt\"#p303823 \n\n![](https://c.tenor.com/gHhvIlp6LBoAAAAM/mega-based-mega.gif)",
      "votes": {
        "upvoters": [
          "Jale (čakijale)",
          "Lyras",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "matt (Matt)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "303852": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Precious Bodily Fluids\"#p303794 nauči raditi pypi packagee više manje\n\nOstalo je dosta subjektivno, jer npr. svaka firma drukčije shvaća tu organizaciju, a definitivno ne bih mogao reći da huggingface po svemu bolje radi posao od facebooka i ekipe\n\nIma primjera kako da NE ORGANIZIRAŠ kod, a to je kako to rade originalni autori papera. Dakle kad vidiš neki repo papera, 99% šanse su da ne želiš tako pisati stvari",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "Jokerrr",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "matt (Matt)",
          "tonkec"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "303925": {
      "poster": "InCogNiTo124",
      "content": "@\"Precious Bodily Fluids\"#p303794 iskreno nemoj izmisljat toplu vodu. Prouci si python lib Cookiecutter i neke ai/ml/ds stubove",
      "votes": {
        "upvoters": [
          "matt (Matt)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "304028": {
      "poster": "Jokerrr",
      "content": "@\"Mićo\"#p303852 evo bas sam zadnjih tjedana visio na repo-u od insightface rada i nokti su mi otpali kad sam vidio tu organizacijsku zbrku. Kmica napravi models.py i unutra ubaci SVE sto se moze interpretirat “modelom” (od resnet modela do modela funkcije gubitka). Naravno, “vrste modela” odvoji znakovnim nizom ========= da se netko ne izgubi.",
      "votes": {
        "upvoters": [
          "matt (Matt)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124",
          "matt (Matt)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "304116": {
      "poster": "matt (Matt)",
      "content": "Duboki učitelji i računalni vidovnjaci, koliko se vašeg rada na poslu svodi na čisti research (čitanje papera)? Je li vam toga previše, premalo ili taman?",
      "votes": {
        "upvoters": [
          "feel_d_boot (iNut)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "AVRFreak",
          "Haki",
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)",
          "member"
        ],
        "wtf": [
          "Ovo_je_moj_nick"
        ],
        "tuga": []
      }
    },
    "304163": {
      "poster": "feel_d_boot (iNut)",
      "content": "@\"Matt\"#p304116 \n\nTaman. Dobijem problem, čitam online članke da nađem neki model koji bi moga riješit naveden problem, nađi rad o njemu. Pročitaj ga, implementiraj ga i igraj se",
      "votes": {
        "upvoters": [
          "matt (Matt)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124",
          "matt (Matt)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "304216": {
      "poster": "Jokerrr",
      "content": "Upomoć.\n\nPokušavam izračunati CosineSimmilarity između tenzora značajki oblika [num_features, H, W] i tenzora središta razreda [num_features, 1, 1], ovaj prvi oblik mi nikako nije pogodan pošto za CosineSimmilarity moram imat 2 vektora. Al kako je rijec o sem. segmentaciji i modelu koji na izlazu daje znacajke takvog oblika nemam puno ideja kaj napravit. Rezultat ovog moraju biti logitsi koji ce zajedno s labelom [1, H, W] ući u softmaks.\n\nVrijeme mi otkucava a ja vise nemam ideja.",
      "votes": {
        "upvoters": [
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "InCogNiTo124",
          "Lyras",
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "304217": {
      "poster": "InCogNiTo124",
      "content": "@\"Jokerrr\"#p304216 sam se ubij",
      "votes": {
        "upvoters": [
          "AVRFreak",
          "Lyras",
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "feel_d_boot (iNut)",
          "matt (Matt)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Amon",
          "Ducky",
          "Lyras",
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "feel_d_boot (iNut)",
          "kix7 (Fish99)"
        ],
        "wtf": [
          "General_Kenobi (Aborteur)"
        ],
        "tuga": []
      }
    },
    "304220": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "@\"Jokerrr\"#p304216 jesi probao povecati kapacitet modela? 🤔",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "304221": {
      "poster": "feel_d_boot (iNut)",
      "content": "@\"InCogNiTo124\"#p304217 \n\n>  sam se ubij\n\n\nSponsored by Photomath™",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "Lyras",
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)",
          "kix7 (Fish99)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Amon",
          "InCogNiTo124",
          "Lyras",
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "304223": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Jokerrr\"#p304216 Najbolje da to pretvoriš u `(num_features, H * W)` i `(num_features, 1)`, pa onda radiš sim po 1. dimenziji (tj similarity 0. dimenzije). Dobit ćeš`(1, H * W)` vektor koji onda možeš i ne moraš pretvarati natrag, ovisno kaj ti treba.\n\nUvijek možeš i sam napisati konvolucijski kernel po H i W ali vjv nije vrijedno toga. Reshape je ionako u optimalnom slučaju view pa je brzina maksimalna.",
      "votes": {
        "upvoters": [
          "Jokerrr",
          "matt (Matt)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "304224": {
      "poster": "Jokerrr",
      "content": "@\"Precious Bodily Fluids\"#p304220 nebih ga smio dirat, samo ubacit taj modul za kosinusnu slicnost prije softmaxa",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "304225": {
      "poster": "Jokerrr",
      "content": "@\"Mićo\"#p304223 hvala ti puno na ovom",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "304244": {
      "poster": "InCogNiTo124",
      "content": "@\"iNut\"#p304221 obligated \"ovo govorim na svoje ime i ne reprezentiram poslodavca\" momenat\n\nAl da, suicid",
      "votes": {
        "upvoters": [
          "Lyras",
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)",
          "feel_d_boot (iNut)",
          "matt (Matt)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "a-z0-9_- (‮🇿🏳️‍🌈⃠🇮🇱⃠   -_9-0z-a)"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "306574": {
      "poster": "matt (Matt)",
      "content": "jel ovo tema za duboko ucenje, je, e pa zas ne pricamo o dubokom ucenju sutra nam ispit u 8:30 koji ste vi neradnici ovo nema nigdje",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "306577": {
      "poster": "InCogNiTo124",
      "content": "@\"Matt\"#p306574 duboko se ne uci, duboko se zna",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "sane_insane"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "306578": {
      "poster": "nnn (dinoo)",
      "content": "@\"InCogNiTo124\"#p306577 deep",
      "votes": {
        "upvoters": [
          "Amon",
          "InCogNiTo124",
          "ghost (ksi)",
          "kix7 (Fish99)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "306628": {
      "poster": "Jokerrr",
      "content": "doias za cv\n\nda/ne?",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "AVRFreak"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "306633": {
      "poster": "InCogNiTo124",
      "content": "@\"Jokerrr\"#p306628 sure sto da ne\n\nKaj je najgore kaj se moze desit",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "306639": {
      "poster": "feel_d_boot (iNut)",
      "content": "Vi ne stavljate kolegije koje ste položili na CV? \n\nako vam cv nema 5+ stranica, jeste li uopće zapošljivi???",
      "votes": {
        "upvoters": [
          "Amon",
          "Emma63194",
          "General_Kenobi (Aborteur)",
          "Jale (čakijale)",
          "Jaster111",
          "matt (Matt)",
          "tata (NISAM ASISTENT)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Ducky",
          "Emma63194",
          "sane_insane"
        ],
        "wtf": [],
        "tuga": []
      }
    },
    "306714": {
      "poster": "Jokerrr",
      "content": "@\"InCogNiTo124\"#999 @\"iNut\"#31 \n\nGospodo mislio sam dal upisat ili ne upisat doias za computer vision. I zasto.",
      "votes": {
        "upvoters": [],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "306716": {
      "poster": "micho (M̵̧̩͑̀͝î̶͍̉ć̴̝̾́̀o̶̺̟̣͂̽)",
      "content": "@\"Jokerrr\"#p306714 Gle, na DOIASu sigurno nećeš raditi išta relevantno za state-of-the-art, koji je trenutno doslovno crna magija, tak da upišeš radi užitka jedino, po meni",
      "votes": {
        "upvoters": [
          "InCogNiTo124",
          "Jokerrr",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "306717": {
      "poster": "feel_d_boot (iNut)",
      "content": "@\"Jokerrr\"#p306714 \n\nUpravo proša predmet i sa sigurnošću ti mogu reći da nećeš puno ako išta naučiti za praktičnu primjenu, a ni nećeš neku intuiciju dobiti. \n\nAli ako gonjaš bodove, predmet je lagan i lako se skupe.",
      "votes": {
        "upvoters": [
          "Ducky",
          "Jokerrr"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "306720": {
      "poster": "InCogNiTo124",
      "content": "@\"Jokerrr\"#p306714 meni je bio mnogo kul, citat o spektru i rastavu signala i o histogramima i o MRI al to sam ja\n\nNe msm da ce skodit u svakom slucaju",
      "votes": {
        "upvoters": [
          "Jokerrr",
          "bodilyfluids (Dragi prijatelj strojnog učenja)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "306875": {
      "poster": "bodilyfluids (Dragi prijatelj strojnog učenja)",
      "content": "malo poduzi razgovor i kritika o trenutnom stanju AI researcha, good listen\n\nhttps://www.youtube.com/watch?v=BwhBtvCNwxo",
      "votes": {
        "upvoters": [
          "matt (Matt)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [],
        "wtf": [],
        "tuga": []
      }
    },
    "306876": {
      "poster": "InCogNiTo124",
      "content": "![](assets/2022-06-30/00009.jpg)",
      "votes": {
        "upvoters": [
          "Amon",
          "Lyras",
          "kix7 (Fish99)"
        ],
        "downvoters": []
      },
      "reactions": {
        "haha": [
          "Ducky",
          "Lyras",
          "bodilyfluids (Dragi prijatelj strojnog učenja)",
          "kix7 (Fish99)",
          "mikimoj",
          "sane_insane"
        ],
        "wtf": [],
        "tuga": []
      }
    }
  }
}